{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from pyvis.network import Network\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, make_scorer\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel, mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor, BayesianRidge\n",
    "from sklearn.linear_model import RANSACRegressor, TheilSenRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor, BaggingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, WhiteKernel\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, concatenate, BatchNormalization, LSTM, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from umap import UMAP\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from pygam import GAM, s, f, l\n",
    "from sklearn.inspection import permutation_importance, partial_dependence, PartialDependenceDisplay\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.search.ScoreBased.GES import ges\n",
    "from causallearn.utils.GraphUtils import GraphUtils\n",
    "from dowhy import CausalModel\n",
    "from econml.dml import CausalForestDML\n",
    "from econml.dr import DRLearner\n",
    "import gym\n",
    "from gym import spaces\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence, plot_objective\n",
    "from pymoo.core.problem import Problem\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
    "from pymoo.operators.crossover.sbx import SBX\n",
    "from pymoo.operators.mutation.pm import PM\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.visualization.scatter import Scatter\n",
    "from pymoo.util.plotting import plot\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='lightgbm')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('visualizations', exist_ok=True)\n",
    "os.makedirs('data_exports', exist_ok=True)\n",
    "\n",
    "# 配置中文字体显示\n",
    "def setup_chinese_font():\n",
    "    \"\"\"配置matplotlib显示中文字体\"\"\"\n",
    "    print(\"设置中文字体显示...\")\n",
    "    \n",
    "    import matplotlib.font_manager as fm\n",
    "    \n",
    "    # 尝试多种中文字体\n",
    "    chinese_fonts = [\n",
    "        # Windows字体\n",
    "        'SimHei', 'SimSun', 'NSimSun', 'Microsoft YaHei',\n",
    "        # Mac字体\n",
    "        'STHeiti', 'STKaiti', 'STSong',\n",
    "        # Linux字体\n",
    "        'WenQuanYi Micro Hei', 'WenQuanYi Zen Hei',\n",
    "        # 通用字体\n",
    "        'Arial Unicode MS', 'DejaVu Sans'\n",
    "    ]\n",
    "    \n",
    "    # 获取已安装的字体列表\n",
    "    font_names = set([f.name for f in fm.fontManager.ttflist])\n",
    "    \n",
    "    # 检查是否有任何中文字体可用\n",
    "    available_chinese_fonts = [f for f in chinese_fonts if f in font_names]\n",
    "    \n",
    "    if available_chinese_fonts:\n",
    "        plt.rcParams['font.family'] = available_chinese_fonts[0]\n",
    "        plt.rcParams['axes.unicode_minus'] = False\n",
    "        print(f\"成功设置中文字体: {available_chinese_fonts[0]}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"未找到适合的中文字体，将使用英文标签\")\n",
    "        # 创建中英文对照表，方便后续使用\n",
    "        global label_translation\n",
    "        label_translation = {\n",
    "            '水接触角': 'Water Contact Angle',\n",
    "            '循环使用次数': 'Cycle Times',\n",
    "            '吸油能力': 'Oil Absorption Capacity',\n",
    "            '综合性能': 'Overall Performance',\n",
    "            '基底材料': 'Base Material',\n",
    "            '改性材料': 'Modifiers',\n",
    "            '制备方法': 'Preparation Method',\n",
    "            '预测性能': 'Predicted Performance',\n",
    "            '综合得分': 'Overall Score'\n",
    "        }\n",
    "        return False\n",
    "\n",
    "# 配置GPU\n",
    "def setup_gpu():\n",
    "    \"\"\"配置GPU并返回可用性\"\"\"\n",
    "    try:\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            # 设置TensorFlow使用第一个GPU\n",
    "            tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "            tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "            print(f\"GPU加速已启用: {gpus[0].name}\")\n",
    "            return True\n",
    "        else:\n",
    "            # 使用CUDA检查GPU\n",
    "            try:\n",
    "                import torch\n",
    "                if torch.cuda.is_available():\n",
    "                    print(f\"通过PyTorch检测到GPU: {torch.cuda.get_device_name(0)}\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(\"未检测到GPU，将使用CPU\")\n",
    "                    return False\n",
    "            except ImportError:\n",
    "                print(\"未检测到GPU，将使用CPU\")\n",
    "                return False\n",
    "    except:\n",
    "        print(\"GPU配置失败，将使用CPU\")\n",
    "        return False\n",
    "\n",
    "# 自定义进度条类\n",
    "class CustomProgressBar:\n",
    "    def __init__(self, total, desc=\"进度\", bar_length=50):\n",
    "        self.total = total\n",
    "        self.desc = desc\n",
    "        self.n = 0\n",
    "        self.bar_length = bar_length\n",
    "        self.start_time = time.time()\n",
    "        self.last_print_time = 0\n",
    "        self._print_progress()\n",
    "        \n",
    "    def update(self, n=1):\n",
    "        self.n += n\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_print_time >= 0.1 or self.n >= self.total:\n",
    "            self._print_progress()\n",
    "            self.last_print_time = current_time\n",
    "    \n",
    "    def _print_progress(self):\n",
    "        percent = min(100, self.n * 100 / self.total)\n",
    "        filled_length = int(self.bar_length * self.n // self.total)\n",
    "        bar = '█' * filled_length + '-' * (self.bar_length - filled_length)\n",
    "        \n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        if self.n > 0:\n",
    "            time_per_iter = elapsed_time / self.n\n",
    "            remaining_iters = self.total - self.n\n",
    "            remaining_time = time_per_iter * remaining_iters\n",
    "            time_str = f\" - 预计剩余: {self._format_time(remaining_time)}\"\n",
    "        else:\n",
    "            time_str = \"\"\n",
    "            \n",
    "        print(f'\\r{self.desc}: |{bar}| {percent:.1f}% {self.n}/{self.total}{time_str}', end='', flush=True)\n",
    "        if self.n >= self.total:\n",
    "            print()\n",
    "    \n",
    "    def _format_time(self, seconds):\n",
    "        \"\"\"将秒数格式化为时:分:秒\"\"\"\n",
    "        m, s = divmod(int(seconds), 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        if h > 0:\n",
    "            return f\"{h}时{m}分{s}秒\"\n",
    "        elif m > 0:\n",
    "            return f\"{m}分{s}秒\"\n",
    "        else:\n",
    "            return f\"{s}秒\"\n",
    "            \n",
    "    def set_description(self, desc):\n",
    "        \"\"\"更新描述文字\"\"\"\n",
    "        self.desc = desc\n",
    "        self._print_progress()\n",
    "        \n",
    "    def close(self):\n",
    "        \"\"\"关闭进度条\"\"\"\n",
    "        if self.n < self.total:\n",
    "            self.n = self.total\n",
    "            self._print_progress()\n",
    "        print()\n",
    "\n",
    "# 设置环境\n",
    "setup_chinese_font()\n",
    "gpu_available = setup_gpu()\n",
    "\n",
    "# 自定义LightGBM包装器\n",
    "class CustomLGBMRegressor:\n",
    "    \"\"\"自定义LightGBM包装器，避免特征名称问题\"\"\"\n",
    "    def __init__(self, **params):\n",
    "        self.params = params\n",
    "        self.booster = None\n",
    "        self.feature_count = None\n",
    "        self.feature_names = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # 存储特征数量以确保一致性检查\n",
    "        self.feature_count = X.shape[1]\n",
    "        \n",
    "        # 保存特征名称\n",
    "        self.feature_names = X.columns.tolist() if hasattr(X, 'columns') else [f'feature_{i}' for i in range(X.shape[1])]\n",
    "        \n",
    "        # 转换为numpy数组以避免特征名称问题\n",
    "        X_values = X.values if hasattr(X, 'values') else X\n",
    "        y_values = y.values if hasattr(y, 'values') else y\n",
    "        \n",
    "        # 创建不含特征名称的数据集\n",
    "        train_data = lgb.Dataset(X_values, label=y_values)\n",
    "        \n",
    "        # 使用原生LightGBM API训练模型\n",
    "        self.booster = lgb.train(\n",
    "            params=self.params,\n",
    "            train_set=train_data,\n",
    "            num_boost_round=self.params.get('n_estimators', 100)\n",
    "        )\n",
    "        \n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"模型尚未训练\")\n",
    "        \n",
    "        # 检查特征数量一致性\n",
    "        if X.shape[1] != self.feature_count:\n",
    "            raise ValueError(f\"特征数量不匹配。期望 {self.feature_count}，实际 {X.shape[1]}\")\n",
    "        \n",
    "        # 转换为numpy数组\n",
    "        X_values = X.values if hasattr(X, 'values') else X\n",
    "        \n",
    "        # 进行预测\n",
    "        return self.booster.predict(X_values)\n",
    "    \n",
    "    # 添加feature_importances_属性以支持特征重要性分析\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"模型尚未训练，无法获取特征重要性\")\n",
    "        \n",
    "        # 从LightGBM Booster中获取特征重要性\n",
    "        importances = self.booster.feature_importance(importance_type='split')\n",
    "        \n",
    "        # 确保返回与特征数一致的重要性数组\n",
    "        if len(importances) != self.feature_count:\n",
    "            print(f\"警告: LightGBM特征重要性数量({len(importances)})与特征数量({self.feature_count})不匹配\")\n",
    "            aligned_importances = np.zeros(self.feature_count)\n",
    "            for i in range(min(len(importances), self.feature_count)):\n",
    "                aligned_importances[i] = importances[i]\n",
    "            return aligned_importances\n",
    "        \n",
    "        return importances\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"返回模型使用的特征名称列表\"\"\"\n",
    "        return self.feature_names\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return self.params\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        self.params.update(params)\n",
    "        return self\n",
    "\n",
    "# 自定义集成模型类\n",
    "class CustomVotingRegressor:\n",
    "    def __init__(self, estimators, weights=None):\n",
    "        self.estimators = estimators\n",
    "        self.weights = weights if weights is not None else [1] * len(estimators)\n",
    "        self.normalize_weights()\n",
    "        \n",
    "    def normalize_weights(self):\n",
    "        weight_sum = sum(self.weights)\n",
    "        self.weights = [w / weight_sum for w in self.weights]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # 所有模型应该已经训练过\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # 获取每个模型的预测\n",
    "        predictions = []\n",
    "        for _, model in self.estimators:\n",
    "            if hasattr(model, 'predict'):\n",
    "                pred = model.predict(X)\n",
    "                predictions.append(pred)\n",
    "        \n",
    "        # 加权平均预测结果\n",
    "        weighted_preds = np.zeros(len(X))\n",
    "        for i, pred in enumerate(predictions):\n",
    "            weighted_preds += pred * self.weights[i]\n",
    "            \n",
    "        return weighted_preds\n",
    "\n",
    "# 自定义堆叠回归器\n",
    "class CustomStackingRegressor:\n",
    "    def __init__(self, base_models, meta_model, use_lightgbm=False, lightgbm_model=None):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.use_lightgbm = use_lightgbm\n",
    "        self.lightgbm_model = lightgbm_model\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # 收集所有基础模型的预测\n",
    "        base_preds = np.zeros((len(X), len(self.base_models) + (1 if self.use_lightgbm else 0)))\n",
    "        \n",
    "        for i, (_, model) in enumerate(self.base_models):\n",
    "            base_preds[:, i] = model.predict(X)\n",
    "            \n",
    "        # 添加LightGBM的预测\n",
    "        if self.use_lightgbm:\n",
    "            base_preds[:, -1] = self.lightgbm_model.predict(X)\n",
    "            \n",
    "        # 使用元模型做最终预测\n",
    "        return self.meta_model.predict(base_preds)\n",
    "\n",
    "# 神经网络进度条回调\n",
    "class MyProgressBar(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epochs):\n",
    "        super().__init__()\n",
    "        self.epochs = epochs\n",
    "        self.progress_bar = None\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.progress_bar = CustomProgressBar(total=self.epochs, desc=\"神经网络训练\")\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.progress_bar.update(1)\n",
    "        val_loss = logs.get('val_loss', 0)\n",
    "        self.progress_bar.set_description(f\"神经网络训练 - epoch {epoch+1}/{self.epochs} - val_loss: {val_loss:.4f}\")\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.progress_bar.close()\n",
    "\n",
    "# 优化进度类\n",
    "class OptimizationProgress:\n",
    "    def __init__(self, description=\"优化进度\", small_dataset=False):\n",
    "        self.iter = 0\n",
    "        self.max_iter = 50 if small_dataset else 100\n",
    "        self.progress_bar = CustomProgressBar(total=self.max_iter, desc=description)\n",
    "        self.last_update_time = time.time()\n",
    "        \n",
    "    def update(self, xk, convergence=None):\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_update_time > 0.5:\n",
    "            if self.iter < self.max_iter:\n",
    "                self.progress_bar.update(1)\n",
    "                self.iter += 1\n",
    "                self.last_update_time = current_time\n",
    "                \n",
    "                if convergence is not None:\n",
    "                    self.progress_bar.set_description(f\"优化进度 - 收敛度: {convergence:.6f}\")\n",
    "        \n",
    "    def close(self):\n",
    "        if self.iter < self.max_iter:\n",
    "            self.progress_bar.update(self.max_iter - self.iter)\n",
    "        self.progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 数据加载与基础预处理 ======================\n",
    "# 指定Excel文件路径\n",
    "excel_path = r\"E:\\360MoveData\\Users\\DELL\\Desktop\\代码\\最终加上的512.xlsx\"  # 修改为您的文件路径\n",
    "use_smote = True  # 启用SMOTE数据增强\n",
    "handle_outliers = True  # 启用异常值处理\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"油吸附材料预测与优化系统 - 数据处理阶段\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 加载Excel数据\n",
    "data = pd.read_excel(excel_path)\n",
    "print(f\"成功加载数据文件: {excel_path}\")\n",
    "print(f\"数据集包含 {len(data)} 行和 {len(data.columns)} 列\")\n",
    "\n",
    "# 显示数据类型概览\n",
    "print(\"\\n数据类型概览:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "print(\"\\n缺失值情况:\")\n",
    "missing_data = data.isnull().sum()\n",
    "print(missing_data[missing_data > 0])\n",
    "\n",
    "# 保存原始数据副本\n",
    "raw_data = data.copy()\n",
    "\n",
    "# 设置目标变量和特征列\n",
    "# 基底材料在第4列（索引3）\n",
    "base_material_column = data.columns[3]\n",
    "# 改性材料在第5-8列（索引4-7）\n",
    "mod_material_columns = data.columns[4:8].tolist()\n",
    "# 制备方法在第9列（索引8）\n",
    "method_column = data.columns[8]\n",
    "# 目标列在第11-13列（索引10-12）\n",
    "target_columns = data.columns[10:13].tolist()\n",
    "\n",
    "material_columns = [base_material_column] + mod_material_columns\n",
    "\n",
    "print(f\"基底材料列: {base_material_column}\")\n",
    "print(f\"改性材料列: {mod_material_columns}\")\n",
    "print(f\"制备方法列: {method_column}\")\n",
    "print(f\"目标列: {target_columns}\")\n",
    "\n",
    "# 提取唯一的材料和制备方法\n",
    "# 提取唯一的基底材料\n",
    "unique_materials = {}\n",
    "unique_materials['base'] = set(data[base_material_column].dropna().unique())\n",
    "unique_materials['base'] = {m for m in unique_materials['base'] if m and isinstance(m, str)}\n",
    "\n",
    "# 提取唯一的改性材料\n",
    "unique_materials['mod'] = set()\n",
    "for col in mod_material_columns:\n",
    "    unique_values = set(data[col].dropna().unique())\n",
    "    unique_materials['mod'].update(unique_values)\n",
    "unique_materials['mod'] = {m for m in unique_materials['mod'] if m and isinstance(m, str)}\n",
    "\n",
    "# 提取唯一的制备方法\n",
    "unique_methods = set(data[method_column].dropna().unique())\n",
    "unique_methods = {m for m in unique_methods if m and isinstance(m, str)}\n",
    "\n",
    "print(f\"提取了 {len(unique_materials['base'])} 种基底材料\")\n",
    "print(f\"提取了 {len(unique_materials['mod'])} 种改性材料\")\n",
    "print(f\"提取了 {len(unique_methods)} 种制备方法\")\n",
    "\n",
    "# 创建材料名称到组成的映射\n",
    "name_to_material_map = {}\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    material_name = row.get(\"材料名称\", f\"材料_{_}\")\n",
    "\n",
    "    if not isinstance(material_name, str) or pd.isna(material_name):\n",
    "        continue\n",
    "\n",
    "    base_material = row[base_material_column]\n",
    "    mod_materials = [row[col] for col in mod_material_columns\n",
    "                     if isinstance(row[col], str) and not pd.isna(row[col])]\n",
    "\n",
    "    name_to_material_map[material_name] = {\n",
    "        'base': base_material,\n",
    "        'mod': mod_materials,\n",
    "        'method': row[method_column]\n",
    "    }\n",
    "\n",
    "# 特征名称清理函数\n",
    "def sanitize_feature_name(name):\n",
    "    \"\"\"清理特征名称，移除LightGBM不支持的特殊字符，确保特征名称符合模型要求\"\"\"\n",
    "    sanitized = str(name)\n",
    "    sanitized = sanitized.replace(' ', '_')\n",
    "    sanitized = re.sub(r'[\\\\\"{}\\[\\]:,]', '_', sanitized)\n",
    "    sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', sanitized)\n",
    "    if sanitized and sanitized[0].isdigit():\n",
    "        sanitized = 'f_' + sanitized\n",
    "    sanitized = re.sub(r'_+', '_', sanitized)\n",
    "    sanitized = sanitized.rstrip('_')\n",
    "    \n",
    "    return sanitized\n",
    "\n",
    "# 存储特征名映射\n",
    "feature_name_map = {}  # 原始特征名到安全特征名的映射\n",
    "reverse_feature_map = {}  # 安全特征名到原始特征名的映射\n",
    "\n",
    "# ====================== 异常值处理 ======================\n",
    "if handle_outliers:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"执行异常值处理\")\n",
    "    \n",
    "    # 目标变量的异常值处理参数\n",
    "    outlier_params = {\n",
    "        '水接触角': {'lower': 0.03, 'upper': 0.97}, \n",
    "        '循环使用次数': {'lower': 0.01, 'upper': 0.99},\n",
    "        '吸油能力': {'lower': 0.01, 'upper': 0.99}\n",
    "    }\n",
    "    \n",
    "    # 处理目标变量的异常值\n",
    "    for target in target_columns:\n",
    "        if target in data.columns and pd.api.types.is_numeric_dtype(data[target]):\n",
    "            # 获取处理参数\n",
    "            lower_percentile = outlier_params.get(target, {}).get('lower', 0.01)\n",
    "            upper_percentile = outlier_params.get(target, {}).get('upper', 0.99)\n",
    "            \n",
    "            # 异常值检测前的分布统计\n",
    "            print(f\"\\n{target} 异常值处理前统计:\")\n",
    "            valid_values = data[target].dropna()\n",
    "            print(f\"  最小值: {valid_values.min():.4f}, 最大值: {valid_values.max():.4f}\")\n",
    "            print(f\"  1%分位数: {valid_values.quantile(0.01):.4f}, 99%分位数: {valid_values.quantile(0.99):.4f}\")\n",
    "            \n",
    "            # 计算百分位阈值\n",
    "            lower_bound = valid_values.quantile(lower_percentile)\n",
    "            upper_bound = valid_values.quantile(upper_percentile)\n",
    "            \n",
    "            # 统计异常值数量\n",
    "            n_lower_outliers = (valid_values < lower_bound).sum()\n",
    "            n_upper_outliers = (valid_values > upper_bound).sum()\n",
    "            print(f\"  检测到 {n_lower_outliers} 个低于{lower_percentile*100}%分位数的异常值\")\n",
    "            print(f\"  检测到 {n_upper_outliers} 个高于{upper_percentile*100}%分位数的异常值\")\n",
    "            \n",
    "            if n_lower_outliers > 0 or n_upper_outliers > 0:\n",
    "                # 应用Winsorization (缩尾处理)\n",
    "                data.loc[data[target] < lower_bound, target] = lower_bound\n",
    "                data.loc[data[target] > upper_bound, target] = upper_bound\n",
    "                print(f\"  已对 {target} 应用缩尾处理，限制在[{lower_bound:.4f}, {upper_bound:.4f}]范围内\")\n",
    "                \n",
    "                # 处理后的统计\n",
    "                print(f\"  处理后 - 最小值: {data[target].min():.4f}, 最大值: {data[target].max():.4f}\")\n",
    "\n",
    "# 检查目标变量的分布\n",
    "for target in target_columns:\n",
    "    # 检查数据类型\n",
    "    if not pd.api.types.is_numeric_dtype(data[target]):\n",
    "        print(f\"警告: 目标变量 {target} 不是数值类型，尝试转换...\")\n",
    "        data[target] = pd.to_numeric(data[target], errors='coerce')\n",
    "\n",
    "    # 报告基本统计信息\n",
    "    valid_values = data[target].dropna()\n",
    "    print(f\"\\n{target} 统计信息:\")\n",
    "    print(f\"  有效值数量: {len(valid_values)}\")\n",
    "    print(f\"  最小值: {valid_values.min()}\")\n",
    "    print(f\"  最大值: {valid_values.max()}\")\n",
    "    print(f\"  平均值: {valid_values.mean()}\")\n",
    "    print(f\"  中位数: {valid_values.median()}\")\n",
    "    print(f\"  标准差: {valid_values.std()}\")\n",
    "\n",
    "# 可视化目标变量分布\n",
    "fig, axes = plt.subplots(1, len(target_columns), figsize=(15, 5))\n",
    "\n",
    "for i, target in enumerate(target_columns):\n",
    "    valid_data = data[target].dropna()\n",
    "    \n",
    "    if len(target_columns) > 1:\n",
    "        ax = axes[i]\n",
    "    else:\n",
    "        ax = axes\n",
    "\n",
    "    sns.histplot(valid_data, kde=True, ax=ax)\n",
    "    ax.set_title(f'{target} 分布')\n",
    "    ax.set_xlabel(target)\n",
    "    ax.set_ylabel('频率')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/target_distributions.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 可视化材料使用频率\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 基底材料使用频率\n",
    "base_counts = data[base_material_column].value_counts().head(10)\n",
    "sns.barplot(x=base_counts.index, y=base_counts.values)\n",
    "plt.title('最常用的10种基底材料')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/base_material_frequency.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 改性材料使用频率\n",
    "mod_counts = pd.Series(dtype='int64')\n",
    "for col in mod_material_columns:\n",
    "    mod_counts = mod_counts.add(data[col].value_counts(), fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_mods = mod_counts.sort_values(ascending=False).head(10)\n",
    "sns.barplot(x=top_mods.index, y=top_mods.values)\n",
    "plt.title('最常用的10种改性材料')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/mod_material_frequency.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 制备方法使用频率\n",
    "method_counts = data[method_column].value_counts().head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=method_counts.index, y=method_counts.values)\n",
    "plt.title('最常用的10种制备方法')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/method_frequency.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 创建目标变量之间的散点图矩阵\n",
    "valid_targets = data[target_columns].dropna()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.pairplot(valid_targets)\n",
    "plt.suptitle('目标变量相关性散点图矩阵', y=1.02)\n",
    "plt.savefig('visualizations/target_correlations.png', dpi=300)\n",
    "plt.show()\n",
    "data.to_csv('data_exports/preprocessed_data.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"基础预处理完成，数据已保存到 data_exports/preprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 特征工程 ======================\n",
    "print(\"=\" * 40)\n",
    "print(\"正在进行特征工程...\")\n",
    "print(\"=\" * 80)\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# ====================== 材料分类与编码系统 ======================\n",
    "class MaterialEncoder:\n",
    "    \"\"\"材料编码系统：将各类材料映射为数值编码\"\"\"\n",
    "    def __init__(self):\n",
    "        # 初始化材料分类字典\n",
    "        self.material_categories = {\n",
    "            1: \"无机纳米材料/金属氧化物\",\n",
    "            2: \"有机高分子/聚合物\",\n",
    "            3: \"表面改性剂/硅烷类物质\",\n",
    "            4: \"碳基材料\",\n",
    "            5: \"MOF/功能有机小分子/其他\"\n",
    "        }\n",
    "        \n",
    "        # 创建材料编码映射字典\n",
    "        self.material_codes = {}\n",
    "        self.material_combinations = {}\n",
    "        \n",
    "        # 填充编码字典\n",
    "        self._init_inorganic_nanomaterials()  # 类别1\n",
    "        self._init_organic_polymers()         # 类别2\n",
    "        self._init_surface_modifiers()        # 类别3\n",
    "        self._init_carbon_materials()         # 类别4\n",
    "        self._init_mof_and_others()           # 类别5\n",
    "    \n",
    "    def _init_inorganic_nanomaterials(self):\n",
    "        \"\"\"初始化无机纳米材料/金属氧化物编码 (100-199)\"\"\"\n",
    "        inorganic_nanomaterials = {\n",
    "            \"SiO2\": 114,\n",
    "            \"SiO2 nanoparticles\": 114,\n",
    "            \"TiO2\": 122,\n",
    "            \"P25 TiO2 nanoparticles\": 122,\n",
    "            \"TiO2 nanoparticles\": 122,\n",
    "            \"Fe3O4 nanoparticles\": 126,\n",
    "            \"Co3O4\": 127,\n",
    "            \"Co3O4 nanoparticles\": 127,\n",
    "            \"Au nanoparticles\": 179,\n",
    "            \"CoO nanoparticles\": 127,\n",
    "            \"Co nanoparticles\": 127,\n",
    "            \"Ni\": 128,\n",
    "            \"Cu2O\": 129,\n",
    "            \"Cu2O nanoparticles\": 129,\n",
    "            \"CuO nanoparticles\": 129,\n",
    "            \"ZnO nanoparticles\": 130,\n",
    "            \"硅藻土\": 116,\n",
    "            \"diatomite\": 116,\n",
    "            \"Mesoporous Silica Nanoparticles (MSNs)\": 117,\n",
    "            \"Mesoporous Silica Nanoparticles\": 117,\n",
    "            \"MSNs\": 117,\n",
    "            \"Ni nanoparticles\": 128,\n",
    "            \"Cu nanoparticles\": 129,\n",
    "            \"Fe3O4 nanoparticals\": 126,\n",
    "            \"Fe3O5 nanoparticles\": 126,\n",
    "            \"MgFe2O4 nanoparticles\": 132,\n",
    "            \"Ni-Co double layered oxides\": 137,\n",
    "            \"vermiculite\": 118,\n",
    "            \"Ag nanoparticles\": 147,\n",
    "            \"Al2O3 nanoparticles\": 113,\n",
    "            \"γ-AlOOH\": 113,\n",
    "            \"DR-Al2O3 nanoparticles\": 113,\n",
    "            \"BiVO4\": 183,\n",
    "            \"MnO2 nanoparticles\": 125,\n",
    "            \"膨润土纳米片\": 115,\n",
    "            \"膨润土纳米片(Laponite RD)\": 115,\n",
    "            \"Kaolin\": 114,\n",
    "            \"LDH\": 120,\n",
    "            \"Mg-calcite CaCO3颗粒\": 120,\n",
    "            \"Sepiolite\": 119,\n",
    "            \"CuNWs nanoparticles\": 129,\n",
    "            \"CN nanoparticles\": 107,\n",
    "            \"NaF\": 111,\n",
    "            \"Sodium fluoride\": 111,\n",
    "            \"MoS2\": 142\n",
    "        }\n",
    "        \n",
    "        self.material_codes.update(inorganic_nanomaterials)\n",
    "    \n",
    "    def _init_organic_polymers(self):\n",
    "        \"\"\"初始化有机高分子/聚合物编码 (200-299)\"\"\"\n",
    "        organic_polymers = {\n",
    "            \"PVA\": 210,\n",
    "            \"PVP\": 210,\n",
    "            \"PS\": 212,\n",
    "            \"PLA\": 215,\n",
    "            \"PAN\": 216,\n",
    "            \"PA\": 220,\n",
    "            \"聚酰胺树脂\": 220,\n",
    "            \"PI\": 225,\n",
    "            \"polypyrrole\": 242,\n",
    "            \"PVDF-HFP\": 235,\n",
    "            \"polyether sulfone\": 238,\n",
    "            \"Polyurethane\": 224,\n",
    "            \"polyurethane\": 224,\n",
    "            \"PU\": 224,\n",
    "            \"Thermoplastic polyurethane\": 225,\n",
    "            \"polymer SHMP-1\": 252,\n",
    "            \"Poly(sulfobetaine methacrylate)\": 258,\n",
    "            \"Polyhedral oligomeric silsesquioxane\": 268,\n",
    "            \"POSS\": 268,\n",
    "            \"lignin\": 275,\n",
    "            \"Epoxy copolymer\": 233,\n",
    "            \"Epoxy cross-linker\": 233,\n",
    "            \"N,N'-亚甲基双丙烯酰胺\": 228,\n",
    "            \"acrylamide\": 207,\n",
    "            \"phenol-formaldehyde resin\": 222,\n",
    "            \"PDMS\": 230,\n",
    "            \"PFA\": 235,\n",
    "            \"PTFE\":235,\n",
    "            \"PANI\": 240,\n",
    "            \"PPy\": 242,\n",
    "            \"PDA\": 245,\n",
    "            \"PEI\": 246,\n",
    "            \"Polysiloxane\": 230,\n",
    "            \"聚偏氟乙烯-六氟丙烯\": 235,\n",
    "            \"Polysiloxane\": 230,\n",
    "            \"BPEI\": 246,\n",
    "            \"PBZ\": 250,\n",
    "            \"Polybenzoxazine\": 250,\n",
    "            \"Polymethylhydrogen silicone\": 228,\n",
    "            \"PFR\": 254,\n",
    "            \"Cellulose\": 270,\n",
    "            \"CMC\": 270,\n",
    "            \"Chitosan\": 273,\n",
    "            \"chitosan\": 273,\n",
    "            \"Lignin\": 275,\n",
    "            \"Silk fibroin\": 280,\n",
    "            \"HDPE\": 204,\n",
    "            \"PEG\": 206,\n",
    "            \"PMHS\": 232,\n",
    "            \"PDVB\": 249,\n",
    "            \"PZAF\": 255,\n",
    "            \"spiropyran methacrylate derivative\": 290,\n",
    "            \"Polydimethylsiloxane\": 230,\n",
    "            \"Polystyrene\": 212,\n",
    "            \"iPD\": 226,\n",
    "            \"Styrene\": 208\n",
    "        }\n",
    "        \n",
    "        self.material_codes.update(organic_polymers)\n",
    "    \n",
    "    def _init_surface_modifiers(self):\n",
    "        \"\"\"初始化表面改性剂/硅烷类物质编码 (300-399)\"\"\"\n",
    "        surface_modifiers = {\n",
    "            # 氨基类硅烷\n",
    "            \"APTES\": 333,\n",
    "            \"APTMS\": 333,\n",
    "            \"(3-Aminopropyl)triethoxysilane\": 333,\n",
    "            \"(3-氨基丙基)三乙氧基硅烷\": 333,\n",
    "            \"3-氨基丙基三乙氧基硅烷\": 333,\n",
    "            \"巯丙基三乙氧基硅烷\": 353,\n",
    "            \n",
    "            # 环氧基硅烷\n",
    "            \"GPTMS\": 343,\n",
    "            \"GPTS\": 343,\n",
    "            \"(3-Glycidyloxypropyl)trimethoxysilane\": 343,\n",
    "            \n",
    "            # 巯基硅烷\n",
    "            \"MPS\": 353,\n",
    "            \"3-mercaptopropyltrisiloxane\": 353,\n",
    "            \"硫醇\": 359,\n",
    "            \"DDT\": 359,\n",
    "            \"n-Dodecylthiol\": 359,\n",
    "            \"n-十二烷基巯基\": 359,\n",
    "            \"n-octadecylthiol\": 359,\n",
    "            \n",
    "            # 乙烯基硅烷\n",
    "            \"VTMS\": 362,\n",
    "            \"VTES\": 362,\n",
    "            \"乙烯基三乙氧基硅烷\": 362,\n",
    "            \n",
    "            # 甲基/短链烷基硅烷\n",
    "            \"TMCS\": 311,\n",
    "            \"甲基三氯硅烷\": 311,\n",
    "            \"MTMS\": 311,\n",
    "            \"MTS\": 311,\n",
    "            \"甲基三氯硅烷\": 311,\n",
    "            \"CTMS\": 311,\n",
    "            \"N-Octyltrichlorosilane\":328,\n",
    "            # 长链烷基硅烷\n",
    "            \"DTMS\": 322,\n",
    "            \"DMTS\": 322,\n",
    "            \"十六烷基三甲氧基硅烷\": 329,\n",
    "            \"二甲基二甲氧基硅烷\": 311,\n",
    "            \"C16TMS\": 329,\n",
    "            \"HDTMS\": 329,\n",
    "            \"HDTS\": 329,\n",
    "            \"OTS\": 329,\n",
    "            \"ODTS\": 329,\n",
    "            \"ODT\": 329,\n",
    "            \"OTMS\": 329,\n",
    "            \"硬脂基三乙氧基硅烷\": 329,\n",
    "            \"ODS\": 329,\n",
    "            \"十八烷基三氯硅烷\": 329,\n",
    "            \"十八烷基三甲氧基硅烷\": 329,\n",
    "            \"三氯(十八烷基)硅烷\": 329,\n",
    "            \n",
    "            # 全氟类硅烷\n",
    "            \"FDTS\": 389,\n",
    "            \"PFDS\": 389,\n",
    "            \"PFOTES\": 388,\n",
    "            \"PFOTS\": 388,\n",
    "            \"PFTOS\": 388,\n",
    "            \"1H,1H,2H,2H-perfluorodecyltrimethoxysilane\": 388,\n",
    "            \"TMHFDS\": 389,\n",
    "            \"三甲氧基（1H,1H,2H,2H-十七氟癸基）硅烷\": 389,\n",
    "            \"1H,1H,2H,2H-全氟癸基三氯硅烷\": 389,\n",
    "            \"十二氟庚基丙基三甲氧基硅烷\": 387,\n",
    "            \"FAS\": 387,\n",
    "            \"FAS-17\": 387,\n",
    "            \n",
    "            # 双硅基化合物\n",
    "            \"HMDS\": 372,\n",
    "            \"1,1,1,3,3,3-hexamethyldisilazane\": 372,\n",
    "            \n",
    "            # 特殊硅烷\n",
    "            \"TEOS\": 304,\n",
    "            \"硅烷\": 300,\n",
    "            \"Silane\": 300,\n",
    "            \n",
    "            # 其他表面改性剂\n",
    "            \"DETA\": 335,\n",
    "            \"ODA\": 329,\n",
    "            \"OTAB\": 328,\n",
    "            \"PFNA\": 389,\n",
    "            \"Hydroxyl-terminated fluorosilicone\": 384,\n",
    "\n",
    "            \"1H,1H,2H,2H-Perfluorodecyltrimethoxysilane\": 389,\n",
    "            \"1H,1H,2H,2H-Perfluorodecyltrichlorosilane\": 389,\n",
    "            \"全氟癸基三氯硅烷\": 389,\n",
    "            \"1H,1H,2H,2H-perfluorodecyltrimethoxysilane\": 389,\n",
    "            \"PFDTS\": 389,\n",
    "            \"1H,1H,2H,2H-Perfluorodecanethiol\": 389,\n",
    "            \"1H,1H,2H,2H-perfluorodecanethiol\": 389,\n",
    "            \"1H, 1H, 2H, 2H-perfluorodecanethiol\": 389,\n",
    "            \"3-Mercaptopropyltriethoxysilane\": 353,\n",
    "            \"steary methacrylate\": 319,\n",
    "            \"1-dodecanethiol\": 359,\n",
    "            \"n-十二烷基硫醇\": 359,\n",
    "            \"dodecafluoroheptyl-propyl-trimethoxysilane\": 387,\n",
    "            \"octadecane thiol\": 359,\n",
    "            \"n-十八烷基巯基\": 359,\n",
    "            \"1H,1H,2H,2H-Perfluorooctyltriethoxysilane\": 388,\n",
    "            \"Perfluorooctlytriethoxysilane\": 388,\n",
    "            \"1H,1H,2H,2H-perfluorooctyltriethoxysilane\": 388,\n",
    "            \"Methyltriethoxysilane\": 311,\n",
    "            \"cetyltrimethoxysilane\": 329,\n",
    "            \"n-hexadecyltriethoxysilane\": 329,\n",
    "            \"octadecyltrichlorosilane\": 329,\n",
    "            \"OMCTS\": 301,\n",
    "            \"三甲基氯硅烷\": 311,\n",
    "            \"hydrophobic silanes\": 320,\n",
    "        }\n",
    "        \n",
    "        self.material_codes.update(surface_modifiers)\n",
    "    \n",
    "    def _init_carbon_materials(self):\n",
    "        \"\"\"初始化碳基材料编码 (400-499)\"\"\"\n",
    "        carbon_materials = {\n",
    "            \"Carbon black\": 405,\n",
    "            \"Activated carbon\": 415,\n",
    "            \"WPAC\": 415,\n",
    "            \"CNT\": 433,\n",
    "            \"CNTs\": 433, \n",
    "            \"CNTS\": 433,\n",
    "            \"MWCNTs\": 438,\n",
    "            \"CNC\": 435,\n",
    "            \"Graphene\": 459,\n",
    "            \"graphene\": 459,\n",
    "            \"N-Graphene\": 464,\n",
    "            \"MXene\": 470,\n",
    "            \"MXene nanosheets\": 470,\n",
    "            \"Ti3C2Tx\": 470,\n",
    "            \"h-BN\": 445,\n",
    "            \"hBN\": 445,\n",
    "            \"Graphite\": 465,\n",
    "            \"Nanodiamond\": 410,\n",
    "            \"NDs-fPDA\": 410,\n",
    "            \"Carbon nanofiber\": 432,\n",
    "            \"碳海绵\": 467,\n",
    "            \"Carbon-based material\": 470,\n",
    "            \"DLC\": 430,\n",
    "            \"CMP-TST\": 447,\n",
    "            \"HCP\": 460\n",
    "        }\n",
    "        \n",
    "        self.material_codes.update(carbon_materials)\n",
    "    \n",
    "    def _init_mof_and_others(self):\n",
    "        \"\"\"初始化MOF/功能有机小分子/其他编码 (500-699)\"\"\"\n",
    "        mof_and_others = {\n",
    "            # MOF材料 (500-599)\n",
    "            \"AlMOF\": 513,\n",
    "            \"FeMOF\": 526,\n",
    "            \"CoMOF\": 527,\n",
    "            \"Co-HHTP\": 527,\n",
    "            \"CuMOF\": 529,\n",
    "            \"MOF-199\": 529,\n",
    "            \"ZnMOF\": 530,\n",
    "            \"ZrMOF\": 540,\n",
    "            \"DyMOF\": 566,\n",
    "            \"CeMOF\": 558,\n",
    "            \"NH2MOF\": 550,\n",
    "            \"MOF-5\": 555,\n",
    "            \"MOF-74\": 557,\n",
    "            \"MOF-808\": 560,\n",
    "            \"UiO-66-NH2\": 545,\n",
    "            \"UiO-66-NH-C18\": 546,\n",
    "            \"UIO-66(F4)\": 547,\n",
    "            \"TFA-COF\": 570,\n",
    "            \"TFB-COF\": 571,\n",
    "            \"Copper terephthalat\": 529,\n",
    "            \"CoFe-PBA\": 528,\n",
    "            \"UiO-66-MOF\": 544,\n",
    "            \"ZIF-8\": 553,\n",
    "            \"ZIF-67\": 553,\n",
    "            \"HKUST-type MOFs\": 529,\n",
    "            \"COF AG1\": 580,\n",
    "                        \n",
    "            # 有机小分子 (600-699)\n",
    "            \"SA\": 618,\n",
    "            \"Stearic acid\": 618,\n",
    "            \"丁烯酸酯\": 615,\n",
    "            \"Lauric acid\": 612,\n",
    "            \"Palmitic acid\": 616,\n",
    "            \"palmitic acid\": 616,\n",
    "            \"Ca stearate\": 630,\n",
    "            \"Mg stearate\": 631,\n",
    "            \"Fe stearate\": 632,\n",
    "            \"Ni stearate\": 633,\n",
    "            \"Zn stearate\": 634,\n",
    "            \"stearate\": 634,\n",
    "            \"BPO\": 624,\n",
    "            \"Beeswax\": 645,\n",
    "            \"Carnauba wax\": 648,\n",
    "            \"SPMA\": 663,\n",
    "            \"Fluoropolymer\": 665,\n",
    "            \"Urushiol\": 650,\n",
    "            \"TA\": 670,\n",
    "            \"柴油\": 625,\n",
    "            \"OPA\": 661,\n",
    "            \"NDM\": 658,\n",
    "            \"HCCP\": 663,\n",
    "            \"TMPTA\": 662,\n",
    "            \"STA\": 667,\n",
    "            \"Stearoyl chloride\": 638,\n",
    "            \"Palmitoyl chloride\": 636,\n",
    "            \"Phosphoric acid\": 610,\n",
    "            \"1,3-Oxazolidine\": 607,\n",
    "            \"5-Acl\": 608,\n",
    "            \"Econea\": 685,\n",
    "            \"MMDI\": 625,\n",
    "            \"Thiophene\": 608,\n",
    "            \"HDS\": 662,\n",
    "            \"HTFO\": 664,\n",
    "            \"Pyrrole\": 667,\n",
    "            \"SCA\": 669,\n",
    "            \"SEP\": 671,\n",
    "            \"azoisobutyronitrile\": 627,\n",
    "            \"Polyoxometalates\": 690,\n",
    "            \"POMs\": 690,\n",
    "            \"phenol-amine\": 609,\n",
    "            \"BPAF\": 688,\n",
    "            \"12-aminododecanoic acid (NH2(CH2)11COOH)\": 621,\n",
    "            \"Cucurbit[6]uril\": 606,\n",
    "            \"pyrrole\": 667,\n",
    "            \"n-tetradecylphosphonic acid\": 628,\n",
    "            \"Tetradecylamine\": 622,\n",
    "            \"aluminum diethylhypophosphite\": 635,\n",
    "            \"citral\": 614,\n",
    "            \"citronellal\": 614,\n",
    "            \"citric acid\": 613,\n",
    "            \"carnauba wax\": 648,\n",
    "            \"beeswax\": 645,\n",
    "            \"生物蜡乳液\": 646,\n",
    "            \"Candelilla wax\": 647,\n",
    "            \"Dodecyl methacrylate\": 624,\n",
    "            \"LMA\": 624,\n",
    "            \"oleic acid\": 628,\n",
    "            \"Acetone\": 606,\n",
    "            \"Tetrahydrofuran\": 607,\n",
    "            \"dopamine hydrochloride\": 619,\n",
    "            \"Spiropyran methacrylate\": 675,\n",
    "            \"phytic acid\": 666,\n",
    "            \"1, 2, 3, 4-butanetetracarboxylic acid\": 623,\n",
    "            \"n-hexadecylamine\": 623,\n",
    "            \"DTBP\": 620,\n",
    "            \"Vinyl silica aerogel particles\": 639,\n",
    "            \"Tea polyphenols\": 637,\n",
    "            \"Fluorinated surfactant Capstone\": 651,\n",
    "            \"席夫碱\": 617,\n",
    "            \"5,6-dimethylbenzimidazole\": 616,\n",
    "            \"NH3.H2O\": 605\n",
    "        }\n",
    "        \n",
    "        self.material_codes.update(mof_and_others)\n",
    "    \n",
    "    def get_material_code(self, material_name):\n",
    "        \"\"\"获取材料的编码\"\"\"\n",
    "        if material_name in self.material_codes:\n",
    "            return self.material_codes[material_name]\n",
    "        return None\n",
    "    \n",
    "    def get_material_category(self, material_name):\n",
    "        \"\"\"获取材料所属的分类\"\"\"\n",
    "        code = self.get_material_code(material_name)\n",
    "        if code is None:\n",
    "            return None\n",
    "        \n",
    "        # 根据编码范围确定分类\n",
    "        if 100 <= code < 200:\n",
    "            return 1  # 无机纳米材料/金属氧化物\n",
    "        elif 200 <= code < 300:\n",
    "            return 2  # 有机高分子/聚合物\n",
    "        elif 300 <= code < 400:\n",
    "            return 3  # 表面改性剂/硅烷类物质\n",
    "        elif 400 <= code < 500:\n",
    "            return 4  # 碳基材料\n",
    "        elif 500 <= code < 700:\n",
    "            return 5  # MOF/功能有机小分子/其他\n",
    "        return None\n",
    "    \n",
    "    def register_combination(self, materials, combination_code):\n",
    "        \"\"\"\n",
    "        注册新的材料组合编码\n",
    "        \n",
    "        参数:\n",
    "        materials: 材料名称列表\n",
    "        combination_code: 组合编码\n",
    "        \"\"\"\n",
    "        # 转换为frozenset作为不可变键\n",
    "        materials_set = frozenset(materials)\n",
    "        self.material_combinations[materials_set] = combination_code\n",
    "    \n",
    "    def get_combination_code(self, materials):\n",
    "        \"\"\"\n",
    "        获取材料组合的编码\n",
    "        \n",
    "        参数:\n",
    "        materials: 材料名称列表或集合\n",
    "        \n",
    "        返回:\n",
    "        组合编码，如果已注册则返回注册值，否则动态计算\n",
    "        \"\"\"\n",
    "        # 转换为frozenset进行查找\n",
    "        materials_set = frozenset(materials)\n",
    "        \n",
    "        # 检查是否已有注册编码\n",
    "        if materials_set in self.material_combinations:\n",
    "            return self.material_combinations[materials_set]\n",
    "        \n",
    "        # 如果未注册，则基于公式计算编码\n",
    "        if len(materials) < 2:\n",
    "            return None\n",
    "            \n",
    "        # 所有材料应当属于同一类别\n",
    "        categories = set(self.get_material_category(material) for material in materials)\n",
    "        if len(categories) != 1 or None in categories:\n",
    "            return None\n",
    "            \n",
    "        category = categories.pop()\n",
    "        material_codes = [self.get_material_code(material) for material in materials]\n",
    "        \n",
    "        # 根据类别应用适当的公式\n",
    "        if category == 1:  # 无机纳米材料\n",
    "            # 公式: 190 + |M₁-M₂|/10\n",
    "            diff = abs(material_codes[0] - material_codes[1]) / 10\n",
    "            code = 190 + round(diff)\n",
    "        elif category == 2:  # 有机高分子\n",
    "            # 公式: 290 + |P₁-P₂|/5\n",
    "            diff = abs(material_codes[0] - material_codes[1]) / 5\n",
    "            code = 290 + round(diff)\n",
    "        elif category == 3:  # 表面改性剂\n",
    "            # 公式: 390 + |F₁-F₂|\n",
    "            f1 = (material_codes[0] % 100) // 10\n",
    "            f2 = (material_codes[1] % 100) // 10\n",
    "            code = 390 + abs(f1 - f2)\n",
    "        elif category == 4:  # 碳基材料\n",
    "            # 公式: 490 + |D₁-D₂| + (D₁+D₂)/2\n",
    "            d1 = (material_codes[0] % 100) // 10\n",
    "            d2 = (material_codes[1] % 100) // 10\n",
    "            code = 490 + abs(d1 - d2) + round((d1 + d2) / 2)\n",
    "        elif category == 5:  # MOF/功能有机小分子\n",
    "            # 检查两种材料是否在同一子范围\n",
    "            is_mof1 = 500 <= material_codes[0] < 600\n",
    "            is_mof2 = 500 <= material_codes[1] < 600\n",
    "            \n",
    "            if is_mof1 and is_mof2:\n",
    "                # 两种都是MOF: 590 + |M₁-M₂|/5\n",
    "                diff = abs((material_codes[0] - 500) - (material_codes[1] - 500)) / 5\n",
    "                code = 590 + round(diff)\n",
    "            elif not is_mof1 and not is_mof2:\n",
    "                # 两种都是有机小分子: 690 + |O₁-O₂|/5\n",
    "                diff = abs((material_codes[0] - 600) - (material_codes[1] - 600)) / 5\n",
    "                code = 690 + round(diff)\n",
    "            else:\n",
    "                # MOF和有机小分子混合: 690 + (M + O)/10\n",
    "                m_val = material_codes[0] - 500 if is_mof1 else material_codes[1] - 500\n",
    "                o_val = material_codes[0] - 600 if not is_mof1 else material_codes[1] - 600\n",
    "                code = 690 + round((m_val + o_val) / 10)\n",
    "        else:\n",
    "            return None\n",
    "        self.register_combination(materials, code)\n",
    "        return code\n",
    "\n",
    "# ====================== 初始化材料组合编码 ======================\n",
    "def initialize_material_combinations(encoder):\n",
    "    \"\"\"初始化常见材料组合编码，确保所有组合编码唯一\"\"\"\n",
    "\n",
    "    encoder.register_combination([\"Fe3O4 nanoparticles\", \"Ag nanoparticles\"], 191)\n",
    "    encoder.register_combination([\"ZnO nanoparticles\", \"CuO nanoparticles\"], 192)\n",
    "    encoder.register_combination([\"Fe3O4 nanoparticles\", \"SiO2 nanoparticles\"], 193)\n",
    "    encoder.register_combination([\"ZnO nanoparticles\", \"Fe3O4 nanoparticles\"], 194)\n",
    "    encoder.register_combination([\"Ag nanoparticles\", \"Ni\"], 195)\n",
    "    encoder.register_combination([\"Sepiolite\", \"SiO2 nanoparticles\"], 196)\n",
    "    encoder.register_combination([\"MnO2 nanoparticles\", \"Co3O4 nanoparticles\"], 197)\n",
    "    \n",
    "\n",
    "    encoder.register_combination([\"PDA\", \"PEI\"], 291)\n",
    "    encoder.register_combination([\"PDMS\", \"PDA\"], 292)\n",
    "    encoder.register_combination([\"Lignin\", \"PDMS\"], 293)\n",
    "    encoder.register_combination([\"PPy\", \"PEG\"], 294)\n",
    "    encoder.register_combination([\"PLA\", \"PDA\"], 295)\n",
    "    encoder.register_combination([\"Chitosan\", \"PAN\"], 296)\n",
    "    encoder.register_combination([\"PAN\", \"PDA\"], 297)\n",
    "    \n",
    "    encoder.register_combination([\"TEOS\", \"DTMS\"], 391)\n",
    "    encoder.register_combination([\"TEOS\", \"HMDS\"], 391.8)\n",
    "    encoder.register_combination([\"APTES\", \"HDTS\"], 392.4)\n",
    "    encoder.register_combination([\"MTS\", \"APTES\"], 393.2)\n",
    "    encoder.register_combination([\"DETA\", \"硫醇\"], 394)\n",
    "    encoder.register_combination([\"GPTMS\", \"HDTMS\"], 394.8)\n",
    "    encoder.register_combination([\"APTES\", \"HDTMS\"], 395.6)\n",
    "    encoder.register_combination([\"OTAB\", \"OTS\"], 396.4)\n",
    "    encoder.register_combination([\"GPTS\", \"APTES\"], 397.2)\n",
    "    encoder.register_combination([\"VTES\", \"十六烷基三甲氧基硅烷\"], 398)\n",
    "    encoder.register_combination([\"MTMS\", \"GPTMS\"], 398.8)\n",
    "    encoder.register_combination([\"TEOS\", \"TMHFDS\"], 399.6)\n",
    "    \n",
    "    encoder.register_combination([\"Graphene\", \"Carbon black\"], 492)\n",
    "    encoder.register_combination([\"Carbon black\", \"h-BN\"], 494)\n",
    "    encoder.register_combination([\"Graphene\", \"CNTs\"], 496)\n",
    "    encoder.register_combination([\"CMP-TST\", \"CNTS\"], 498)\n",
    "    encoder.register_combination([\"Carbon-based material\", \"NDs-fPDA\"], 500)\n",
    "    \n",
    "    encoder.register_combination([\"ZnMOF\", \"CoMOF\"], 594)\n",
    "    encoder.register_combination([\"HTFO\", \"BPO\"], 691.2)\n",
    "    encoder.register_combination([\"Beeswax\", \"Carnauba wax\"], 692.4)\n",
    "    encoder.register_combination([\"Zn stearate\", \"ZnMOF\"], 693.6)\n",
    "    encoder.register_combination([\"MOF-5\", \"OPA\"], 694.8)\n",
    "    encoder.register_combination([\"HCCP\", \"BPAF\"], 696)\n",
    "    encoder.register_combination([\"Polyoxometalates\", \"MOF-808\"], 697.2)\n",
    "    encoder.register_combination([\"palmitic acid\", \"Urushiol\"], 698.4)\n",
    "    encoder.register_combination([\"1,3-Oxazolidine\", \"Stearoyl chloride\"], 699.6)\n",
    "    encoder.register_combination([\"UiO-66-NH2\", \"ZrMOF\"], 598)\n",
    "\n",
    "# ====================== 创建编码参照表 ======================\n",
    "def create_encoding_reference_table(encoder, materials_by_category, combinations_by_category):\n",
    "    \"\"\"创建编码参照表\"\"\"\n",
    "    # 创建参照表数据\n",
    "    reference_data = []\n",
    "    # 添加单一材料编码\n",
    "    for category_id, materials in materials_by_category.items():\n",
    "        category_name = encoder.material_categories[category_id]\n",
    "        for material in materials:\n",
    "            code = encoder.get_material_code(material)\n",
    "            reference_data.append({\n",
    "                \"材料名称\": material,\n",
    "                \"分类ID\": category_id,\n",
    "                \"分类名称\": category_name,\n",
    "                \"编码值\": code,\n",
    "                \"编码类型\": \"单一材料\"\n",
    "            })\n",
    "    for category_id, combinations in combinations_by_category.items():\n",
    "        category_name = encoder.material_categories[category_id]\n",
    "        for materials, code in combinations.items():\n",
    "            material_list = list(materials)\n",
    "            material_name = \" + \".join(material_list)\n",
    "            reference_data.append({\n",
    "                \"材料名称\": material_name,\n",
    "                \"分类ID\": category_id,\n",
    "                \"分类名称\": category_name,\n",
    "                \"编码值\": code,\n",
    "                \"编码类型\": \"材料组合\"\n",
    "            })\n",
    "    df = pd.DataFrame(reference_data)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values(by=[\"分类ID\", \"编码值\"])\n",
    "    return df\n",
    "\n",
    "#特征工程函数\n",
    "def process_material_features(data, encoder, mod_material_columns):\n",
    "    print(\"\\n正在进行材料特征工程...\")\n",
    "    feature_data = data.copy()\n",
    "    for category_id in range(1, 6):\n",
    "        category_name = encoder.material_categories[category_id]\n",
    "        column_name = f\"类别{category_id}_{category_name}\"\n",
    "        feature_data[column_name] = 0\n",
    "    materials_by_category = defaultdict(set)\n",
    "    combinations_by_category = defaultdict(dict)\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        materials = []\n",
    "        for col in mod_material_columns:\n",
    "            if pd.notna(row[col]) and row[col]:\n",
    "                materials.append(row[col])\n",
    "        \n",
    "        if len(materials) < 1:\n",
    "            continue  \n",
    "        \n",
    "        materials_by_category_row = defaultdict(list)\n",
    "        for material in materials:\n",
    "            category = encoder.get_material_category(material)\n",
    "            if category:\n",
    "                materials_by_category_row[category].append(material)\n",
    "                materials_by_category[category].add(material)\n",
    "            else:\n",
    "                print(f\"警告: 材料 '{material}' 在编码系统中未找到 (行 {idx+1})\")\n",
    "        \n",
    "        for category, category_materials in materials_by_category_row.items():\n",
    "            column_name = f\"类别{category}_{encoder.material_categories[category]}\"\n",
    "            \n",
    "            if len(category_materials) == 1:\n",
    "                material = category_materials[0]\n",
    "                code = encoder.get_material_code(material)\n",
    "                feature_data.loc[idx, column_name] = code\n",
    "            else:\n",
    "                category_materials_set = frozenset(category_materials)\n",
    "                \n",
    "                if len(category_materials) == 2:\n",
    "                    code = encoder.get_combination_code(category_materials)\n",
    "                    feature_data.loc[idx, column_name] = code\n",
    "                    combinations_by_category[category][category_materials_set] = code\n",
    "                else:\n",
    "                    first_two = category_materials[:2]\n",
    "                    code = encoder.get_combination_code(first_two)\n",
    "                    feature_data.loc[idx, column_name] = code\n",
    "                    combinations_by_category[category][frozenset(first_two)] = code\n",
    "                    print(f\"注意: 行 {idx+1} 中类别 {category} 有超过2种材料，仅使用前两种计算组合编码。\")\n",
    "    reference_table = create_encoding_reference_table(encoder, materials_by_category, combinations_by_category)\n",
    "    \n",
    "    print(f\"特征工程完成。已处理 {len(data)} 行数据。\")\n",
    "    print(f\"已识别 {sum(len(materials) for materials in materials_by_category.values())} 种单一材料\")\n",
    "    print(f\"已创建 {sum(len(combos) for combos in combinations_by_category.values())} 种材料组合编码\")\n",
    "    return feature_data, reference_table\n",
    "# ====================== 制备方法和基底材料编码 ======================\n",
    "def encode_method_and_base(data, method_column, base_material_column):\n",
    "    \"\"\"为制备方法和基底材料创建系统化编码\"\"\"\n",
    "    print(\"\\n正在对制备方法和基底材料进行系统化编码...\")\n",
    "    feature_data = data.copy()\n",
    "    \n",
    "    # 基底材料编码映射 - 材质类型码(1-9) × 10 + 材料特性值(1-9)\n",
    "    base_mapping = {\n",
    "        \"Polyurethane sponge\": 11,\n",
    "        \"Polystyrene\": 21,\n",
    "        \"Polystyrene sponge\": 22,\n",
    "        \"Polystyrene\": 23,\n",
    "        \"Cellulose sponge\": 31, \n",
    "        \"Ethyl Cellulose\": 32,\n",
    "        \"Plant Fiber Sponge\": 33,\n",
    "        \"kapok fibre sponge\": 34,\n",
    "        \"Polyacrylonitrile sponge\": 41,\n",
    "        \"PAN sponge\": 42,\n",
    "        \"PVA sponge\": 51,\n",
    "        \"Polyamide sponge\": 61,\n",
    "        \"Melamine sponge\": 71,\n",
    "        \"Melamine-formaldehyde sponge\": 72,\n",
    "        \"PDMS sponge\": 81,\n",
    "        \"silicone sponge\": 82,\n",
    "        \"TEOS\": 83,\n",
    "        \"nickel sponge\": 91,\n",
    "        \"CuNWs nanoparticles\": 92,\n",
    "        \"EcoFlex\": 93,\n",
    "        \"carrageenan sponge\": 94,\n",
    "        \"CS sponge\": 95,\n",
    "        \"PET sponge\": 96,\n",
    "        # 添加到 base_mapping 字典中\n",
    "        \"PLA sponge\": 25,\n",
    "        \"Chitosan sponge\": 95,\n",
    "        \"CS sponge\": 95,\n",
    "        \"Luffa sponge\": 35,\n",
    "        \"Polyethylene sponge\": 24,\n",
    "        \"nylon fibrous sponge\": 62,\n",
    "        \"rock wool\": 37,\n",
    "        \"polyester fabric sponge\": 27,\n",
    "        \"碳海绵\": 98,  # 特殊材料类\n",
    "        \"TODB\": 97\n",
    "    }\n",
    "    \n",
    "    # 制备方法编码映射 - 方法类型码(1-9) × 10 + 工艺变体码(0-9)\n",
    "    method_mapping = {\n",
    "        \"浸渍法\": 10,\n",
    "        \"PDA浸渍法\": 11,\n",
    "        \"BPO浸渍法\": 12,\n",
    "        \"APTES浸渍法\": 13,\n",
    "        \"PFR浸渍法\": 14,\n",
    "        \"丙烯酸树脂浸渍法\": 15,\n",
    "        \"硅橡胶粘合剂浸渍法\": 16,\n",
    "        \"PVDF浸渍法\": 17,\n",
    "        \"OTS浸渍法\": 18,\n",
    "        \"环氧树脂浸渍法\": 19,\n",
    "        \"HDTMS浸渍法\": 21,\n",
    "        \"MTS浸渍法\": 22,\n",
    "        \"PDMS浸渍法\": 23,\n",
    "        \"气相沉积法\": 43,\n",
    "        \"硅橡胶浸渍法\": 16,\n",
    "        \"原位生长\": 32,\n",
    "        \"天然乳胶浸渍法\": 24,\n",
    "        \"Silk ﬁbroin浸渍法\": 25,\n",
    "        \"PVA浸渍法\": 26,\n",
    "        \"PDA浸渍法_x000D_\": 27,\n",
    "        \"PDMS浸渍法_x000D_\": 28,\n",
    "        \"酚醛树脂浸渍法\": 29,\n",
    "        \"模板法\": 30,\n",
    "        \"原位聚合\": 31,\n",
    "        \"溶剂热还原法\": 40,\n",
    "        \"静电纺丝法\": 41,\n",
    "        \"发泡法\": 42,\n",
    "        \"涂覆法\": 50\n",
    "    }\n",
    "    \n",
    "    feature_data['制备方法_编码'] = data[method_column].map(method_mapping)\n",
    "    feature_data['基底材料_编码'] = data[base_material_column].map(base_mapping)\n",
    "    \n",
    "    # 检查是否有未被编码的材料或方法（添加行号信息）\n",
    "    for idx, row in data.iterrows():\n",
    "        if pd.notna(row[method_column]) and row[method_column] not in method_mapping:\n",
    "            print(f\"警告: 制备方法 '{row[method_column]}' 在编码系统中未找到 (行 {idx+1})\")\n",
    "        if pd.notna(row[base_material_column]) and row[base_material_column] not in base_mapping:\n",
    "            print(f\"警告: 基底材料 '{row[base_material_column]}' 在编码系统中未找到 (行 {idx+1})\")\n",
    "        \n",
    "    # 处理缺失值\n",
    "    feature_data['制备方法_编码'].fillna(0, inplace=True)\n",
    "    feature_data['基底材料_编码'].fillna(0, inplace=True)\n",
    "    \n",
    "    # 创建方法和基底材料编码参照表\n",
    "    method_df = pd.DataFrame([\n",
    "        {'制备方法': method, '编码值': code, '方法类型': f\"类型{code//10}\", '工艺变体': code%10} \n",
    "        for method, code in method_mapping.items()\n",
    "    ])\n",
    "    \n",
    "    base_df = pd.DataFrame([\n",
    "        {'基底材料': base, '编码值': code, '材质类型': f\"类型{code//10}\", '特性值': code%10} \n",
    "        for base, code in base_mapping.items()\n",
    "    ])\n",
    "    \n",
    "    print(f\"完成制备方法编码，共 {len(method_mapping)} 种方法\")\n",
    "    print(f\"完成基底材料编码，共 {len(base_mapping)} 种材料\")\n",
    "    \n",
    "    return feature_data, method_df, base_df\n",
    "# ====================== 执行特征工程 ======================\n",
    "def run_feature_engineering(data, base_material_column, mod_material_columns, method_column, target_columns, year_column=None):\n",
    "    \"\"\"执行完整的特征工程流程\n",
    "    \n",
    "    参数:\n",
    "    data: 原始数据\n",
    "    base_material_column: 基底材料列名\n",
    "    mod_material_columns: 改性材料列名列表\n",
    "    method_column: 制备方法列名\n",
    "    target_columns: 目标变量列名列表\n",
    "    year_column: 年份列名 (可选)\n",
    "    \"\"\"\n",
    "    output_dir = \"data_exports\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    encoder = MaterialEncoder()\n",
    "    initialize_material_combinations(encoder)\n",
    "    feature_data, material_reference = process_material_features(data, encoder, mod_material_columns)\n",
    "    feature_data, method_reference, base_reference = encode_method_and_base(\n",
    "        feature_data, method_column, base_material_column)\n",
    "    \n",
    "    # 如果提供了年份列，则将其添加到特征数据中\n",
    "    if year_column and year_column in data.columns:\n",
    "        feature_data[year_column] = data[year_column]\n",
    "    \n",
    "    # 1. 保存带有材料名称的特征数据（用于查看）\n",
    "    feature_data_with_names = feature_data.copy()\n",
    "    feature_data_with_names.to_excel(f\"{output_dir}/material_features_with_names.xlsx\", index=False)\n",
    "    \n",
    "    # 2. 保存纯数值编码的特征数据（用于模型训练）\n",
    "    target_data = data[target_columns].copy()\n",
    "    feature_cols = [col for col in feature_data.columns \n",
    "                   if col.startswith('类别') or col == '制备方法_编码' or col == '基底材料_编码']\n",
    "    \n",
    "    # 如果年份列存在，也将其包含在特征数据中\n",
    "    if year_column and year_column in feature_data.columns:\n",
    "        feature_cols.append(year_column)\n",
    "    \n",
    "    feature_data_numeric = pd.concat([feature_data[feature_cols], target_data], axis=1)\n",
    "    print(\"\\n特征数据列顺序:\")\n",
    "    for i, col in enumerate(feature_data_numeric.columns):\n",
    "        print(f\"{i+1}. {col}\")\n",
    "    feature_data_numeric.to_excel(f\"{output_dir}/material_features_numeric.xlsx\", index=False)\n",
    "    feature_data_numeric.to_csv(f\"{output_dir}/material_features_numeric.csv\", index=False)\n",
    "    \n",
    "    # 3. 保存编码参照表\n",
    "    material_reference.to_excel(f\"{output_dir}/material_encoding_reference.xlsx\", index=False)\n",
    "    method_reference.to_excel(f\"{output_dir}/method_encoding_reference.xlsx\", index=False)\n",
    "    base_reference.to_excel(f\"{output_dir}/base_material_encoding_reference.xlsx\", index=False)\n",
    "    \n",
    "    # 4. 保存完整的编码表，包含所有单一材料的编码\n",
    "    all_materials = []\n",
    "    # 添加所有单一材料\n",
    "    for category_id in range(1, 6):\n",
    "        category_name = encoder.material_categories[category_id]\n",
    "        for material, code in encoder.material_codes.items():\n",
    "            if encoder.get_material_category(material) == category_id:\n",
    "                all_materials.append({\n",
    "                    \"材料名称\": material,\n",
    "                    \"分类ID\": category_id,\n",
    "                    \"分类名称\": category_name,\n",
    "                    \"编码值\": code,\n",
    "                    \"编码类型\": \"单一材料\"\n",
    "                })\n",
    "    \n",
    "    # 添加所有组合材料（从material_reference中获取）\n",
    "    combo_rows = material_reference[material_reference['编码类型'] == '材料组合']\n",
    "    if not combo_rows.empty:\n",
    "        all_materials.extend(combo_rows.to_dict('records'))\n",
    "    \n",
    "    # 创建完整的编码表\n",
    "    all_materials_df = pd.DataFrame(all_materials)\n",
    "    if not all_materials_df.empty:\n",
    "        all_materials_df = all_materials_df.sort_values(by=[\"分类ID\", \"编码值\"])\n",
    "        all_materials_df.to_excel(f\"{output_dir}/complete_material_encoding_table.xlsx\", index=False)\n",
    "    \n",
    "    print(f\"\\n特征工程完成！\")\n",
    "    print(f\"带有材料名称的特征数据已保存至: {output_dir}/material_features_with_names.xlsx\")\n",
    "    print(f\"用于模型训练的特征数据已保存至: {output_dir}/material_features_numeric.xlsx\")\n",
    "    print(f\"编码参照表已保存至: {output_dir}/material_encoding_reference.xlsx\")\n",
    "    print(f\"完整的材料编码表已保存至: {output_dir}/complete_material_encoding_table.xlsx\")\n",
    "    \n",
    "    return feature_data, feature_data_numeric, material_reference, method_reference, base_reference\n",
    "\n",
    "excel_path = r\"E:\\360MoveData\\Users\\DELL\\Desktop\\代码\\data_exports\\preprocessed_data.csv\"  # 修改为您的文件路径\n",
    "\n",
    "# 加载Excel数据\n",
    "data = pd.read_csv(excel_path)\n",
    "print(f\"成功加载数据文件: {excel_path}\")\n",
    "print(f\"数据集包含 {len(data)} 行和 {len(data.columns)} 列\")\n",
    "# 设置列信息\n",
    "base_material_column = data.columns[3]\n",
    "mod_material_columns = data.columns[4:8].tolist()\n",
    "method_column = data.columns[8]\n",
    "target_columns = data.columns[10:13].tolist()  # 目标变量列（水接触角、循环使用次数、吸油能力）\n",
    "year_column = data.columns[13]  # 第14列是年份列\n",
    "\n",
    "feature_data, feature_data_numeric, material_ref, method_ref, base_ref = run_feature_engineering(\n",
    "    data, base_material_column, mod_material_columns, method_column, target_columns, year_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "\n",
    "# 忽略特定警告\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# 文件名净化函数\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"将文件名中的无效字符替换为下划线\"\"\"\n",
    "    invalid_chars = ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|']\n",
    "    for char in invalid_chars:\n",
    "        filename = filename.replace(char, '_')\n",
    "    return filename\n",
    "\n",
    "# 定义目标变量特定的容忍度值\n",
    "target_tolerance = {\n",
    "    '水接触角': 0.2,  # 保持当前容忍度\n",
    "    '循环使用次数': 0.3,  # 提高循环使用次数的容忍度\n",
    "    '吸油能力': 0.3   # 提高吸油能力的容忍度\n",
    "}\n",
    "\n",
    "# 添加误差容忍的评估函数\n",
    "def tolerance_r2_score(y_true, y_pred, tolerance=0.15, target=None):\n",
    "    \"\"\"\n",
    "    计算容忍度R²评分，允许一定误差范围内的预测被视为准确\n",
    "    \n",
    "    参数:\n",
    "    y_true: 真实值\n",
    "    y_pred: 预测值\n",
    "    tolerance: 容忍度，表示为真实值的百分比\n",
    "    target: 目标变量名称，用于选择特定的容忍度\n",
    "    \n",
    "    返回:\n",
    "    修正后的R²分数\n",
    "    \"\"\"\n",
    "    # 确保输入数据是numpy数组并且形状正确\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    \n",
    "    # 如果提供了目标变量名，则使用目标特定的容忍度\n",
    "    if target and target in target_tolerance:\n",
    "        tolerance = target_tolerance[target]\n",
    "    \n",
    "    # 计算容忍范围\n",
    "    tolerance_values = tolerance * np.abs(y_true)\n",
    "    # 计算残差\n",
    "    residuals = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # 调整残差，使误差在容忍范围内的视为0\n",
    "    adjusted_residuals = np.maximum(0, residuals - tolerance_values)\n",
    "    \n",
    "    # 计算修正后的总平方和\n",
    "    y_true_mean = np.mean(y_true)\n",
    "    tss = np.sum((y_true - y_true_mean) ** 2)\n",
    "    \n",
    "    # 计算修正后的残差平方和\n",
    "    rss = np.sum(adjusted_residuals ** 2)\n",
    "    \n",
    "    # 计算修正后的R²\n",
    "    if tss == 0:\n",
    "        return 0  # 防止除以0\n",
    "    \n",
    "    tolerance_r2 = 1 - (rss / tss)\n",
    "    return tolerance_r2\n",
    "\n",
    "def prediction_within_tolerance(y_true, y_pred, tolerance=0.15, target=None):\n",
    "    \"\"\"\n",
    "    计算预测值在目标值±容忍范围内的比例\n",
    "    \"\"\"\n",
    "    # 确保输入为numpy数组\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    # 如果提供了目标变量名，则使用目标特定的容忍度\n",
    "    if target and target in target_tolerance:\n",
    "        tolerance = target_tolerance[target]\n",
    "    \n",
    "    # 计算容忍范围\n",
    "    tolerance_values = tolerance * np.abs(y_true)\n",
    "    \n",
    "    # 检查预测是否在容忍范围内\n",
    "    within_tolerance = np.abs(y_true - y_pred) <= tolerance_values\n",
    "    \n",
    "    # 计算在容忍范围内的预测比例\n",
    "    return np.mean(within_tolerance)\n",
    "\n",
    "def make_tolerance_scorer(target):\n",
    "    def scorer_function(estimator, X, y):\n",
    "        y_pred = estimator.predict(X)\n",
    "        return tolerance_r2_score(y, y_pred, target=target)\n",
    "    \n",
    "    # 设置函数名称\n",
    "    scorer_function.__name__ = f'tolerance_scorer_{target}'\n",
    "    return scorer_function\n",
    "\n",
    "# 神经网络进度条回调\n",
    "class MyProgressBar(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epochs):\n",
    "        super(MyProgressBar, self).__init__()\n",
    "        self.epochs = epochs\n",
    "        self.progress_bar = None\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(\"开始神经网络训练...\")\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get('val_loss', 0)\n",
    "        if epoch % 10 == 0 or epoch == self.epochs - 1:\n",
    "            print(f\"epoch {epoch+1}/{self.epochs} - val_loss: {val_loss:.4f}\")\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"神经网络训练完成\")\n",
    "\n",
    "# LightGBM自定义封装器\n",
    "class CustomLGBMRegressor:\n",
    "    def __init__(self, **params):\n",
    "        self.params = params\n",
    "        self.model = None       \n",
    "    def fit(self, X, y):\n",
    "        import lightgbm as lgb\n",
    "        feature_names = [f'f{i}' for i in range(X.shape[1])]\n",
    "        X_values = X.values if hasattr(X, 'values') else X\n",
    "        train_data = lgb.Dataset(X_values, label=y, feature_name=feature_names)\n",
    "        self.model = lgb.train(self.params, train_data, num_boost_round=self.params.get('n_estimators', 100))\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call fit first.\")\n",
    "            \n",
    "        # 保持一致性，也将预测数据转换为numpy数组\n",
    "        X_values = X.values if hasattr(X, 'values') else X\n",
    "        return self.model.predict(X_values)\n",
    "\n",
    "# 自定义投票回归器\n",
    "class CustomVotingRegressor:\n",
    "    def __init__(self, estimators, weights=None):\n",
    "        self.estimators = estimators\n",
    "        self.weights = weights if weights is not None else [1] * len(estimators)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for name, model in self.estimators:\n",
    "            if hasattr(model, 'predict'):\n",
    "                pred = model.predict(X)\n",
    "                predictions.append(pred)\n",
    "            else:\n",
    "                if isinstance(model, dict) and 'model' in model:\n",
    "                    if model.get('needs_scaling', False) and 'scaler' in model:\n",
    "                        X_scaled = model['scaler'].transform(X)\n",
    "                        pred = model['model'].predict(X_scaled)\n",
    "                    else:\n",
    "                        pred = model['model'].predict(X)\n",
    "                    predictions.append(pred)\n",
    "        weighted_preds = np.average(predictions, axis=0, weights=self.weights)\n",
    "        return weighted_preds\n",
    "\n",
    "# 绘制学习曲线来检测过拟合\n",
    "def plot_learning_curves(model, X_train, y_train, X_test, y_test, model_name, target_name, cv=5):\n",
    "    \"\"\"\n",
    "    绘制学习曲线，用于检测过拟合\n",
    "    \n",
    "    参数:\n",
    "    model: 训练模型\n",
    "    X_train, y_train: 训练数据\n",
    "    X_test, y_test: 测试数据\n",
    "    model_name: 模型名称\n",
    "    target_name: 目标变量名称\n",
    "    cv: 交叉验证折数\n",
    "    \"\"\"\n",
    "    # 这里我们只对支持partial_fit或warm_start的模型进行学习曲线绘制\n",
    "    if not (hasattr(model, 'partial_fit') or (hasattr(model, 'warm_start') and model.warm_start)):\n",
    "        print(f\"{model_name}模型不支持增量学习，无法绘制学习曲线\")\n",
    "        return\n",
    "    \n",
    "    # 为了简化，我们仅对部分数据进行评估\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    for size in train_sizes:\n",
    "        n_samples = int(len(X_train) * size)\n",
    "        if n_samples < 5:  # 确保至少有5个样本\n",
    "            continue\n",
    "            \n",
    "        X_subset = X_train.iloc[:n_samples] if hasattr(X_train, 'iloc') else X_train[:n_samples]\n",
    "        y_subset = y_train.iloc[:n_samples] if hasattr(y_train, 'iloc') else y_train[:n_samples]\n",
    "        \n",
    "        # 重新训练模型\n",
    "        model.fit(X_subset, y_subset)\n",
    "        \n",
    "        # 计算训练集和测试集上的分数\n",
    "        train_score = r2_score(y_subset, model.predict(X_subset))\n",
    "        test_score = r2_score(y_test, model.predict(X_test))\n",
    "        \n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "    \n",
    "    # 绘制学习曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes * len(X_train), train_scores, 'o-', color='r', label='训练集 R²')\n",
    "    plt.plot(train_sizes * len(X_train), test_scores, 'o-', color='g', label='测试集 R²')\n",
    "    plt.title(f'{target_name} - {model_name} 学习曲线')\n",
    "    plt.xlabel('训练样本数')\n",
    "    plt.ylabel('R² 分数')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # 检测过拟合\n",
    "    if len(train_scores) > 0 and len(test_scores) > 0:\n",
    "        train_final = train_scores[-1]\n",
    "        test_final = test_scores[-1]\n",
    "        gap = train_final - test_final\n",
    "        \n",
    "        print(f\"最终训练集 R²: {train_final:.4f}\")\n",
    "        print(f\"最终测试集 R²: {test_final:.4f}\")\n",
    "        print(f\"训练集与测试集 R² 差距: {gap:.4f}\")\n",
    "        \n",
    "        if gap > 0.2:\n",
    "            print(\"警告: 模型可能存在过拟合 (训练集和测试集的R²差距大于0.2)\")\n",
    "        elif gap < 0:\n",
    "            print(\"警告: 模型可能存在欠拟合 (测试集R²高于训练集)\")\n",
    "        else:\n",
    "            print(\"模型拟合良好 (训练集和测试集的R²差距小于0.2)\")\n",
    "\n",
    "# 评估模型函数\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, target, model_name):\n",
    "    \"\"\"\n",
    "    评估模型在训练集和测试集上的性能，包括标准R²和容忍度R²\n",
    "    \"\"\"\n",
    "    # 获取目标特定的容忍度\n",
    "    current_tolerance = target_tolerance.get(target, 0.15)\n",
    "    \n",
    "    # 在训练集上评估\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    if len(y_train_pred.shape) > 1 and y_train_pred.shape[1] == 1:\n",
    "        y_train_pred = y_train_pred.flatten()\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    train_tol_r2 = tolerance_r2_score(y_train, y_train_pred, tolerance=current_tolerance, target=target)\n",
    "    train_within_tol = prediction_within_tolerance(y_train, y_train_pred, tolerance=current_tolerance, target=target)\n",
    "    \n",
    "    # 在测试集上评估\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    if len(y_test_pred.shape) > 1 and y_test_pred.shape[1] == 1:\n",
    "        y_test_pred = y_test_pred.flatten()\n",
    "        \n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_tol_r2 = tolerance_r2_score(y_test, y_test_pred, tolerance=current_tolerance, target=target)\n",
    "    test_within_tol = prediction_within_tolerance(y_test, y_test_pred, tolerance=current_tolerance, target=target)\n",
    "    \n",
    "    # 打印评估结果\n",
    "    print(f\"\\n{model_name} 在 {target} 上的评估结果:\")\n",
    "    print(f\"训练集: R²={train_r2:.4f}, 容忍度R²={train_tol_r2:.4f}, 在容忍范围内比例={train_within_tol:.2%}\")\n",
    "    print(f\"测试集: R²={test_r2:.4f}, 容忍度R²={test_tol_r2:.4f}, 在容忍范围内比例={test_within_tol:.2%}\")\n",
    "    \n",
    "    # 检测过拟合\n",
    "    r2_gap = train_r2 - test_r2\n",
    "    if r2_gap > 0.2:\n",
    "        print(f\"警告: 模型可能存在过拟合 (R²差距: {r2_gap:.4f})\")\n",
    "    elif r2_gap < -0.1:\n",
    "        print(f\"警告: 模型可能存在欠拟合 (R²差距: {r2_gap:.4f})\")\n",
    "    else:\n",
    "        print(f\"模型拟合良好 (R²差距: {r2_gap:.4f})\")\n",
    "    \n",
    "    # 绘制预测值与实际值的对比散点图\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 训练集散点图\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_train, y_train_pred, alpha=0.5)\n",
    "    plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')\n",
    "    plt.xlabel('实际值')\n",
    "    plt.ylabel('预测值')\n",
    "    plt.title(f'训练集: R²={train_r2:.4f}')\n",
    "    \n",
    "    # 测试集散点图\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('实际值')\n",
    "    plt.ylabel('预测值')\n",
    "    plt.title(f'测试集: R²={test_r2:.4f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'train_r2': train_r2,\n",
    "        'train_tol_r2': train_tol_r2,\n",
    "        'train_within_tol': train_within_tol,\n",
    "        'test_r2': test_r2,\n",
    "        'test_tol_r2': test_tol_r2,\n",
    "        'test_within_tol': test_within_tol\n",
    "    }\n",
    "# 加载特征数据\n",
    "feature_data_path = \"data_exports/material_features_numeric.xlsx\"\n",
    "\n",
    "# 加载特征数据\n",
    "data = pd.read_excel(feature_data_path)\n",
    "print(f\"成功加载特征数据: {feature_data_path}\")\n",
    "print(f\"数据集包含 {len(data)} 行和 {len(data.columns)} 列\")\n",
    "    \n",
    "# 数值预处理\n",
    "# 1. 第一列减100再归一化\n",
    "data.iloc[:, 0] = data.iloc[:, 0] - 100\n",
    "data.iloc[:, 0] = 20*(data.iloc[:, 0] - data.iloc[:, 0].min()) / (data.iloc[:, 0].max() - data.iloc[:, 0].min())\n",
    "\n",
    "# 2. 第二列减200再归一化\n",
    "data.iloc[:, 1] = data.iloc[:, 1] - 200\n",
    "data.iloc[:, 1] = 20*(data.iloc[:, 1] - data.iloc[:, 1].min()) / (data.iloc[:, 1].max() - data.iloc[:, 1].min())\n",
    "\n",
    "# 3. 第三列减300再归一化\n",
    "data.iloc[:, 2] = data.iloc[:, 2] - 300\n",
    "data.iloc[:, 2] = 20*(data.iloc[:, 2] - data.iloc[:, 2].min()) / (data.iloc[:, 2].max() - data.iloc[:, 2].min())\n",
    "\n",
    "# 4. 第四列减400再归一化\n",
    "data.iloc[:, 3] = data.iloc[:, 3] - 400\n",
    "data.iloc[:, 3] = 20*(data.iloc[:, 3] - data.iloc[:, 3].min()) / (data.iloc[:, 3].max() - data.iloc[:, 3].min())\n",
    "\n",
    "# 5. 第五列减500除以二再归一化\n",
    "data.iloc[:, 4] = (data.iloc[:, 4] - 500) / 2\n",
    "data.iloc[:, 4] = 20*(data.iloc[:, 4] - data.iloc[:, 4].min()) / (data.iloc[:, 4].max() - data.iloc[:, 4].min())\n",
    "\n",
    "# 识别特征列和目标列\n",
    "category_columns = [col for col in data.columns if col.startswith('类别')]\n",
    "method_column = '制备方法_编码'\n",
    "base_material_column = '基底材料_编码'\n",
    "target_columns = data.columns[-3:].tolist()\n",
    "feature_columns = category_columns + [method_column, base_material_column]\n",
    "data_original = data.copy()\n",
    "\n",
    "# ---------------------- 为不同模型类型创建预处理数据 ----------------------\n",
    "\n",
    "# 1. 线性模型数据 - 将材料类别编码除以1000以减小与0的差距\n",
    "data_linear = data.copy()\n",
    "for col in category_columns:\n",
    "    data_linear[col] = data_linear[col] / 100.0\n",
    "print(\"\\n线性模型数据预处理 - 材料类别列除以1000\")\n",
    "print(f\"处理前范围: {data[category_columns].max().max():.1f}\")\n",
    "print(f\"处理后范围: {data_linear[category_columns].max().max():.4f}\")\n",
    "\n",
    "# 2. 树模型(XGBoost/LightGBM)数据 - 将0替换为NaN\n",
    "data_tree = data.copy()\n",
    "for col in category_columns:\n",
    "    data_tree[col] = data_tree[col].replace(0, np.nan)\n",
    "# 计算缺失值百分比\n",
    "nan_percent = data_tree[category_columns].isna().mean().mean() * 100\n",
    "print(f\"\\n树模型数据预处理 - 将0替换为NaN\")\n",
    "print(f\"缺失值百分比: {nan_percent:.1f}%\")\n",
    "\n",
    "# 3. 神经网络数据 - 添加存在标志\n",
    "data_nn = data.copy()\n",
    "# 为每个类别特征创建存在标志\n",
    "for col in category_columns:\n",
    "    # 创建存在标志列（1表示存在，0表示不存在）\n",
    "    flag_col = f\"{col}_存在标志\"\n",
    "    data_nn[flag_col] = (data_nn[col] > 0).astype(int)\n",
    "\n",
    "# 神经网络特征列（新增了存在标志）\n",
    "nn_feature_columns = feature_columns + [f\"{col}_存在标志\" for col in category_columns]\n",
    "\n",
    "print(f\"\\n神经网络数据预处理 - 添加材料存在标志\")\n",
    "print(f\"原始特征数: {len(feature_columns)}\")\n",
    "print(f\"增加存在标志后特征数: {len(nn_feature_columns)}\")\n",
    "\n",
    "# 打印每种数据处理的效果\n",
    "print(\"\\n数据预处理示例（第一行）:\")\n",
    "print(\"原始数据:\")\n",
    "print(data[category_columns].iloc[0])\n",
    "print(\"\\n线性模型数据:\")\n",
    "print(data_linear[category_columns].iloc[0])\n",
    "print(\"\\n树模型数据:\")\n",
    "print(data_tree[category_columns].iloc[0])\n",
    "print(\"\\n神经网络数据（包含存在标志）:\")\n",
    "print(data_nn[[col for col in data_nn.columns if col.startswith('类别')]].iloc[0])\n",
    "\n",
    "# 为模型类型定义模型组\n",
    "linear_models = [\"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"HuberRegressor\"]\n",
    "tree_models_nan_support = [\"XGBoost\", \"LightGBM\", \"HistGradientBoosting\"]  # 支持NaN的树模型\n",
    "tree_models_no_nan = [\"RandomForest\", \"GradientBoosting\"]  # 不支持NaN的树模型\n",
    "advanced_models = [\"SVR\", \"GaussianProcess\"]  # 高级模型\n",
    "ensemble_models = [\"VotingEnsemble\", \"Stacking\", \"Bagging\"]  # 集成模型\n",
    "nn_models = [\"DeepNN\"]  # 神经网络模型\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# 数据分割函数\n",
    "def create_bins(y, n_bins=5):\n",
    "    \"\"\"将连续变量分箱，用于分层抽样\"\"\"\n",
    "    try:\n",
    "        bins = pd.qcut(y, q=n_bins, duplicates='drop')\n",
    "        return bins\n",
    "    except:\n",
    "        try:\n",
    "            bins = pd.cut(y, bins=n_bins)\n",
    "            return bins\n",
    "        except:\n",
    "            print(f\"为 {len(y)} 个样本创建分层变量失败，使用随机分层\")\n",
    "            return pd.Series(np.random.randint(0, min(5, len(y)//2+1), size=len(y)))\n",
    "\n",
    "# 创建数据分割\n",
    "X_train = {}\n",
    "X_test = {}\n",
    "X_train_linear = {}\n",
    "X_test_linear = {}\n",
    "X_train_tree = {}\n",
    "X_test_tree = {}\n",
    "X_train_nn = {}\n",
    "X_test_nn = {}\n",
    "# 添加填充了NaN的数据版本，用于不支持NaN的树模型\n",
    "X_train_tree_filled = {}\n",
    "X_test_tree_filled = {}\n",
    "y_train = {}\n",
    "y_test = {}\n",
    "\n",
    "# 设置年份分割阈值\n",
    "year_split_threshold = 2024\n",
    "year_column = '年份'  # 确保这里使用正确的年份列名\n",
    "\n",
    "# 检查年份列是否存在\n",
    "if year_column not in data.columns:\n",
    "    print(f\"警告: 找不到年份列 '{year_column}'。如果年份列有其他名称，请相应调整代码。\")\n",
    "    print(\"将退回到随机分割数据。\")\n",
    "    use_year_split = False\n",
    "else:\n",
    "    use_year_split = True\n",
    "    print(f\"使用年份分割: {year_split_threshold}年之前为训练集，{year_split_threshold}年及以后为测试集\")\n",
    "\n",
    "# 仍设置交叉验证，以备不使用年份分割的情况\n",
    "cv_folds = min(5, int(len(data) * 0.1))\n",
    "print(f\"交叉验证设置: {cv_folds} 折\")\n",
    "kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# 为每个目标变量分割数据\n",
    "for target in target_columns:\n",
    "    print(f\"\\n为目标 {target} 准备训练数据...\")\n",
    "    # 准备特征和目标数据\n",
    "    valid_mask = data[target].notna()\n",
    "    \n",
    "    # 原始数据\n",
    "    X_orig = data_original.loc[valid_mask, feature_columns]\n",
    "    y_orig = data_original.loc[valid_mask, target]\n",
    "    \n",
    "    # 线性模型数据\n",
    "    X_lin = data_linear.loc[valid_mask, feature_columns]\n",
    "    \n",
    "    # 树模型数据（带NaN）\n",
    "    X_tree_data = data_tree.loc[valid_mask, feature_columns]\n",
    "    \n",
    "    # 为不支持NaN的树模型创建填充了NaN的数据版本\n",
    "    X_tree_filled_data = X_tree_data.copy()\n",
    "    # 使用0填充NaN值\n",
    "    for col in X_tree_filled_data.columns:\n",
    "        X_tree_filled_data[col] = X_tree_filled_data[col].fillna(0)\n",
    "    \n",
    "    # 神经网络数据\n",
    "    X_nn_data = data_nn.loc[valid_mask, nn_feature_columns]\n",
    "    \n",
    "    # 基于年份或随机分割数据\n",
    "    if use_year_split:\n",
    "        # 获取年份数据\n",
    "        years = data_original.loc[valid_mask, year_column]\n",
    "        \n",
    "        # 创建训练集和测试集掩码\n",
    "        train_mask = years < year_split_threshold\n",
    "        test_mask = ~train_mask\n",
    "        \n",
    "        # 检查训练集和测试集大小\n",
    "        if sum(train_mask) == 0:\n",
    "            print(f\"警告: 没有{year_split_threshold}年之前的数据。使用随机分割。\")\n",
    "            # 退回到随机分割\n",
    "            strat_var = create_bins(y_orig, n_bins=min(3, len(y_orig)//5))\n",
    "            X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "                X_orig, y_orig, test_size=0.2, random_state=42, stratify=strat_var\n",
    "            )\n",
    "        elif sum(test_mask) == 0:\n",
    "            print(f\"警告: 没有{year_split_threshold}年及之后的数据。使用随机分割。\")\n",
    "            # 退回到随机分割\n",
    "            strat_var = create_bins(y_orig, n_bins=min(3, len(y_orig)//5))\n",
    "            X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "                X_orig, y_orig, test_size=0.2, random_state=42, stratify=strat_var\n",
    "            )\n",
    "        else:\n",
    "            # 使用年份分割数据\n",
    "            X_train_orig = X_orig[train_mask]\n",
    "            X_test_orig = X_orig[test_mask]\n",
    "            y_train_orig = y_orig[train_mask]\n",
    "            y_test_orig = y_orig[test_mask]\n",
    "            \n",
    "            print(f\"使用年份分割: 训练集({sum(train_mask)}样本, <{year_split_threshold}年), 测试集({sum(test_mask)}样本, ≥{year_split_threshold}年)\")\n",
    "    else:\n",
    "        # 使用随机分割\n",
    "        strat_var = create_bins(y_orig, n_bins=min(3, len(y_orig)//5))\n",
    "        X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "            X_orig, y_orig, test_size=0.2, random_state=42, stratify=strat_var\n",
    "        )\n",
    "        print(\"使用随机分层抽样分割数据\")\n",
    "    \n",
    "    # 使用相同索引分割预处理后的数据\n",
    "    if use_year_split and sum(train_mask) > 0 and sum(test_mask) > 0:\n",
    "        # 直接使用掩码分割其他数据集\n",
    "        X_train_lin = X_lin[train_mask]\n",
    "        X_test_lin = X_lin[test_mask]\n",
    "        \n",
    "        X_train_tree_data = X_tree_data[train_mask]\n",
    "        X_test_tree_data = X_tree_data[test_mask]\n",
    "        \n",
    "        X_train_tree_filled_data = X_tree_filled_data[train_mask]\n",
    "        X_test_tree_filled_data = X_tree_filled_data[test_mask]\n",
    "        \n",
    "        X_train_nn_data = X_nn_data[train_mask]\n",
    "        X_test_nn_data = X_nn_data[test_mask]\n",
    "    else:\n",
    "        # 使用索引分割其他数据集\n",
    "        X_train_lin = X_lin.loc[X_train_orig.index]\n",
    "        X_test_lin = X_lin.loc[X_test_orig.index]\n",
    "        \n",
    "        X_train_tree_data = X_tree_data.loc[X_train_orig.index]\n",
    "        X_test_tree_data = X_tree_data.loc[X_test_orig.index]\n",
    "        \n",
    "        X_train_tree_filled_data = X_tree_filled_data.loc[X_train_orig.index]\n",
    "        X_test_tree_filled_data = X_tree_filled_data.loc[X_test_orig.index]\n",
    "        \n",
    "        X_train_nn_data = X_nn_data.loc[X_train_orig.index]\n",
    "        X_test_nn_data = X_nn_data.loc[X_test_orig.index]\n",
    "    \n",
    "    # 保存分割的数据\n",
    "    X_train[target] = X_train_orig\n",
    "    X_test[target] = X_test_orig\n",
    "    X_train_linear[target] = X_train_lin\n",
    "    X_test_linear[target] = X_test_lin\n",
    "    X_train_tree[target] = X_train_tree_data\n",
    "    X_test_tree[target] = X_test_tree_data\n",
    "    X_train_tree_filled[target] = X_train_tree_filled_data\n",
    "    X_test_tree_filled[target] = X_test_tree_filled_data\n",
    "    X_train_nn[target] = X_train_nn_data\n",
    "    X_test_nn[target] = X_test_nn_data\n",
    "    y_train[target] = y_train_orig\n",
    "    y_test[target] = y_test_orig\n",
    "    \n",
    "    print(f\"训练集: {len(y_train[target])} 样本, 测试集: {len(y_test[target])} 样本\")\n",
    "    print(f\"训练集目标均值: {y_train[target].mean():.4f}, 测试集目标均值: {y_test[target].mean():.4f}\")\n",
    "\n",
    "# 预处理器存储\n",
    "imputers = {}\n",
    "scalers = {}\n",
    "\n",
    "# 存储训练的模型\n",
    "models = {}\n",
    "for target in target_columns:\n",
    "    models[target] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 保存预处理数据 ======================\n",
    "print(\"=\" * 50)\n",
    "print(\"保存预处理数据\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# 创建数据导出文件夹\n",
    "export_folder = 'data_exports'\n",
    "os.makedirs(export_folder, exist_ok=True)\n",
    "print(f\"创建/检查数据导出文件夹: {export_folder}\")\n",
    "\n",
    "# 保存数据分割结果\n",
    "print(\"\\n保存数据分割结果...\")\n",
    "\n",
    "# 1. 保存原始训练和测试数据\n",
    "with open(os.path.join(export_folder, 'X_train.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "print(\"✓ 已保存 X_train.pkl\")\n",
    "\n",
    "with open(os.path.join(export_folder, 'X_test.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_test, f)\n",
    "print(\"✓ 已保存 X_test.pkl\")\n",
    "\n",
    "# 2. 保存线性模型数据\n",
    "with open(os.path.join(export_folder, 'X_train_linear.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_train_linear, f)\n",
    "print(\"✓ 已保存 X_train_linear.pkl\")\n",
    "\n",
    "with open(os.path.join(export_folder, 'X_test_linear.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_test_linear, f)\n",
    "print(\"✓ 已保存 X_test_linear.pkl\")\n",
    "\n",
    "# 3. 保存树模型数据（支持NaN）\n",
    "with open(os.path.join(export_folder, 'X_train_tree.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_train_tree, f)\n",
    "print(\"✓ 已保存 X_train_tree.pkl\")\n",
    "\n",
    "with open(os.path.join(export_folder, 'X_test_tree.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_test_tree, f)\n",
    "print(\"✓ 已保存 X_test_tree.pkl\")\n",
    "\n",
    "# 4. 保存树模型数据（填充NaN，用于RandomForest等）\n",
    "with open(os.path.join(export_folder, 'X_train_tree_filled.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_train_tree_filled, f)\n",
    "print(\"✓ 已保存 X_train_tree_filled.pkl\")\n",
    "\n",
    "with open(os.path.join(export_folder, 'X_test_tree_filled.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_test_tree_filled, f)\n",
    "print(\"✓ 已保存 X_test_tree_filled.pkl\")\n",
    "\n",
    "# 5. 保存神经网络数据\n",
    "with open(os.path.join(export_folder, 'X_train_nn.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_train_nn, f)\n",
    "print(\"✓ 已保存 X_train_nn.pkl\")\n",
    "\n",
    "with open(os.path.join(export_folder, 'X_test_nn.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_test_nn, f)\n",
    "print(\"✓ 已保存 X_test_nn.pkl\")\n",
    "\n",
    "# 6. 保存目标变量\n",
    "with open(os.path.join(export_folder, 'y_train.pkl'), 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "print(\"✓ 已保存 y_train.pkl\")\n",
    "\n",
    "with open(os.path.join(export_folder, 'y_test.pkl'), 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "print(\"✓ 已保存 y_test.pkl\")\n",
    "\n",
    "# 7. 保存其他有用的数据\n",
    "# 保存目标列名\n",
    "with open(os.path.join(export_folder, 'target_columns.pkl'), 'wb') as f:\n",
    "    pickle.dump(target_columns, f)\n",
    "print(\"✓ 已保存 target_columns.pkl\")\n",
    "\n",
    "# 保存特征列名\n",
    "with open(os.path.join(export_folder, 'feature_columns.pkl'), 'wb') as f:\n",
    "    pickle.dump(feature_columns, f)\n",
    "print(\"✓ 已保存 feature_columns.pkl\")\n",
    "\n",
    "# 保存神经网络特征列名\n",
    "with open(os.path.join(export_folder, 'nn_feature_columns.pkl'), 'wb') as f:\n",
    "    pickle.dump(nn_feature_columns, f)\n",
    "print(\"✓ 已保存 nn_feature_columns.pkl\")\n",
    "\n",
    "# 保存容忍度设置\n",
    "with open(os.path.join(export_folder, 'target_tolerance.pkl'), 'wb') as f:\n",
    "    pickle.dump(target_tolerance, f)\n",
    "print(\"✓ 已保存 target_tolerance.pkl\")\n",
    "\n",
    "# 保存原始数据（可选，如果需要的话）\n",
    "with open(os.path.join(export_folder, 'data_original.pkl'), 'wb') as f:\n",
    "    pickle.dump(data_original, f)\n",
    "print(\"✓ 已保存 data_original.pkl\")\n",
    "\n",
    "# 保存预处理后的数据版本\n",
    "preprocessing_data = {\n",
    "    'data_linear': data_linear,\n",
    "    'data_tree': data_tree,\n",
    "    'data_nn': data_nn\n",
    "}\n",
    "with open(os.path.join(export_folder, 'preprocessing_data.pkl'), 'wb') as f:\n",
    "    pickle.dump(preprocessing_data, f)\n",
    "print(\"✓ 已保存 preprocessing_data.pkl\")\n",
    "\n",
    "# 显示保存的数据统计信息\n",
    "print(f\"\\n数据保存完成! 保存位置: {export_folder}/\")\n",
    "print(\"\\n数据统计信息:\")\n",
    "print(f\"目标变量数量: {len(target_columns)}\")\n",
    "print(f\"目标变量: {target_columns}\")\n",
    "\n",
    "for target in target_columns:\n",
    "    print(f\"\\n{target}:\")\n",
    "    print(f\"  训练集样本数: {len(y_train[target])}\")\n",
    "    print(f\"  测试集样本数: {len(y_test[target])}\")\n",
    "    print(f\"  特征数量 (原始): {len(X_train[target].columns)}\")\n",
    "    print(f\"  特征数量 (线性): {len(X_train_linear[target].columns)}\")\n",
    "    print(f\"  特征数量 (树模型): {len(X_train_tree[target].columns)}\")\n",
    "    print(f\"  特征数量 (神经网络): {len(X_train_nn[target].columns)}\")\n",
    "    print(f\"  训练集目标均值: {y_train[target].mean():.4f}\")\n",
    "    print(f\"  测试集目标均值: {y_test[target].mean():.4f}\")\n",
    "\n",
    "# 验证保存的文件\n",
    "print(f\"\\n保存的文件列表:\")\n",
    "saved_files = os.listdir(export_folder)\n",
    "for file in sorted(saved_files):\n",
    "    file_path = os.path.join(export_folder, file)\n",
    "    file_size = os.path.getsize(file_path) / 1024  # KB\n",
    "    print(f\"  {file} ({file_size:.1f} KB)\")\n",
    "\n",
    "print(f\"\\n总共保存了 {len(saved_files)} 个文件\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 水接触角XGBoost贝叶斯超参数优化\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 创建保存数据的文件夹\n",
    "save_folder = '模型可视化数据'\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "    print(f\"已创建文件夹：{save_folder}\")\n",
    "\n",
    "# 选择目标变量\n",
    "target = '水接触角'\n",
    "print(f\"训练 {target} 的XGBoost模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.03)\n",
    "\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_tree[target]\n",
    "X_test_model = X_test_tree[target]\n",
    "\n",
    "# 从您的代码中复制的完整函数定义\n",
    "def tolerance_r2_score(y_true, y_pred, tolerance=0.15, target=None):\n",
    "    \"\"\"\n",
    "    计算容忍度R²评分，允许一定误差范围内的预测被视为准确\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    \n",
    "    if target and target in target_tolerance:\n",
    "        tolerance = target_tolerance[target]\n",
    "    \n",
    "    tolerance_values = tolerance * np.abs(y_true)\n",
    "    residuals = np.abs(y_true - y_pred)\n",
    "    adjusted_residuals = np.maximum(0, residuals - tolerance_values)\n",
    "    \n",
    "    y_true_mean = np.mean(y_true)\n",
    "    tss = np.sum((y_true - y_true_mean) ** 2)\n",
    "    rss = np.sum(adjusted_residuals ** 2)\n",
    "    \n",
    "    if tss == 0:\n",
    "        return 0\n",
    "    \n",
    "    tolerance_r2 = 1 - (rss / tss)\n",
    "    return tolerance_r2\n",
    "\n",
    "def prediction_within_tolerance(y_true, y_pred, tolerance=0.15, target=None):\n",
    "    \"\"\"\n",
    "    计算预测值在目标值±容忍范围内的比例\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    if target and target in target_tolerance:\n",
    "        tolerance = target_tolerance[target]\n",
    "    \n",
    "    tolerance_values = tolerance * np.abs(y_true)\n",
    "    within_tolerance = np.abs(y_true - y_pred) <= tolerance_values\n",
    "    \n",
    "    return np.mean(within_tolerance)\n",
    "\n",
    "def make_tolerance_scorer(target_name):\n",
    "    def tolerance_score(y_true, y_pred):\n",
    "        tolerance = target_tolerance.get(target_name, 0.03)\n",
    "        relative_errors = np.abs(y_true - y_pred) / np.abs(y_true)\n",
    "        within_tolerance = np.mean(relative_errors <= tolerance)\n",
    "        return within_tolerance\n",
    "    return tolerance_score\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, target, model_name):\n",
    "    \"\"\"\n",
    "    评估模型在训练集和测试集上的性能，包括标准R²和容忍度R²\n",
    "    \"\"\"\n",
    "    current_tolerance = target_tolerance.get(target, 0.15)\n",
    "    \n",
    "    # 在训练集上评估\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    if len(y_train_pred.shape) > 1 and y_train_pred.shape[1] == 1:\n",
    "        y_train_pred = y_train_pred.flatten()\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    train_tol_r2 = tolerance_r2_score(y_train, y_train_pred, tolerance=current_tolerance, target=target)\n",
    "    train_within_tol = prediction_within_tolerance(y_train, y_train_pred, tolerance=current_tolerance, target=target)\n",
    "    \n",
    "    # 在测试集上评估\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    if len(y_test_pred.shape) > 1 and y_test_pred.shape[1] == 1:\n",
    "        y_test_pred = y_test_pred.flatten()\n",
    "        \n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_tol_r2 = tolerance_r2_score(y_test, y_test_pred, tolerance=current_tolerance, target=target)\n",
    "    test_within_tol = prediction_within_tolerance(y_test, y_test_pred, tolerance=current_tolerance, target=target)\n",
    "    \n",
    "    print(f\"\\n{model_name} 在 {target} 上的评估结果:\")\n",
    "    print(f\"训练集: R²={train_r2:.4f}, 容忍度R²={train_tol_r2:.4f}, 在容忍范围内比例={train_within_tol:.2%}\")\n",
    "    print(f\"测试集: R²={test_r2:.4f}, 容忍度R²={test_tol_r2:.4f}, 在容忍范围内比例={test_within_tol:.2%}\")\n",
    "    \n",
    "    # 绘制预测值与实际值的对比散点图\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 训练集散点图\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_train, y_train_pred, alpha=0.6, s=30)\n",
    "    min_val = min(min(y_train), min(y_train_pred))\n",
    "    max_val = max(max(y_train), max(y_train_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "    plt.xlabel('实际值')\n",
    "    plt.ylabel('预测值')\n",
    "    plt.title(f'训练集: R²={train_r2:.4f}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 测试集散点图\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_test, y_test_pred, alpha=0.6, s=30)\n",
    "    min_val = min(min(y_test), min(y_test_pred))\n",
    "    max_val = max(max(y_test), max(y_test_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "    plt.xlabel('实际值')\n",
    "    plt.ylabel('预测值')\n",
    "    plt.title(f'测试集: R²={test_r2:.4f}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'train_r2': train_r2,\n",
    "        'train_tol_r2': train_tol_r2,\n",
    "        'train_within_tol': train_within_tol,\n",
    "        'test_r2': test_r2,\n",
    "        'test_tol_r2': test_tol_r2,\n",
    "        'test_within_tol': test_within_tol\n",
    "    }\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# 设置基础参数\n",
    "base_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'hist',\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42,\n",
    "    'missing': np.nan\n",
    "}\n",
    "\n",
    "# 定义贝叶斯优化的搜索空间（基于您原始代码的参数范围）\n",
    "dimensions = [\n",
    "    Integer(50, 100, name='n_estimators'),           # 您原始代码：[100, 50, 90, 80]\n",
    "    Real(0.5, 0.8, name='learning_rate'),            # 您原始代码：[0.7, 0.8, 0.6, 0.5]\n",
    "    Integer(3, 6, name='max_depth'),                 # 您原始代码：[4, 5, 6, 3]\n",
    "    Integer(2, 6, name='min_child_weight'),          # 您原始代码：[5, 4, 6, 3, 2]\n",
    "    Real(0.0, 0.2, name='gamma'),                    # 您原始代码：[0, 0.1, 0.2]\n",
    "    Real(0.5, 0.6, name='subsample'),               # 您原始代码：[0.6, 0.5]\n",
    "    Real(0.8, 1.0, name='colsample_bytree'),        # 您原始代码：[0.9, 1.0, 0.8]\n",
    "    Real(0.0, 1.0, name='reg_alpha'),               # 您原始代码：[0, 0.4, 0.5, 0.6, 1.0]\n",
    "    Real(0.5, 1.0, name='reg_lambda')               # 您原始代码：[1.0, 0.5, 0.7, 0.8]\n",
    "]\n",
    "\n",
    "# 交叉验证设置\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 定义目标函数\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def objective(**params):\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=params['max_depth'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        gamma=params['gamma'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        **base_params\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        cv_scores = cross_val_score(\n",
    "            model, X_train_model, y_train[target], \n",
    "            cv=kf, scoring=tol_scorer_wrapped\n",
    "        )\n",
    "        return -cv_scores.mean()\n",
    "    except:\n",
    "        return 1.0\n",
    "\n",
    "# 执行贝叶斯优化\n",
    "print(\"执行贝叶斯优化...\")\n",
    "result = gp_minimize(\n",
    "    func=objective,\n",
    "    dimensions=dimensions,\n",
    "    n_calls=50,\n",
    "    n_initial_points=10,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 获取最佳参数\n",
    "best_params = dict(zip([dim.name for dim in dimensions], result.x))\n",
    "print(f\"最佳参数: {best_params}\")\n",
    "print(f\"最佳CV得分: {-result.fun:.4f}\")\n",
    "\n",
    "# 使用最佳参数创建最终模型\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_child_weight=best_params['min_child_weight'],\n",
    "    gamma=best_params['gamma'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree'],\n",
    "    reg_alpha=best_params['reg_alpha'],\n",
    "    reg_lambda=best_params['reg_lambda'],\n",
    "    **base_params\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_model, y_train[target])\n",
    "\n",
    "# 交叉验证\n",
    "cv_scores = cross_val_score(xgb_model, X_train_model, y_train[target], cv=kf, scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    xgb_model, X_train_model, y_train[target], cv=kf, \n",
    "    scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "models[target]['XGBoost'] = xgb_model\n",
    "\n",
    "# 创建模型保存文件夹\n",
    "model_folder = '训练模型文件'\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    print(f\"已创建模型文件夹：{model_folder}\")\n",
    "\n",
    "xgb_model_file = os.path.join(model_folder, f'{target}_XGBoost模型.pkl')\n",
    "with open(xgb_model_file, 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "print(f\"XGBoost 模型已保存至 {xgb_model_file}\")\n",
    "\n",
    "# 评估模型\n",
    "results = evaluate_model(xgb_model, X_train_model, y_train[target], X_test_model, y_test[target], target, \"XGBoost\")\n",
    "\n",
    "# 获取预测值\n",
    "y_pred_train = xgb_model.predict(X_train_model)\n",
    "y_pred_test = xgb_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到单独的文件\n",
    "train_file = os.path.join(save_folder, f'{target}_XGBoost训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_XGBoost测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "# 特征重要性\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - XGBoost特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_XGBoost特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "\n",
    "# 绘制优化收敛图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_convergence(result)\n",
    "plt.title('贝叶斯优化收敛过程')\n",
    "plt.show()\n",
    "\n",
    "# 保存优化结果\n",
    "optimization_history = pd.DataFrame({\n",
    "    '迭代次数': range(1, len(result.func_vals) + 1),\n",
    "    '目标函数值': result.func_vals,\n",
    "    '最佳目标函数值': [min(result.func_vals[:i+1]) for i in range(len(result.func_vals))]\n",
    "})\n",
    "\n",
    "param_names = [dim.name for dim in dimensions]\n",
    "for i, param_name in enumerate(param_names):\n",
    "    optimization_history[f'参数_{param_name}'] = [x[i] for x in result.x_iters]\n",
    "\n",
    "optimization_history_file = os.path.join(save_folder, f'{target}_贝叶斯优化历史.csv')\n",
    "optimization_history.to_csv(optimization_history_file, index=False)\n",
    "print(f\"优化历史数据已保存至 {optimization_history_file}\")\n",
    "\n",
    "print(f\"\\n贝叶斯优化完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#水接触角xgboost\n",
    "from xgboost import XGBRegressor\n",
    "import os\n",
    "\n",
    "# 创建保存数据的文件夹\n",
    "save_folder = '模型可视化数据'\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "    print(f\"已创建文件夹：{save_folder}\")\n",
    "# 选择目标变量\n",
    "target = '水接触角'\n",
    "print(f\"训练 {target} 的XGBoost模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.03)\n",
    "\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_tree[target]\n",
    "X_test_model = X_test_tree[target]\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# 设置基础参数\n",
    "base_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'hist',\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42,\n",
    "    'missing': np.nan\n",
    "}\n",
    "\n",
    "# 设置超参数\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 50, 90,80],  \n",
    "    'learning_rate': [0.7,0.8,0.6,0.5],\n",
    "    'max_depth': [ 4, 5,6,3],\n",
    "    'min_child_weight': [ 5,4,6,3,2],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.5],#1\n",
    "    'colsample_bytree': [0.9, 1.0, 0.8],#1\n",
    "    'reg_alpha': [0, 0.4,0.5, 0.6,1.0],#1\n",
    "    'reg_lambda': [1.0,0.5,0.7,0.8]\n",
    "}\n",
    "# 创建基础模型\n",
    "base_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1.0,\n",
    "    **base_params\n",
    ")\n",
    "\n",
    "# 执行超参数优化\n",
    "print(\"执行超参数优化...\")\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=1000,\n",
    "    cv=kf,\n",
    "    scoring=tol_scorer_wrapped,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(X_train_model, y_train[target])\n",
    "xgb_model = search.best_estimator_\n",
    "print(f\"最佳参数: {search.best_params_}\")\n",
    "print(f\"最佳CV得分: {search.best_score_:.4f}\")\n",
    "# 交叉验证\n",
    "cv_scores = cross_val_score(xgb_model, X_train_model, y_train[target], cv=kf, scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    xgb_model, X_train_model, y_train[target], cv=kf, \n",
    "    scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "# 保存模型\n",
    "models[target]['XGBoost'] = xgb_model\n",
    "# 创建模型保存文件夹\n",
    "model_folder = '训练模型文件'\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    print(f\"已创建模型文件夹：{model_folder}\")\n",
    "\n",
    "xgb_model_file = os.path.join(model_folder, f'{target}_XGBoost模型.pkl')\n",
    "with open(xgb_model_file, 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "print(f\"XGBoost 模型已保存至 {xgb_model_file}\")\n",
    "# 评估模型\n",
    "results = evaluate_model(xgb_model, X_train_model, y_train[target], X_test_model, y_test[target], target, \"XGBoost\")\n",
    "# 获取预测值\n",
    "y_pred_train = xgb_model.predict(X_train_model)\n",
    "y_pred_test = xgb_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到单独的文件\n",
    "train_file = os.path.join(save_folder, f'{target}_XGBoost训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_XGBoost测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "# 特征重要性\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - XGBoost特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_XGBoost特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "\n",
    "# 绘制训练过程中的损失曲线\n",
    "eval_set = [(X_train_model, y_train[target]), (X_test_model, y_test[target])]\n",
    "model_train = XGBRegressor(**{**base_model.get_params(), 'eval_metric': 'rmse'})\n",
    "model_train.fit(X_train_model, y_train[target], eval_set=eval_set, verbose=False)\n",
    "\n",
    "results = model_train.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_axis, results['validation_0']['rmse'], label='训练集')\n",
    "plt.plot(x_axis, results['validation_1']['rmse'], label='测试集')\n",
    "plt.legend()\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('迭代次数')\n",
    "plt.title('XGBoost训练进度')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# 保存训练进度数据\n",
    "training_progress_data = pd.DataFrame({\n",
    "    '迭代次数': x_axis,\n",
    "    '训练集RMSE': results['validation_0']['rmse'],\n",
    "    '测试集RMSE': results['validation_1']['rmse']\n",
    "})\n",
    "training_progress_file = os.path.join(save_folder, f'{target}_XGBoost训练进度.csv')\n",
    "training_progress_data.to_csv(training_progress_file, index=False)\n",
    "print(f\"训练进度数据已保存至 {training_progress_file}\")\n",
    "# 学习率影响分析\n",
    "learning_rates = [0.005, 0.01, 0.03, 0.05, 0.1, 0.2]\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 创建用于保存学习率分析数据的DataFrame\n",
    "lr_analysis_data = pd.DataFrame()\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = XGBRegressor(\n",
    "        learning_rate=lr,\n",
    "        n_estimators=500,\n",
    "        max_depth=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        tree_method='hist',\n",
    "        random_state=42\n",
    "    )\n",
    "    eval_set = [(X_test_model, y_test[target])]\n",
    "    model.fit(X_train_model, y_train[target], eval_set=eval_set, verbose=False)\n",
    "    results = model.evals_result()\n",
    "    \n",
    "    # 将当前学习率的结果添加到DataFrame\n",
    "    temp_df = pd.DataFrame({\n",
    "        '迭代次数': range(len(results['validation_0']['rmse'])),\n",
    "        f'学习率_{lr}': results['validation_0']['rmse']\n",
    "    })\n",
    "    \n",
    "    if lr_analysis_data.empty:\n",
    "        lr_analysis_data = temp_df\n",
    "    else:\n",
    "        lr_analysis_data = pd.merge(\n",
    "            lr_analysis_data, temp_df, on='迭代次数', how='outer'\n",
    "        )\n",
    "    \n",
    "    plt.plot(results['validation_0']['rmse'], label=f'学习率: {lr}')\n",
    "\n",
    "# 保存学习率分析数据\n",
    "lr_analysis_file = os.path.join(save_folder, f'{target}_XGBoost学习率分析.csv')\n",
    "lr_analysis_data.to_csv(lr_analysis_file, index=False)\n",
    "print(f\"学习率分析数据已保存至 {lr_analysis_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightGBM with Hyperparameter Optimization\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "print(f\"训练 {target} 的LightGBM模型...\")\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.03)\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_tree[target]\n",
    "X_test_model = X_test_tree[target]\n",
    "\n",
    "# 设置基础参数\n",
    "base_params = {\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'rmse',\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# 您当前的最佳参数配置（作为基准参考）\n",
    "current_best_params = {\n",
    "    'n_estimators': 8000,\n",
    "    'learning_rate': 0.001,\n",
    "    'num_leaves': 20,\n",
    "    'max_depth': 11,\n",
    "    'min_child_samples': 1,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    'reg_alpha': 10,\n",
    "    'reg_lambda': 1.0,\n",
    "}\n",
    "\n",
    "# 定义手动交叉验证函数（计算R²分数）\n",
    "def manual_cross_validation_r2(params, X_data, y_data, cv_folds=5):\n",
    "    \"\"\"手动实现交叉验证计算R²分数\"\"\"\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.metrics import r2_score\n",
    "    \n",
    "    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    cv_r2_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_data):\n",
    "        # 分割训练和验证数据\n",
    "        if hasattr(X_data, 'iloc'):\n",
    "            X_train_fold = X_data.iloc[train_idx]\n",
    "            X_val_fold = X_data.iloc[val_idx]\n",
    "            y_train_fold = y_data.iloc[train_idx]\n",
    "            y_val_fold = y_data.iloc[val_idx]\n",
    "        else:\n",
    "            X_train_fold = X_data[train_idx]\n",
    "            X_val_fold = X_data[val_idx]\n",
    "            y_train_fold = y_data[train_idx]\n",
    "            y_val_fold = y_data[val_idx]\n",
    "        \n",
    "        # 创建并训练模型\n",
    "        fold_model = CustomLGBMRegressor(**params)\n",
    "        fold_model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # 预测并计算R²分数\n",
    "        y_pred = fold_model.predict(X_val_fold)\n",
    "        r2 = r2_score(y_val_fold, y_pred)\n",
    "        cv_r2_scores.append(r2)\n",
    "    \n",
    "    return np.mean(cv_r2_scores)\n",
    "\n",
    "# 首先评估您当前参数的基准性能\n",
    "print(\"评估当前参数的基准性能...\")\n",
    "baseline_params = {**current_best_params, **base_params}\n",
    "baseline_r2 = manual_cross_validation_r2(baseline_params, X_train_model, y_train[target])\n",
    "print(f\"当前参数的交叉验证R²: {baseline_r2:.6f}\")\n",
    "\n",
    "# 定义超参数优化的目标函数（最大化R²分数）\n",
    "def objective(trial):\n",
    "    # 定义参数搜索空间（在您的最佳参数附近进行更精细的调整）\n",
    "    params = {\n",
    "        # n_estimators: 在8000附近进行更小范围搜索\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 7000, 9000, step=250),\n",
    "        \n",
    "        # learning_rate: 在0.001附近进行更精细搜索\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0008, 0.0015),\n",
    "        \n",
    "        # num_leaves: 在20附近进行小范围搜索\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 18, 25),\n",
    "        \n",
    "        # max_depth: 在11附近进行小范围搜索\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 13),\n",
    "        \n",
    "        # min_child_samples: 在1附近搜索\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 3),\n",
    "        \n",
    "        # subsample: 在1.0附近进行微调\n",
    "        'subsample': trial.suggest_float('subsample', 0.95, 1.0),\n",
    "        \n",
    "        # colsample_bytree: 在1.0附近进行微调\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.95, 1.0),\n",
    "        \n",
    "        # reg_alpha: 在10附近进行精细调整\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 8.0, 12.0),\n",
    "        \n",
    "        # reg_lambda: 在1.0附近进行精细调整\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.8, 1.5),\n",
    "        \n",
    "        **base_params\n",
    "    }\n",
    "    \n",
    "    # 使用手动交叉验证评估R²分数\n",
    "    cv_r2 = manual_cross_validation_r2(params, X_train_model, y_train[target])\n",
    "    \n",
    "    # 返回R²分数（Optuna将最大化此值）\n",
    "    return cv_r2\n",
    "\n",
    "# 执行超参数优化\n",
    "print(\"开始超参数优化...\")\n",
    "print(\"这可能需要一些时间，请耐心等待...\")\n",
    "\n",
    "# 创建研究对象（最大化R²分数）\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',  # 改为最大化R²分数\n",
    "    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# 执行优化（您可以根据需要调整n_trials的数量）\n",
    "study.optimize(objective, n_trials=50, timeout=1800)  # 减少试验次数，聚焦精细优化\n",
    "\n",
    "# 获取最优参数\n",
    "best_params = study.best_params\n",
    "best_r2_score = study.best_value\n",
    "\n",
    "print(f\"超参数优化完成!\")\n",
    "print(f\"基准R²分数: {baseline_r2:.6f}\")\n",
    "print(f\"最佳交叉验证R²: {best_r2_score:.6f}\")\n",
    "print(f\"R²提升幅度: {(best_r2_score - baseline_r2):.6f}\")\n",
    "\n",
    "# 只有当优化结果确实更好时才使用新参数\n",
    "if best_r2_score > baseline_r2:\n",
    "    print(\"优化成功！使用新的参数配置\")\n",
    "    print(\"最优参数配置:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    lgb_params = {**best_params, **base_params}\n",
    "else:\n",
    "    print(\"优化未能改善性能，保持原有参数配置\")\n",
    "    lgb_params = baseline_params\n",
    "    best_params = current_best_params\n",
    "    best_r2_score = baseline_r2\n",
    "\n",
    "print(\"使用最优参数训练最终模型...\")\n",
    "print(\"使用自定义LightGBM包装器训练模型\")\n",
    "\n",
    "# 确保保存当前使用的特征列\n",
    "feature_cols = X_train_model.columns.tolist() if hasattr(X_train_model, 'columns') else None\n",
    "\n",
    "# 创建并训练模型（使用优化后的参数）\n",
    "lgb_model = CustomLGBMRegressor(**lgb_params)\n",
    "lgb_model.fit(X_train_model, y_train[target])\n",
    "\n",
    "# 保存模型\n",
    "models[target]['LightGBM'] = lgb_model\n",
    "# 创建模型保存文件夹（如果已存在则不会重复创建）\n",
    "model_folder = '训练模型文件'\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    print(f\"已创建模型文件夹：{model_folder}\")\n",
    "\n",
    "lgb_model_file = os.path.join(model_folder, f'{target}_LightGBM模型.pkl')\n",
    "with open(lgb_model_file, 'wb') as f:\n",
    "    pickle.dump(lgb_model, f)\n",
    "print(f\"LightGBM 模型已保存至 {lgb_model_file}\")\n",
    "\n",
    "# 保存最优参数配置\n",
    "params_file = os.path.join(model_folder, f'{target}_LightGBM最优参数.txt')\n",
    "with open(params_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"目标变量: {target}\\n\")\n",
    "    f.write(f\"基准R²分数: {baseline_r2:.6f}\\n\")\n",
    "    f.write(f\"最佳交叉验证R²: {best_r2_score:.6f}\\n\")\n",
    "    f.write(f\"R²提升幅度: {(best_r2_score - baseline_r2):.6f}\\n\")\n",
    "    f.write(f\"优化试验次数: {len(study.trials)}\\n\\n\")\n",
    "    f.write(\"使用的参数配置:\\n\")\n",
    "    for param, value in best_params.items():\n",
    "        f.write(f\"{param}: {value}\\n\")\n",
    "print(f\"参数配置已保存至 {params_file}\")\n",
    "\n",
    "# 评估模型\n",
    "results = evaluate_model(lgb_model, X_train_model, y_train[target], X_test_model, y_test[target], target, \"LightGBM\")\n",
    "\n",
    "print(\"LightGBM模型训练成功\")\n",
    "# 获取预测值\n",
    "y_pred_train = lgb_model.predict(X_train_model)\n",
    "y_pred_test = lgb_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到单独的文件\n",
    "train_file = os.path.join(save_folder, f'{target}_LightGBM训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_LightGBM测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "# 特征重要性可视化\n",
    "if hasattr(lgb_model.model, 'feature_importance') and feature_cols is not None:\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': lgb_model.model.feature_importance(importance_type='gain')\n",
    "    })\n",
    "    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "    plt.xlabel('增益重要性')\n",
    "    plt.ylabel('特征')\n",
    "    plt.title(f'{target} - LightGBM特征重要性 (优化后参数)')\n",
    "    plt.grid(True, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_LightGBM特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "\n",
    "# 可选：可视化优化过程\n",
    "print(\"\\n生成超参数优化可视化图表...\")\n",
    "try:\n",
    "    from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "    \n",
    "    # 优化历史\n",
    "    fig_history = plot_optimization_history(study)\n",
    "    fig_history.write_html(os.path.join(model_folder, f'{target}_优化历史.html'))\n",
    "    \n",
    "    # 参数重要性\n",
    "    fig_importance = plot_param_importances(study)\n",
    "    fig_importance.write_html(os.path.join(model_folder, f'{target}_参数重要性.html'))\n",
    "    \n",
    "    print(f\"优化可视化图表已保存至 {model_folder}\")\n",
    "except ImportError:\n",
    "    print(\"未安装plotly，跳过可视化生成\")\n",
    "\n",
    "# 添加预测不准确样本分析\n",
    "# 获取测试集预测值\n",
    "y_pred = lgb_model.predict(X_test_model)\n",
    "\n",
    "# 将y_test转换为numpy数组格式进行处理\n",
    "if hasattr(y_test[target], 'values'):\n",
    "    y_true_values = y_test[target].values\n",
    "else:\n",
    "    y_true_values = y_test[target]\n",
    "\n",
    "# 计算绝对误差\n",
    "errors = np.abs(y_true_values - y_pred)\n",
    "\n",
    "# 设置容忍度阈值\n",
    "tolerance = 5.0  # 可以根据需要调整\n",
    "\n",
    "# 找出误差超过容忍度的样本\n",
    "inaccurate_mask = errors > tolerance\n",
    "inaccurate_indices = np.where(inaccurate_mask)[0]\n",
    "\n",
    "print(f\"\\n预测不准确的样本数量: {len(inaccurate_indices)} (占测试集的 {len(inaccurate_indices)/len(y_test)*100:.2f}%)\")\n",
    "print(f\"使用的容忍度阈值: {tolerance}\")\n",
    "\n",
    "# 创建预测不准确样本的分析数据\n",
    "if len(inaccurate_indices) > 0:\n",
    "    # 尝试获取原始索引，如果不可用则使用数组位置索引\n",
    "    try:\n",
    "        if hasattr(y_test, 'index'):\n",
    "            original_indices = [y_test.index[i] for i in inaccurate_indices]\n",
    "        elif isinstance(X_test_model, pd.DataFrame) and hasattr(X_test_model, 'index'):\n",
    "            original_indices = [X_test_model.index[i] for i in inaccurate_indices]\n",
    "        else:\n",
    "            # 如果无法获取原始索引，使用数组位置作为标识\n",
    "            original_indices = inaccurate_indices\n",
    "    except Exception as e:\n",
    "        print(f\"无法获取原始索引: {str(e)}\")\n",
    "        original_indices = inaccurate_indices\n",
    "    \n",
    "    # 创建包含预测不准确样本信息的DataFrame\n",
    "    inaccurate_samples = []\n",
    "    for i, idx in enumerate(inaccurate_indices):\n",
    "        # 安全地获取实际值\n",
    "        if hasattr(y_test[target], 'iloc'):\n",
    "            actual = y_test[target].iloc[idx]\n",
    "        else:\n",
    "            actual = y_true_values[idx]\n",
    "        \n",
    "        inaccurate_samples.append({\n",
    "            '样本索引': original_indices[i],\n",
    "            '实际值': actual,\n",
    "            '预测值': y_pred[idx],\n",
    "            '绝对误差': errors[idx],\n",
    "            '相对误差(%)': (errors[idx] / np.abs(actual)) * 100 if actual != 0 else float('inf')\n",
    "        })\n",
    "    \n",
    "    inaccurate_df = pd.DataFrame(inaccurate_samples)\n",
    "    # 按误差降序排列\n",
    "    inaccurate_df = inaccurate_df.sort_values('绝对误差', ascending=False)\n",
    "    \n",
    "    # 打印预测不准确的样本信息\n",
    "    print(\"\\n预测不准确的样本详情 (按误差降序排列):\")\n",
    "    print(inaccurate_df)\n",
    "    \n",
    "    # 保存结果到文件\n",
    "    inaccurate_df.to_csv(f'{target}_不准确预测.csv', index=False)\n",
    "else:\n",
    "    print(f\"\\n没有发现预测不准确的样本 (容忍度阈值: {tolerance})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HistGradientBoosting\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "print(f\"训练 {target} 的HistGradientBoosting模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.03)\n",
    "\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_tree[target]\n",
    "X_test_model = X_test_tree[target]\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# 设置超参数\n",
    "param_dist = {\n",
    "    'max_iter': [450, 400,350,440,460],\n",
    "    'learning_rate': [0.01, 0.008, 0.011,0.009],\n",
    "    'max_depth': [9, 11, 10,8,7,6,5],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'l2_regularization': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# 创建基础模型\n",
    "base_model = HistGradientBoostingRegressor(\n",
    "    max_iter=3000,\n",
    "    learning_rate=1,\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=4,\n",
    "    l2_regularization=0.5,\n",
    "    loss='squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 执行超参数优化\n",
    "if len(X_train_model) >= 90:\n",
    "    print(\"执行超参数优化...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=2500,\n",
    "        cv=kf,\n",
    "        scoring=tol_scorer_wrapped,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train_model, y_train[target])\n",
    "    hgb_model = search.best_estimator_\n",
    "    print(f\"最佳参数: {search.best_params_}\")\n",
    "    print(f\"最佳CV得分: {search.best_score_:.4f}\")\n",
    "else:\n",
    "    # 使用预定义的参数\n",
    "    print(\"使用预定义参数...\")\n",
    "    hgb_model = base_model\n",
    "    hgb_model.fit(X_train_model, y_train[target])\n",
    "\n",
    "# 交叉验证\n",
    "cv_scores = cross_val_score(hgb_model, X_train_model, y_train[target], cv=kf, scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    hgb_model, X_train_model, y_train[target], cv=kf, \n",
    "    scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "models[target]['HistGradientBoosting'] = hgb_model\n",
    "# 使用pickle保存HistGradientBoosting模型\n",
    "hgb_model_file = os.path.join(model_folder, f'{target}_HistGradientBoosting模型.pkl')\n",
    "with open(hgb_model_file, 'wb') as f:\n",
    "    pickle.dump(hgb_model, f)\n",
    "print(f\"HistGradientBoosting模型已保存至 {hgb_model_file}\")\n",
    "# 评估模型\n",
    "results = evaluate_model(hgb_model, X_train_model, y_train[target], X_test_model, y_test[target], target, \"HistGradientBoosting\")\n",
    "# 获取预测值\n",
    "y_pred_train = hgb_model.predict(X_train_model)\n",
    "y_pred_test = hgb_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到文件\n",
    "train_file = os.path.join(save_folder, f'{target}_HistGradientBoosting训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_HistGradientBoosting测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "# 由于HistGradientBoosting不直接提供特征重要性，使用permutation importance评估\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(\n",
    "    hgb_model, X_test_model, y_test[target], \n",
    "    n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('置换重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - HistGradientBoosting特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_HistGradientBoosting特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "# 不同学习率和迭代次数的影响分析\n",
    "learning_rates = [0.01, 0.008, 0.011,0.009]\n",
    "max_iters = [50, 100, 200, 300]\n",
    "fig, axs = plt.subplots(len(learning_rates), 1, figsize=(10, 4*len(learning_rates)), sharex=True)\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    for iter_count in max_iters:\n",
    "        model = HistGradientBoostingRegressor(\n",
    "            max_iter=iter_count,\n",
    "            learning_rate=lr,\n",
    "            max_depth=3,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train_model, y_train[target])\n",
    "        train_score = r2_score(y_train[target], model.predict(X_train_model))\n",
    "        test_score = r2_score(y_test[target], model.predict(X_test_model))\n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "    \n",
    "    axs[i].plot(max_iters, train_scores, 'o-', label='训练集 R²')\n",
    "    axs[i].plot(max_iters, test_scores, 'o-', label='测试集 R²')\n",
    "    axs[i].set_title(f'学习率 = {lr}')\n",
    "    axs[i].set_ylabel('R²')\n",
    "    axs[i].grid(True)\n",
    "    axs[i].legend()\n",
    "plt.xlabel('迭代次数')\n",
    "plt.suptitle('HistGradientBoosting - 学习率和迭代次数影响')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 准备保存学习率和迭代次数影响分析数据\n",
    "analysis_data = []\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    for j, iter_count in enumerate(max_iters):\n",
    "        analysis_data.append({\n",
    "            '学习率': lr,\n",
    "            '迭代次数': iter_count,\n",
    "            '训练集R²': train_scores[j],\n",
    "            '测试集R²': test_scores[j]\n",
    "        })\n",
    "\n",
    "# 转换为DataFrame并保存\n",
    "lr_iter_analysis = pd.DataFrame(analysis_data)\n",
    "lr_analysis_file = os.path.join(save_folder, f'{target}_HistGradientBoosting学习率迭代分析.csv')\n",
    "lr_iter_analysis.to_csv(lr_analysis_file, index=False)\n",
    "print(f\"学习率和迭代次数分析数据已保存至 {lr_analysis_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# 选择目标变量\n",
    "print(f\"训练 {target} 的随机森林模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.03)\n",
    "\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_tree_filled[target]\n",
    "X_test_model = X_test_tree_filled[target]\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# 设置超参数最佳参数: {'n_estimators': 900, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_depth': 4}\n",
    "param_dist = {\n",
    "    'n_estimators': [ 900,1000,800],\n",
    "    'max_depth': [4, 5, 6,2,3,7],\n",
    "    'min_samples_split': [ 5,4,6,7,8],\n",
    "    'min_samples_leaf': [7, 4,8,9,5,6],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# 创建基础模型\n",
    "base_model = RandomForestRegressor(\n",
    "    n_estimators=800,\n",
    "    max_depth=None,\n",
    "    min_samples_split=3,\n",
    "    min_samples_leaf=4,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 执行超参数优化\n",
    "if len(X_train_model) >= 90:\n",
    "    print(\"执行超参数优化...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=500,#变大后训练集R方变大但是时间可能很长2000/0.3569；4000/0.2091；3000/0.1221；1000/0.1577\n",
    "        cv=kf,\n",
    "        scoring=tol_scorer_wrapped,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train_model, y_train[target])\n",
    "    rf_model = search.best_estimator_\n",
    "    print(f\"最佳参数: {search.best_params_}\")\n",
    "    print(f\"最佳CV得分: {search.best_score_:.4f}\")\n",
    "else:\n",
    "    # 使用预定义的参数\n",
    "    print(\"使用预定义参数...\")\n",
    "    rf_model = base_model\n",
    "    rf_model.fit(X_train_model, y_train[target])\n",
    "\n",
    "# 交叉验证\n",
    "cv_scores = cross_val_score(rf_model, X_train_model, y_train[target], cv=kf, scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    rf_model, X_train_model, y_train[target], cv=kf, \n",
    "    scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "\n",
    "# 如果模型有oob_score属性，输出oob分数\n",
    "if hasattr(rf_model, 'oob_score_'):\n",
    "    print(f\"袋外评分 (OOB score): {rf_model.oob_score_:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "models[target]['RandomForest'] = rf_model\n",
    "# 使用pickle保存随机森林模型\n",
    "rf_model_file = os.path.join(model_folder, f'{target}_随机森林模型.pkl')\n",
    "with open(rf_model_file, 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "print(f\"随机森林模型已保存至 {rf_model_file}\")\n",
    "# 评估模型\n",
    "results = evaluate_model(rf_model, X_train_model, y_train[target], X_test_model, y_test[target], target, \"RandomForest\")\n",
    "# 获取预测值\n",
    "y_pred_train = rf_model.predict(X_train_model)\n",
    "y_pred_test = rf_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到文件\n",
    "train_file = os.path.join(save_folder, f'{target}_随机森林训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_随机森林测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "# 特征重要性\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - RandomForest特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_随机森林特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "# 不同参数组合的影响\n",
    "n_estimators_range = [10, 50, 100, 200, 300, 400,800,1000]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "oob_scores = []\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=n_est,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=4,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf.fit(X_train_model, y_train[target])\n",
    "    train_scores.append(r2_score(y_train[target], rf.predict(X_train_model)))\n",
    "    test_scores.append(r2_score(y_test[target], rf.predict(X_test_model)))\n",
    "    oob_scores.append(rf.oob_score_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators_range, train_scores, 'o-', label='训练集 R²')\n",
    "plt.plot(n_estimators_range, test_scores, 'o-', label='测试集 R²')\n",
    "plt.plot(n_estimators_range, oob_scores, 'o-', label='OOB R²')\n",
    "plt.xlabel('树的数量')\n",
    "plt.ylabel('R²')\n",
    "plt.title('RandomForest - 树数量对性能的影响')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# 保存树数量影响分析数据\n",
    "trees_analysis_data = pd.DataFrame({\n",
    "    '树的数量': n_estimators_range,\n",
    "    '训练集R²': train_scores,\n",
    "    '测试集R²': test_scores,\n",
    "    '袋外评分': oob_scores\n",
    "})\n",
    "trees_analysis_file = os.path.join(save_folder, f'{target}_随机森林树数量分析.csv')\n",
    "trees_analysis_data.to_csv(trees_analysis_file, index=False)\n",
    "print(f\"树数量影响分析数据已保存至 {trees_analysis_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C, Matern\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import traceback\n",
    "\n",
    "# 设置Matplotlib正常显示中文\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']  # 'SimHei' 是黑体\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题\n",
    "\n",
    "\n",
    "# --- 代码开始 ---\n",
    "target=\"水接触角\"\n",
    "print(f\"训练 {target} 的高斯过程回归模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 选择数据集\n",
    "X_train_model = X_train_linear[target]\n",
    "X_test_model = X_test_linear[target]\n",
    "\n",
    "# 步骤1: 对输入特征进行标准化\n",
    "print(\"\\n步骤1: 对输入特征进行标准化...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_model)\n",
    "X_test_scaled = scaler.transform(X_test_model)\n",
    "print(\"特征标准化完成。\")\n",
    "\n",
    "# 步骤2: 扩展核函数库并进行自动化选择与优化 (核心修改)\n",
    "print(\"\\n步骤2: 扩展核函数库，进行更全面的自动化模型选择...\")\n",
    "\n",
    "# 定义一个更丰富的、带优化边界的核函数字典\n",
    "kernels_to_try = {\n",
    "    \"RBF\": \n",
    "        C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) \n",
    "        + WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-10, 1e1)),\n",
    "    \n",
    "    \"Matern (nu=1.5)\": \n",
    "        C(1.0, (1e-3, 1e3)) * Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2), nu=1.5) \n",
    "        + WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-10, 1e1)),\n",
    "        \n",
    "    \"Matern (nu=2.5)\": \n",
    "        C(1.0, (1e-3, 1e3)) * Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2), nu=2.5) \n",
    "        + WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-10, 1e1)),\n",
    "        \n",
    "    \"RationalQuadratic\": \n",
    "        C(1.0, (1e-3, 1e3)) * RationalQuadratic(length_scale=1.0, alpha=0.1, length_scale_bounds=(1e-2, 1e2), alpha_bounds=(1e-2, 1e2)) \n",
    "        + WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-10, 1e1)),\n",
    "\n",
    "    # 复合核示例：RBF + Matern (更复杂的模型，可能需要更多数据来避免过拟合)\n",
    "    \"RBF + Matern\":\n",
    "        C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "        + C(1.0, (1e-3, 1e3)) * Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2), nu=1.5)\n",
    "        + WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-10, 1e1))\n",
    "}\n",
    "\n",
    "best_score = -np.inf\n",
    "best_kernel_name = \"\"\n",
    "best_model_from_cv = None\n",
    "\n",
    "print(\"开始测试多个核函数，优中选优...\")\n",
    "for name, kernel in kernels_to_try.items():\n",
    "    print(f\"  > 正在测试核函数: {name}...\")\n",
    "    gp = GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        n_restarts_optimizer=15, # 保证充分优化\n",
    "        normalize_y=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    try:\n",
    "        # 使用交叉验证来评估当前核函数的性能\n",
    "        score = cross_val_score(gp, X_train_scaled, y_train[target], cv=min(3, cv_folds), scoring='r2').mean()\n",
    "        print(f\"    交叉验证 R² 平均分: {score:.4f}\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_kernel_name = name\n",
    "            # 训练一个模型以备后用\n",
    "            gp.fit(X_train_scaled, y_train[target])\n",
    "            best_model_from_cv = gp\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    核函数 {name} 训练失败: {e}\")\n",
    "        continue\n",
    "\n",
    "if best_model_from_cv is None:\n",
    "    raise RuntimeError(\"所有核函数都训练失败，请检查数据或核函数参数！\")\n",
    "\n",
    "print(f\"\\n[决策] 最佳核函数为: '{best_kernel_name}' (交叉验证最高分: {best_score:.4f})\")\n",
    "\n",
    "# 将选出的最佳模型作为最终模型\n",
    "best_model = best_model_from_cv\n",
    "print(f\"最终选定的模型核函数参数: {best_model.kernel_}\")\n",
    "\n",
    "\n",
    "\n",
    "# 步骤3: 评估并可视化最终模型\n",
    "print(f\"\\n步骤3: 评估并可视化最终模型...\")\n",
    "\n",
    "# 在训练集和测试集上获取预测值\n",
    "y_pred_train = best_model.predict(X_train_scaled)\n",
    "y_pred_test = best_model.predict(X_test_scaled)\n",
    "\n",
    "# 计算最终的R²分数\n",
    "train_r2 = r2_score(y_train[target], y_pred_train)\n",
    "test_r2 = r2_score(y_test[target], y_pred_test)\n",
    "print(f\"训练集最终 R²: {train_r2:.4f}\")\n",
    "print(f\"测试集最终 R²: {test_r2:.4f}\")\n",
    "\n",
    "# 绘制“实际值 vs. 预测值”对比图 (训练集和测试集)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "fig.suptitle(f'\"{target}\" 的模型预测效果 (最佳核: {best_kernel_name})', fontsize=16)\n",
    "\n",
    "# 训练集图\n",
    "axes[0].scatter(y_train[target], y_pred_train, alpha=0.6)\n",
    "min_val_train = min(y_train[target].min(), y_pred_train.min())\n",
    "max_val_train = max(y_train[target].max(), y_pred_train.max())\n",
    "axes[0].plot([min_val_train, max_val_train], [min_val_train, max_val_train], 'r--', lw=2, label='理想情况 (y=x)')\n",
    "axes[0].set_title(f'训练集 (R² = {train_r2:.4f})')\n",
    "axes[0].set_xlabel('实际值')\n",
    "axes[0].set_ylabel('预测值')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "axes[0].axis('equal')\n",
    "\n",
    "# 测试集图\n",
    "axes[1].scatter(y_test[target], y_pred_test, alpha=0.6)\n",
    "min_val_test = min(y_test[target].min(), y_pred_test.min())\n",
    "max_val_test = max(y_test[target].max(), y_pred_test.max())\n",
    "axes[1].plot([min_val_test, max_val_test], [min_val_test, max_val_test], 'r--', lw=2, label='理想情况 (y=x)')\n",
    "axes[1].set_title(f'测试集 (R² = {test_r2:.4f})')\n",
    "axes[1].set_xlabel('实际值')\n",
    "axes[1].set_ylabel('预测值')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "axes[1].axis('equal')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "# 步骤4: 保存模型、结果和特征重要性\n",
    "print(f\"\\n步骤4: 保存模型与分析结果...\")\n",
    "models[target]['GaussianProcess'] = best_model\n",
    "gp_model_file = os.path.join(model_folder, f'{target}_高斯过程模型.pkl')\n",
    "with open(gp_model_file, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"高斯过程回归模型已保存至 {gp_model_file}\")\n",
    "\n",
    "# 创建并保存预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "train_file = os.path.join(save_folder, f'{target}_高斯过程训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_高斯过程测试集预测结果.csv')\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "print(f\"训练集和测试集预测结果已保存。\")\n",
    "\n",
    "\n",
    "# 计算并可视化特征重要性\n",
    "perm_importance = permutation_importance(\n",
    "    best_model, X_test_scaled, y_test[target],\n",
    "    n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('置换重要性 (Permutation Importance)')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - GaussianProcess特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_高斯过程特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存。\")\n",
    "\n",
    "\n",
    "# 步骤5: 分析预测不准确的样本\n",
    "print(f\"\\n步骤5: 分析预测不准确的样本...\")\n",
    "\n",
    "y_true_values = y_test[target].values\n",
    "errors = np.abs(y_true_values - y_pred_test)\n",
    "tolerance = 5.0  # 您可以根据需要调整此阈值\n",
    "\n",
    "inaccurate_mask = errors > tolerance\n",
    "inaccurate_indices = np.where(inaccurate_mask)[0]\n",
    "\n",
    "print(f\"\\n预测不准确的样本数量: {len(inaccurate_indices)} (占测试集的 {len(inaccurate_indices)/len(y_test)*100:.2f}%)，使用的容忍度阈值: {tolerance}\")\n",
    "\n",
    "if len(inaccurate_indices) > 0:\n",
    "    original_indices = [X_test_model.index[i] for i in inaccurate_indices]\n",
    "\n",
    "    inaccurate_samples = []\n",
    "    for i, idx in enumerate(inaccurate_indices):\n",
    "        actual = y_true_values[idx]\n",
    "        prediction = y_pred_test[idx]\n",
    "        inaccurate_samples.append({\n",
    "            '样本索引': original_indices[i],\n",
    "            '实际值': actual,\n",
    "            '预测值': prediction,\n",
    "            '绝对误差': errors[idx],\n",
    "            '相对误差(%)': (errors[idx] / np.abs(actual)) * 100 if actual != 0 else float('inf'),\n",
    "        })\n",
    "    \n",
    "    inaccurate_df = pd.DataFrame(inaccurate_samples).sort_values('绝对误差', ascending=False)\n",
    "    \n",
    "    print(\"\\n预测不准确的样本详情 (按误差降序排列):\")\n",
    "    print(inaccurate_df)\n",
    "    \n",
    "    inaccurate_file_path = os.path.join(save_folder, f'{target}_GP不准确预测.csv')\n",
    "    inaccurate_df.to_csv(inaccurate_file_path, index=False)\n",
    "    print(f\"不准确样本分析已保存至 {inaccurate_file_path}\")\n",
    "else:\n",
    "    print(f\"\\n在容忍度阈值 {tolerance} 内，没有发现预测不准确的样本。\")\n",
    "\n",
    "print(\"\\n高斯过程回归模型训练、评估和分析全部完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接运行的模型加载代码\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 如果需要加载神经网络模型，需要先定义KerasRegressorWrapper类\n",
    "class KerasRegressorWrapper:\n",
    "    def __init__(self, hidden_layers=[128, 64, 32], dropout_rate=0.3, \n",
    "                 learning_rate=0.001, epochs=100, batch_size=32):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X, verbose=0).flatten()\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'hidden_layers': self.hidden_layers,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "# 设置模型文件夹路径\n",
    "model_folder = '训练模型文件'\n",
    "\n",
    "# 初始化models字典\n",
    "if 'models' not in locals():\n",
    "    models = {}\n",
    "\n",
    "print(f\"为目标变量 {target} 加载模型...\")\n",
    "\n",
    "# 查找模型文件，排除Keras模型\n",
    "model_files = [f for f in os.listdir(model_folder) \n",
    "              if f.startswith(f'{target}_') and f.endswith('.pkl') \n",
    "              and not f.endswith('_features.pkl')\n",
    "              and 'Ensemble' not in f and '集成' not in f\n",
    "              and 'Keras' not in f and 'Neural' not in f and 'NN' not in f]\n",
    "\n",
    "print(f\"找到 {len(model_files)} 个模型文件: {model_files}\")\n",
    "\n",
    "# 初始化目标变量的模型字典\n",
    "if target not in models:\n",
    "    models[target] = {}\n",
    "\n",
    "# 加载每个模型\n",
    "for model_file in model_files:\n",
    "    # 从文件名提取模型名称\n",
    "    model_name = model_file.replace(f'{target}_', '').replace('模型.pkl', '')\n",
    "    \n",
    "    print(f\"  加载模型: {model_name}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    model_path = os.path.join(model_folder, model_file)\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    models[target][model_name] = model\n",
    "    print(f\"    {model_name} 加载成功\")\n",
    "\n",
    "print(f\"成功加载 {len(models[target])} 个模型\")\n",
    "print(f\"可用模型: {list(models[target].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VotingEnsemble - 基于模型标准R²性能分配权重\n",
    "print(f\"训练 {target} 的VotingEnsemble集成模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 使用已有模型和已知性能 - 无需重新评估\n",
    "base_models = []\n",
    "model_scores = {}  # 存储标准R²分数\n",
    "model_datasets = {}  # 存储每个模型对应的数据集\n",
    "\n",
    "print(\"收集已有模型的性能评估结果...\")\n",
    "# 使用原始训练代码计算的标准R²\n",
    "if 'XGBoost' in models[target]:\n",
    "    # 不重新评估，而是计算一次标准R²\n",
    "    model = models[target]['XGBoost']\n",
    "    y_pred = model.predict(X_test_tree[target])\n",
    "    r2 = r2_score(y_test[target], y_pred)\n",
    "    base_models.append(('xgb', model))\n",
    "    model_scores['xgb'] = r2\n",
    "    model_datasets['xgb'] = 'tree'\n",
    "    print(f\"  XGBoost - R²: {r2:.4f}\")\n",
    "\n",
    "if 'LightGBM' in models[target]:\n",
    "    model = models[target]['LightGBM']\n",
    "    y_pred = model.predict(X_test_tree[target])\n",
    "    r2 = r2_score(y_test[target], y_pred)\n",
    "    base_models.append(('lgb', model))\n",
    "    model_scores['lgb'] = r2\n",
    "    model_datasets['lgb'] = 'tree'\n",
    "    print(f\"  LightGBM - R²: {r2:.4f}\")\n",
    "\n",
    "if 'HistGradientBoosting' in models[target]:\n",
    "    model = models[target]['HistGradientBoosting']\n",
    "    y_pred = model.predict(X_test_tree[target])\n",
    "    r2 = r2_score(y_test[target], y_pred)\n",
    "    base_models.append(('hgb', model))\n",
    "    model_scores['hgb'] = r2\n",
    "    model_datasets['hgb'] = 'tree'\n",
    "    print(f\"  HistGradientBoosting - R²: {r2:.4f}\")\n",
    "\n",
    "if 'RandomForest' in models[target]:\n",
    "    model = models[target]['RandomForest']\n",
    "    y_pred = model.predict(X_test_tree_filled[target])\n",
    "    r2 = r2_score(y_test[target], y_pred)\n",
    "    base_models.append(('rf', model))\n",
    "    model_scores['rf'] = r2\n",
    "    model_datasets['rf'] = 'tree_filled'\n",
    "    print(f\"  RandomForest - R²: {r2:.4f}\")\n",
    "\n",
    "if 'GaussianProcess' in models[target]:\n",
    "    model = models[target]['GaussianProcess']\n",
    "    y_pred = model.predict(X_test_linear[target])\n",
    "    r2 = r2_score(y_test[target], y_pred)\n",
    "    base_models.append(('gp', model))\n",
    "    model_scores['gp'] = r2\n",
    "    model_datasets['gp'] = 'linear'\n",
    "    print(f\"  GaussianProcess - R²: {r2:.4f}\")\n",
    "\n",
    "# 改为基于标准R²性能计算权重，而不是容忍度R²\n",
    "print(\"\\n根据标准R²模型性能分配权重...\")\n",
    "\n",
    "# 基于标准R²计算权重\n",
    "total_score = sum(model_scores.values())\n",
    "if total_score > 0:  # 防止除以零错误\n",
    "    weights = [model_scores[name] / total_score * len(model_scores) for name, _ in base_models]\n",
    "else:\n",
    "    weights = [1.0 for _ in base_models]  # 如果总分为0，则均等分配权重\n",
    "\n",
    "print(\"  基于标准R²分配权重\")\n",
    "\n",
    "# 确保权重至少为0.5，防止某些模型权重过低\n",
    "min_weight = 0.5\n",
    "weights = [max(w, min_weight) for w in weights]\n",
    "\n",
    "# 打印权重\n",
    "for i, (name, _) in enumerate(base_models):\n",
    "    print(f\"  {name} 权重: {weights[i]:.4f}\")\n",
    "\n",
    "# 检查是否有足够的模型可用于集成\n",
    "if len(base_models) >= 2:\n",
    "    try:\n",
    "        # 创建自定义投票回归器的封装，确保使用正确的数据集\n",
    "        class EnhancedVotingRegressor:\n",
    "            def __init__(self, estimators, weights, datasets, target_name):\n",
    "                self.estimators = estimators\n",
    "                self.weights = weights\n",
    "                self.datasets = datasets\n",
    "                self.target_name = target_name\n",
    "                \n",
    "                # 归一化权重\n",
    "                self.weights = np.array(self.weights)\n",
    "                self.weights = self.weights / np.sum(self.weights)\n",
    "                \n",
    "            def predict(self, X):\n",
    "                # 对每个模型获取预测，并根据模型类型使用适当的数据预处理\n",
    "                predictions = []\n",
    "                \n",
    "                for i, (name, model) in enumerate(self.estimators):\n",
    "                    # 选择合适的数据格式\n",
    "                    dataset_type = self.datasets.get(name, 'standard')\n",
    "                    \n",
    "                    if dataset_type == 'tree':\n",
    "                        # 对于支持NaN的树模型，直接使用X\n",
    "                        X_model = X\n",
    "                    elif dataset_type == 'tree_filled':\n",
    "                        # 对于不支持NaN的树模型，需要填充X\n",
    "                        if isinstance(X, pd.DataFrame):\n",
    "                            X_model = X.fillna(0)\n",
    "                        else:\n",
    "                            X_model = X\n",
    "                    elif dataset_type == 'linear':\n",
    "                        # 对于线性模型，使用线性预处理的X\n",
    "                        X_model = X\n",
    "                    else:\n",
    "                        # 默认情况下直接使用X\n",
    "                        X_model = X\n",
    "                    \n",
    "                    # 获取当前模型的预测\n",
    "                    pred = model.predict(X_model)\n",
    "                    predictions.append(pred)\n",
    "                \n",
    "                # 加权平均所有预测\n",
    "                weighted_pred = np.zeros(predictions[0].shape)\n",
    "                for i, pred in enumerate(predictions):\n",
    "                    weighted_pred += self.weights[i] * pred\n",
    "                \n",
    "                return weighted_pred\n",
    "        \n",
    "        # 创建投票集成模型\n",
    "        print(\"创建投票集成模型...\")\n",
    "        voting_model = EnhancedVotingRegressor(\n",
    "            estimators=base_models,\n",
    "            weights=weights,\n",
    "            datasets=model_datasets,\n",
    "            target_name=target\n",
    "        )\n",
    "        \n",
    "        # 获取训练集和测试集预测\n",
    "        train_predictions = {}\n",
    "        test_predictions = {}\n",
    "        \n",
    "        # 获取每个基础模型的预测\n",
    "        for name, model in base_models:\n",
    "            if model_datasets[name] == 'tree':\n",
    "                train_predictions[name] = model.predict(X_train_tree[target])\n",
    "                test_predictions[name] = model.predict(X_test_tree[target])\n",
    "            elif model_datasets[name] == 'tree_filled':\n",
    "                train_predictions[name] = model.predict(X_train_tree_filled[target])\n",
    "                test_predictions[name] = model.predict(X_test_tree_filled[target])\n",
    "            elif model_datasets[name] == 'linear':\n",
    "                train_predictions[name] = model.predict(X_train_linear[target])\n",
    "                test_predictions[name] = model.predict(X_test_linear[target])\n",
    "        \n",
    "        # 计算加权预测\n",
    "        y_train_pred = np.zeros(len(y_train[target]))\n",
    "        y_test_pred = np.zeros(len(y_test[target]))\n",
    "        \n",
    "        for i, (name, _) in enumerate(base_models):\n",
    "            y_train_pred += weights[i] * train_predictions[name]\n",
    "            y_test_pred += weights[i] * test_predictions[name]\n",
    "        \n",
    "        # 归一化权重\n",
    "        total_weight = sum(weights)\n",
    "        y_train_pred /= total_weight\n",
    "        y_test_pred /= total_weight\n",
    "        \n",
    "        # 计算性能指标\n",
    "        train_r2 = r2_score(y_train[target], y_train_pred)\n",
    "        test_r2 = r2_score(y_test[target], y_test_pred)\n",
    "        \n",
    "        train_tol_r2 = tolerance_r2_score(y_train[target], y_train_pred, tolerance=current_tolerance, target=target)\n",
    "        test_tol_r2 = tolerance_r2_score(y_test[target], y_test_pred, tolerance=current_tolerance, target=target)\n",
    "        \n",
    "        train_within_tol = prediction_within_tolerance(y_train[target], y_train_pred, tolerance=current_tolerance, target=target)\n",
    "        test_within_tol = prediction_within_tolerance(y_test[target], y_test_pred, tolerance=current_tolerance, target=target)\n",
    "        # 保存训练集和测试集的预测结果\n",
    "        train_prediction = pd.DataFrame({\n",
    "            '实际值': y_train[target],\n",
    "            '集成预测值': y_train_pred,\n",
    "            '误差': np.abs(y_train[target] - y_train_pred)\n",
    "        })\n",
    "\n",
    "        test_prediction = pd.DataFrame({\n",
    "            '实际值': y_test[target],\n",
    "            '集成预测值': y_test_pred,\n",
    "            '误差': np.abs(y_test[target] - y_test_pred)\n",
    "        })\n",
    "\n",
    "        # 添加各基础模型的预测结果\n",
    "        for name, _ in base_models:\n",
    "            train_prediction[f'{name}预测值'] = train_predictions[name]\n",
    "            test_prediction[f'{name}预测值'] = test_predictions[name]\n",
    "\n",
    "        # 保存到文件\n",
    "        train_file = os.path.join(save_folder, f'{target}_投票集成模型训练集预测结果.csv')\n",
    "        test_file = os.path.join(save_folder, f'{target}_投票集成模型测试集预测结果.csv')\n",
    "\n",
    "        train_prediction.to_csv(train_file, index=False)\n",
    "        test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "        print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "        print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "        # 输出性能指标\n",
    "        print(f\"\\n投票集成模型性能:\")\n",
    "        print(f\"训练集: R²={train_r2:.4f}, 容忍度R²={train_tol_r2:.4f}, 在容忍范围内比例={train_within_tol:.2%}\")\n",
    "        print(f\"测试集: R²={test_r2:.4f}, 容忍度R²={test_tol_r2:.4f}, 在容忍范围内比例={test_within_tol:.2%}\")\n",
    "        \n",
    "        # 与各个基础模型比较性能\n",
    "        print(\"\\n与各基础模型性能比较:\")\n",
    "        for name, _ in base_models:\n",
    "            base_train_pred = train_predictions[name]\n",
    "            base_test_pred = test_predictions[name]\n",
    "            \n",
    "            base_train_r2 = r2_score(y_train[target], base_train_pred)\n",
    "            base_test_r2 = r2_score(y_test[target], base_test_pred)\n",
    "            \n",
    "            print(f\"  vs {name}:\")\n",
    "            print(f\"    训练集R²: {train_r2:.4f} vs {base_train_r2:.4f} (差异: {train_r2-base_train_r2:.4f})\")\n",
    "            print(f\"    测试集R²: {test_r2:.4f} vs {base_test_r2:.4f} (差异: {test_r2-base_test_r2:.4f})\")\n",
    "        \n",
    "        # 保存模型\n",
    "        models[target]['VotingEnsemble'] = voting_model\n",
    "        # 使用pickle保存投票集成模型\n",
    "        ensemble_model_file = os.path.join(model_folder, f'{target}_投票集成模型.pkl')\n",
    "        with open(ensemble_model_file, 'wb') as f:\n",
    "            pickle.dump(voting_model, f)\n",
    "        print(f\"投票集成模型已保存至 {ensemble_model_file}\")\n",
    "        # 可视化: 预测vs实际值散点图 (训练集和测试集)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 训练集散点图\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(y_train[target], y_train_pred, alpha=0.5)\n",
    "        plt.plot([y_train[target].min(), y_train[target].max()], [y_train[target].min(), y_train[target].max()], 'r--')\n",
    "        plt.xlabel('实际值')\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title(f'训练集: R²={train_r2:.4f}')\n",
    "        \n",
    "        # 测试集散点图\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(y_test[target], y_test_pred, alpha=0.5)\n",
    "        plt.plot([y_test[target].min(), y_test[target].max()], [y_test[target].min(), y_test[target].max()], 'r--')\n",
    "        plt.xlabel('实际值')\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title(f'测试集: R²={test_r2:.4f}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 绘制误差分布\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 训练集误差\n",
    "        plt.subplot(1, 2, 1)\n",
    "        train_errors = y_train[target] - y_train_pred\n",
    "        plt.hist(train_errors, bins=30, alpha=0.7)\n",
    "        plt.axvline(x=0, color='r', linestyle='--')\n",
    "        plt.xlabel('预测误差')\n",
    "        plt.ylabel('频次')\n",
    "        plt.title(f'训练集误差分布 (MAE={np.abs(train_errors).mean():.4f})')\n",
    "        \n",
    "        # 测试集误差\n",
    "        plt.subplot(1, 2, 2)\n",
    "        test_errors = y_test[target] - y_test_pred\n",
    "        plt.hist(test_errors, bins=30, alpha=0.7)\n",
    "        plt.axvline(x=0, color='r', linestyle='--')\n",
    "        plt.xlabel('预测误差')\n",
    "        plt.ylabel('频次')\n",
    "        plt.title(f'测试集误差分布 (MAE={np.abs(test_errors).mean():.4f})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 绘制权重分布\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        model_names = [name for name, _ in base_models]\n",
    "        plt.bar(model_names, weights)\n",
    "        plt.xlabel('模型')\n",
    "        plt.ylabel('权重')\n",
    "        plt.title(f'{target} - 投票集成模型权重分布')\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 对比各模型预测分布\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        model_preds = [y_test_pred] + [test_predictions[name] for name, _ in base_models]\n",
    "        model_labels = ['Voting'] + [name for name, _ in base_models]\n",
    "        \n",
    "        plt.boxplot(model_preds, labels=model_labels)\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title('投票集成模型与各基础模型预测分布对比')\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"创建投票集成模型失败: {str(e)}\")\n",
    "        print(f\"错误详情: {traceback.format_exc()}\")\n",
    "else:\n",
    "    print(\"没有足够的基础模型来创建投票集成\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 自适应集成模型 - 根据样本特征动态选择最佳模型\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(f\"训练 {target} 的自适应集成模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 创建有效模型列表及其对应的数据集\n",
    "available_models = []\n",
    "model_input_data = {}\n",
    "\n",
    "if 'XGBoost' in models[target]:\n",
    "    available_models.append('XGBoost')\n",
    "    model_input_data['XGBoost'] = {\n",
    "        'train': X_train_tree[target],\n",
    "        'test': X_test_tree[target]\n",
    "    }\n",
    "\n",
    "if 'LightGBM' in models[target]:\n",
    "    available_models.append('LightGBM')\n",
    "    model_input_data['LightGBM'] = {\n",
    "        'train': X_train_tree[target],\n",
    "        'test': X_test_tree[target]\n",
    "    }\n",
    "    \n",
    "if 'HistGradientBoosting' in models[target]:\n",
    "    available_models.append('HistGradientBoosting')\n",
    "    model_input_data['HistGradientBoosting'] = {\n",
    "        'train': X_train_tree[target],\n",
    "        'test': X_test_tree[target]\n",
    "    }\n",
    "    \n",
    "if 'RandomForest' in models[target]:\n",
    "    available_models.append('RandomForest')\n",
    "    model_input_data['RandomForest'] = {\n",
    "        'train': X_train_tree_filled[target],\n",
    "        'test': X_test_tree_filled[target]\n",
    "    }\n",
    "    \n",
    "if 'GaussianProcess' in models[target]:\n",
    "    available_models.append('GaussianProcess')\n",
    "    model_input_data['GaussianProcess'] = {\n",
    "        'train': X_train_linear[target],\n",
    "        'test': X_test_linear[target]\n",
    "    }\n",
    "\n",
    "print(f\"可用模型: {available_models}\")\n",
    "\n",
    "if len(available_models) < 2:\n",
    "    print(\"自适应集成至少需要两个模型，目前可用模型不足\")\n",
    "else:\n",
    "    try:\n",
    "        # 步骤1: 为每个样本生成各模型的预测\n",
    "        print(\"为每个样本生成所有模型的预测...\")\n",
    "        train_predictions = {}\n",
    "        test_predictions = {}\n",
    "        \n",
    "        for model_name in available_models:\n",
    "            model = models[target][model_name]\n",
    "            # 使用适当的数据集进行预测\n",
    "            train_data = model_input_data[model_name]['train']\n",
    "            test_data = model_input_data[model_name]['test']\n",
    "            \n",
    "            train_pred = model.predict(train_data)\n",
    "            test_pred = model.predict(test_data)\n",
    "            \n",
    "            train_predictions[model_name] = train_pred\n",
    "            test_predictions[model_name] = test_pred\n",
    "        \n",
    "        # 步骤2: 计算每个样本的每个模型预测误差\n",
    "        print(\"计算各模型在每个样本上的预测误差...\")\n",
    "        train_errors = {}\n",
    "        for model_name in available_models:\n",
    "            pred = train_predictions[model_name]\n",
    "            error = np.abs(y_train[target].values - pred)\n",
    "            train_errors[model_name] = error\n",
    "        \n",
    "        # 步骤3: 创建一个元模型，学习如何根据特征选择最佳模型\n",
    "        print(\"训练元模型来决定每个样本应使用哪个模型...\")\n",
    "        \n",
    "        # 为每个样本找出表现最好的模型\n",
    "        best_model_indices = np.zeros(len(y_train[target]), dtype=int)\n",
    "        model_name_to_idx = {name: idx for idx, name in enumerate(available_models)}\n",
    "        \n",
    "        for i in range(len(y_train[target])):\n",
    "            model_errors = [train_errors[model_name][i] for model_name in available_models]\n",
    "            best_model_idx = np.argmin(model_errors)\n",
    "            best_model_indices[i] = best_model_idx\n",
    "        \n",
    "        # 用原始特征训练一个分类器来预测最佳模型\n",
    "        meta_classifier = RandomForestClassifier(\n",
    "            n_estimators=200, \n",
    "            max_depth=4,\n",
    "            min_samples_split=2,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        meta_classifier.fit(X_train[target], best_model_indices)\n",
    "        \n",
    "        # 步骤4: 在训练集和测试集上使用元模型选择最佳模型\n",
    "        print(\"在训练集和测试集上应用元模型...\")\n",
    "        train_best_models = meta_classifier.predict(X_train[target])\n",
    "        test_best_models = meta_classifier.predict(X_test[target])\n",
    "        \n",
    "        # 步骤5: 根据元模型的选择，为每个样本选择相应的预测\n",
    "        train_adaptive_predictions = np.zeros(len(y_train[target]))\n",
    "        test_adaptive_predictions = np.zeros(len(y_test[target]))\n",
    "        \n",
    "        # 为训练集计算自适应预测\n",
    "        for i in range(len(y_train[target])):\n",
    "            selected_model = available_models[train_best_models[i]]\n",
    "            train_adaptive_predictions[i] = train_predictions[selected_model][i]\n",
    "        \n",
    "        # 为测试集计算自适应预测\n",
    "        for i in range(len(y_test[target])):\n",
    "            selected_model = available_models[test_best_models[i]]\n",
    "            test_adaptive_predictions[i] = test_predictions[selected_model][i]\n",
    "        \n",
    "        # 步骤6: 评估自适应集成的性能\n",
    "        train_r2 = r2_score(y_train[target], train_adaptive_predictions)\n",
    "        train_tol_r2 = tolerance_r2_score(y_train[target], train_adaptive_predictions, tolerance=current_tolerance, target=target)\n",
    "        train_within_tol = prediction_within_tolerance(y_train[target], train_adaptive_predictions, tolerance=current_tolerance, target=target)\n",
    "        \n",
    "        test_r2 = r2_score(y_test[target], test_adaptive_predictions)\n",
    "        test_tol_r2 = tolerance_r2_score(y_test[target], test_adaptive_predictions, tolerance=current_tolerance, target=target)\n",
    "        test_within_tol = prediction_within_tolerance(y_test[target], test_adaptive_predictions, tolerance=current_tolerance, target=target)\n",
    "        \n",
    "        print(f\"\\n自适应集成模型性能:\")\n",
    "        print(f\"训练集: R²={train_r2:.4f}, 容忍度R²={train_tol_r2:.4f}, 在容忍范围内比例={train_within_tol:.2%}\")\n",
    "        print(f\"测试集: R²={test_r2:.4f}, 容忍度R²={test_tol_r2:.4f}, 在容忍范围内比例={test_within_tol:.2%}\")\n",
    "        \n",
    "        # 步骤7: 比较自适应集成与各个基础模型的性能\n",
    "        print(\"\\n与各基础模型性能比较:\")\n",
    "        for model_name in available_models:\n",
    "            model_train_pred = train_predictions[model_name]\n",
    "            model_test_pred = test_predictions[model_name]\n",
    "            \n",
    "            model_train_r2 = r2_score(y_train[target], model_train_pred)\n",
    "            model_test_r2 = r2_score(y_test[target], model_test_pred)\n",
    "            \n",
    "            train_r2_diff = train_r2 - model_train_r2\n",
    "            test_r2_diff = test_r2 - model_test_r2\n",
    "            \n",
    "            print(f\"  vs {model_name}:\")\n",
    "            print(f\"    训练集R²: {train_r2:.4f} vs {model_train_r2:.4f} (差异: {train_r2_diff:.4f})\")\n",
    "            print(f\"    测试集R²: {test_r2:.4f} vs {model_test_r2:.4f} (差异: {test_r2_diff:.4f})\")\n",
    "        \n",
    "        # 步骤8: 分析各模型被选择的频率\n",
    "        train_model_selection_counts = np.bincount(train_best_models, minlength=len(available_models))\n",
    "        train_model_selection_percent = train_model_selection_counts / len(train_best_models) * 100\n",
    "        \n",
    "        test_model_selection_counts = np.bincount(test_best_models, minlength=len(available_models))\n",
    "        test_model_selection_percent = test_model_selection_counts / len(test_best_models) * 100\n",
    "        \n",
    "        print(\"\\n各模型在训练集上的选择频率:\")\n",
    "        for i, model_name in enumerate(available_models):\n",
    "            print(f\"  {model_name}: {train_model_selection_counts[i]} 次 ({train_model_selection_percent[i]:.2f}%)\")\n",
    "        \n",
    "        print(\"\\n各模型在测试集上的选择频率:\")\n",
    "        for i, model_name in enumerate(available_models):\n",
    "            print(f\"  {model_name}: {test_model_selection_counts[i]} 次 ({test_model_selection_percent[i]:.2f}%)\")\n",
    "        \n",
    "        # 步骤9: 创建并保存自适应集成模型\n",
    "        class AdaptiveEnsembleModel:\n",
    "            def __init__(self, meta_classifier, models_dict, available_models, model_input_data):\n",
    "                self.meta_classifier = meta_classifier\n",
    "                self.models_dict = models_dict\n",
    "                self.available_models = available_models\n",
    "                self.model_input_data = model_input_data\n",
    "                \n",
    "                # 添加数据类型映射\n",
    "                self.data_type_map = {\n",
    "                    'XGBoost': 'tree',\n",
    "                    'LightGBM': 'tree',\n",
    "                    'HistGradientBoosting': 'tree',\n",
    "                    'RandomForest': 'tree_filled',\n",
    "                    'GaussianProcess': 'linear'\n",
    "                }\n",
    "                \n",
    "            def predict(self, X):\n",
    "                # 确保X是DataFrame格式，保持列名\n",
    "                if not isinstance(X, pd.DataFrame):\n",
    "                    if hasattr(X, 'shape') and len(X.shape) == 2:\n",
    "                        if hasattr(X_train[target], 'columns'):\n",
    "                            X = pd.DataFrame(X, columns=X_train[target].columns)\n",
    "                        else:\n",
    "                            X = pd.DataFrame(X)\n",
    "                \n",
    "                # 首先预测每个样本应使用哪个模型\n",
    "                model_choices = self.meta_classifier.predict(X)\n",
    "                \n",
    "                # 初始化预测结果数组\n",
    "                predictions = np.zeros(len(X))\n",
    "                \n",
    "                # 为每个样本获取相应模型的预测\n",
    "                for i in range(len(X)):\n",
    "                    # 获取为当前样本选择的模型\n",
    "                    model_idx = model_choices[i]\n",
    "                    model_name = self.available_models[model_idx]\n",
    "                    model = self.models_dict[model_name]\n",
    "                    \n",
    "                    # 准备单个样本的数据\n",
    "                    if isinstance(X, pd.DataFrame):\n",
    "                        x_sample = X.iloc[[i]]\n",
    "                    else:\n",
    "                        if len(X.shape) == 1:\n",
    "                            x_sample = X.reshape(1, -1)\n",
    "                        else:\n",
    "                            x_sample = X[[i]]\n",
    "                    \n",
    "                    # 根据模型类型进行预处理\n",
    "                    data_type = self.data_type_map.get(model_name, 'standard')\n",
    "                    \n",
    "                    if data_type == 'tree':\n",
    "                        # 支持NaN值的树模型，不需要特殊处理\n",
    "                        x_processed = x_sample\n",
    "                    elif data_type == 'tree_filled':\n",
    "                        # 不支持NaN的树模型，需要填充\n",
    "                        if isinstance(x_sample, pd.DataFrame):\n",
    "                            x_processed = x_sample.fillna(0)\n",
    "                        else:\n",
    "                            x_processed = np.nan_to_num(x_sample, 0)\n",
    "                    elif data_type == 'linear':\n",
    "                        # 线性模型的特殊处理，如果有需要\n",
    "                        x_processed = x_sample\n",
    "                    else:\n",
    "                        # 默认情况\n",
    "                        x_processed = x_sample\n",
    "                    \n",
    "                    # 获取预测\n",
    "                    pred = model.predict(x_processed)\n",
    "                    predictions[i] = pred[0] if hasattr(pred, '__len__') else pred\n",
    "                \n",
    "                return predictions\n",
    "                \n",
    "            def get_feature_importances(self):\n",
    "                # 获取元分类器的特征重要性\n",
    "                if hasattr(self.meta_classifier, 'feature_importances_'):\n",
    "                    return self.meta_classifier.feature_importances_\n",
    "                return None\n",
    "        \n",
    "        # 创建自适应集成模型实例\n",
    "        adaptive_model = AdaptiveEnsembleModel(\n",
    "            meta_classifier=meta_classifier,\n",
    "            models_dict=models[target],\n",
    "            available_models=available_models,\n",
    "            model_input_data=model_input_data\n",
    "        )\n",
    "        \n",
    "        # 保存模型\n",
    "        models[target]['AdaptiveEnsemble'] = adaptive_model\n",
    "        # 使用pickle保存自适应集成模型\n",
    "        adaptive_model_file = os.path.join(model_folder, f'{target}_自适应集成模型.pkl')\n",
    "        with open(adaptive_model_file, 'wb') as f:\n",
    "            pickle.dump(adaptive_model, f)\n",
    "        print(f\"自适应集成模型已保存至 {adaptive_model_file}\")\n",
    "\n",
    "        # 保存训练集和测试集的预测结果\n",
    "        train_prediction = pd.DataFrame({\n",
    "            '实际值': y_train[target],\n",
    "            '自适应集成预测值': train_adaptive_predictions,\n",
    "            '误差': np.abs(y_train[target] - train_adaptive_predictions)\n",
    "        })\n",
    "\n",
    "        test_prediction = pd.DataFrame({\n",
    "            '实际值': y_test[target],\n",
    "            '自适应集成预测值': test_adaptive_predictions,\n",
    "            '误差': np.abs(y_test[target] - test_adaptive_predictions)\n",
    "        })\n",
    "\n",
    "        # 添加各基础模型的预测结果以便比较\n",
    "        for model_name in available_models:\n",
    "            train_prediction[f'{model_name}预测值'] = train_predictions[model_name]\n",
    "            test_prediction[f'{model_name}预测值'] = test_predictions[model_name]\n",
    "\n",
    "        # 保存到文件\n",
    "        train_file = os.path.join(save_folder, f'{target}_自适应集成训练集预测结果.csv')\n",
    "        test_file = os.path.join(save_folder, f'{target}_自适应集成测试集预测结果.csv')\n",
    "\n",
    "        train_prediction.to_csv(train_file, index=False)\n",
    "        test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "        print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "        print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "        # 可视化: 预测vs实际值散点图 (训练集和测试集)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 训练集散点图\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(y_train[target], train_adaptive_predictions, alpha=0.5)\n",
    "        plt.plot([y_train[target].min(), y_train[target].max()], [y_train[target].min(), y_train[target].max()], 'r--')\n",
    "        plt.xlabel('实际值')\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title(f'训练集: R²={train_r2:.4f}')\n",
    "        \n",
    "        # 测试集散点图\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(y_test[target], test_adaptive_predictions, alpha=0.5)\n",
    "        plt.plot([y_test[target].min(), y_test[target].max()], [y_test[target].min(), y_test[target].max()], 'r--')\n",
    "        plt.xlabel('实际值')\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title(f'测试集: R²={test_r2:.4f}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 绘制误差分布\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 训练集误差\n",
    "        plt.subplot(1, 2, 1)\n",
    "        train_errors_plot = y_train[target] - train_adaptive_predictions\n",
    "        plt.hist(train_errors_plot, bins=30, alpha=0.7)\n",
    "        plt.axvline(x=0, color='r', linestyle='--')\n",
    "        plt.xlabel('预测误差')\n",
    "        plt.ylabel('频次')\n",
    "        plt.title(f'训练集误差分布 (MAE={np.abs(train_errors_plot).mean():.4f})')\n",
    "        \n",
    "        # 测试集误差\n",
    "        plt.subplot(1, 2, 2)\n",
    "        test_errors_plot = y_test[target] - test_adaptive_predictions\n",
    "        plt.hist(test_errors_plot, bins=30, alpha=0.7)\n",
    "        plt.axvline(x=0, color='r', linestyle='--')\n",
    "        plt.xlabel('预测误差')\n",
    "        plt.ylabel('频次')\n",
    "        plt.title(f'测试集误差分布 (MAE={np.abs(test_errors_plot).mean():.4f})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 保存元分类器的特征重要性\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': X_train[target].columns,\n",
    "            'Importance': meta_classifier.feature_importances_\n",
    "        })\n",
    "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "        # 保存特征重要性数据\n",
    "        importance_file = os.path.join(save_folder, f'{target}_自适应集成特征重要性.csv')\n",
    "        feature_importance.to_csv(importance_file, index=False)\n",
    "        print(f\"特征重要性数据已保存至 {importance_file}\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "        plt.xlabel('重要性')\n",
    "        plt.ylabel('特征')\n",
    "        plt.title(f'{target} - 自适应集成模型选择特征重要性')\n",
    "        plt.grid(True, axis='x')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 绘制模型选择频率饼图\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 训练集上的模型选择频率\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.pie(train_model_selection_counts, labels=available_models, autopct='%1.1f%%')\n",
    "        plt.title(f'训练集 - 模型选择频率')\n",
    "        \n",
    "        # 测试集上的模型选择频率\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.pie(test_model_selection_counts, labels=available_models, autopct='%1.1f%%')\n",
    "        plt.title(f'测试集 - 模型选择频率')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        # 保存模型选择频率数据\n",
    "        model_selection_data = pd.DataFrame({\n",
    "            '模型': available_models,\n",
    "            '训练集选择次数': train_model_selection_counts,\n",
    "            '训练集选择百分比': train_model_selection_percent,\n",
    "            '测试集选择次数': test_model_selection_counts,\n",
    "            '测试集选择百分比': test_model_selection_percent\n",
    "        })\n",
    "\n",
    "        selection_file = os.path.join(save_folder, f'{target}_自适应集成模型选择频率.csv')\n",
    "        model_selection_data.to_csv(selection_file, index=False)\n",
    "        print(f\"模型选择频率数据已保存至 {selection_file}\")\n",
    "        # 绘制误差分布与模型选择关系\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # 对比测试集上各模型的预测结果\n",
    "        model_data = [test_adaptive_predictions] + [test_predictions[model] for model in available_models]\n",
    "        model_labels = ['自适应集成'] + available_models\n",
    "        \n",
    "        plt.boxplot(model_data, labels=model_labels)\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title('自适应集成模型与各基础模型预测分布对比')\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"创建自适应集成模型失败: {str(e)}\")\n",
    "        print(f\"错误详情: {traceback.format_exc()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加权平均集成模型 - 使用标准R²优化权重\n",
    "print(f\"训练 {target} 的加权平均集成模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 创建有效模型列表及其预测结果\n",
    "available_models = []\n",
    "train_predictions = {}\n",
    "test_predictions = {}\n",
    "\n",
    "if 'XGBoost' in models[target]:\n",
    "    model = models[target]['XGBoost']\n",
    "    train_pred = model.predict(X_train_tree[target])\n",
    "    test_pred = model.predict(X_test_tree[target])\n",
    "    available_models.append('XGBoost')\n",
    "    train_predictions['XGBoost'] = train_pred\n",
    "    test_predictions['XGBoost'] = test_pred\n",
    "\n",
    "if 'LightGBM' in models[target]:\n",
    "    model = models[target]['LightGBM']\n",
    "    train_pred = model.predict(X_train_tree[target])\n",
    "    test_pred = model.predict(X_test_tree[target])\n",
    "    available_models.append('LightGBM')\n",
    "    train_predictions['LightGBM'] = train_pred\n",
    "    test_predictions['LightGBM'] = test_pred\n",
    "    \n",
    "if 'HistGradientBoosting' in models[target]:\n",
    "    model = models[target]['HistGradientBoosting']\n",
    "    train_pred = model.predict(X_train_tree[target])\n",
    "    test_pred = model.predict(X_test_tree[target])\n",
    "    available_models.append('HistGradientBoosting')\n",
    "    train_predictions['HistGradientBoosting'] = train_pred\n",
    "    test_predictions['HistGradientBoosting'] = test_pred\n",
    "    \n",
    "if 'RandomForest' in models[target]:\n",
    "    model = models[target]['RandomForest']\n",
    "    train_pred = model.predict(X_train_tree_filled[target])\n",
    "    test_pred = model.predict(X_test_tree_filled[target])\n",
    "    available_models.append('RandomForest')\n",
    "    train_predictions['RandomForest'] = train_pred\n",
    "    test_predictions['RandomForest'] = test_pred\n",
    "    \n",
    "if 'GaussianProcess' in models[target]:\n",
    "    model = models[target]['GaussianProcess']\n",
    "    train_pred = model.predict(X_train_linear[target])\n",
    "    test_pred = model.predict(X_test_linear[target])\n",
    "    available_models.append('GaussianProcess')\n",
    "    train_predictions['GaussianProcess'] = train_pred\n",
    "    test_predictions['GaussianProcess'] = test_pred\n",
    "\n",
    "print(f\"可用模型: {available_models}\")\n",
    "\n",
    "if len(available_models) < 2:\n",
    "    print(\"加权平均集成至少需要两个模型，目前可用模型不足\")\n",
    "else:\n",
    "    try:\n",
    "        # 通过优化找到最优权重\n",
    "        print(\"寻找最优权重组合...\")\n",
    "        from scipy.optimize import minimize\n",
    "        \n",
    "        # 定义自定义加权平均函数\n",
    "        def weighted_prediction(weights, preds_list):\n",
    "            weighted_preds = np.zeros(preds_list[0].shape)\n",
    "            for i, preds in enumerate(preds_list):\n",
    "                weighted_preds += weights[i] * preds\n",
    "            return weighted_preds\n",
    "        \n",
    "        # 定义要优化的损失函数（最大化标准R²）- 修改为使用标准R²而非容忍度R²\n",
    "        def neg_r2(weights, preds_list, y_true):\n",
    "            # 归一化权重确保和为1\n",
    "            weights = np.array(weights)\n",
    "            weights = weights / np.sum(weights)\n",
    "            \n",
    "            weighted_preds = weighted_prediction(weights, preds_list)\n",
    "            r2 = r2_score(y_true, weighted_preds)\n",
    "            return -r2  # 最小化负的R²（即最大化R²）\n",
    "        \n",
    "        # 准备用于优化的预测值列表\n",
    "        train_preds_list = [train_predictions[model_name] for model_name in available_models]\n",
    "        \n",
    "        # 初始权重（均等）\n",
    "        initial_weights = np.ones(len(available_models)) / len(available_models)\n",
    "        \n",
    "        # 约束：权重和为1，所有权重非负\n",
    "        constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "        bounds = [(0, 1) for _ in range(len(available_models))]\n",
    "        \n",
    "        # 使用SLSQP优化算法寻找最优权重\n",
    "        print(\"优化权重中...\")\n",
    "        result = minimize(\n",
    "            neg_r2, \n",
    "            initial_weights, \n",
    "            args=(train_preds_list, y_train[target]),\n",
    "            bounds=bounds,\n",
    "            constraints=constraints,\n",
    "            method='SLSQP'\n",
    "        )\n",
    "        \n",
    "        if result.success:\n",
    "            # 获取最优权重并归一化\n",
    "            optimal_weights = result.x\n",
    "            optimal_weights = optimal_weights / np.sum(optimal_weights)\n",
    "            \n",
    "            print(\"\\n找到最优权重组合:\")\n",
    "            for i, model_name in enumerate(available_models):\n",
    "                print(f\"  {model_name}: {optimal_weights[i]:.4f}\")\n",
    "                \n",
    "            # 使用最优权重在训练集和测试集上评估性能\n",
    "            train_weighted_preds = weighted_prediction(\n",
    "                optimal_weights, \n",
    "                [train_predictions[model_name] for model_name in available_models]\n",
    "            )\n",
    "            \n",
    "            test_weighted_preds = weighted_prediction(\n",
    "                optimal_weights, \n",
    "                [test_predictions[model_name] for model_name in available_models]\n",
    "            )\n",
    "            \n",
    "            # 计算性能指标\n",
    "            train_r2 = r2_score(y_train[target], train_weighted_preds)\n",
    "            train_tol_r2 = tolerance_r2_score(y_train[target], train_weighted_preds, tolerance=current_tolerance, target=target)\n",
    "            train_within_tol = prediction_within_tolerance(y_train[target], train_weighted_preds, tolerance=current_tolerance, target=target)\n",
    "            \n",
    "            test_r2 = r2_score(y_test[target], test_weighted_preds)\n",
    "            test_tol_r2 = tolerance_r2_score(y_test[target], test_weighted_preds, tolerance=current_tolerance, target=target)\n",
    "            test_within_tol = prediction_within_tolerance(y_test[target], test_weighted_preds, tolerance=current_tolerance, target=target)\n",
    "            \n",
    "            print(\"\\n加权平均集成性能:\")\n",
    "            print(f\"  训练集 - R²: {train_r2:.4f}, 容忍度R²: {train_tol_r2:.4f}, 在容忍范围内: {train_within_tol:.2%}\")\n",
    "            print(f\"  测试集 - R²: {test_r2:.4f}, 容忍度R²: {test_tol_r2:.4f}, 在容忍范围内: {test_within_tol:.2%}\")\n",
    "            \n",
    "            # 与各个基础模型比较性能\n",
    "            print(\"\\n与各基础模型性能比较:\")\n",
    "            for model_name in available_models:\n",
    "                model_test_pred = test_predictions[model_name]\n",
    "                model_r2 = r2_score(y_test[target], model_test_pred)\n",
    "                model_tol_r2 = tolerance_r2_score(y_test[target], model_test_pred, tolerance=current_tolerance, target=target)\n",
    "                \n",
    "                r2_diff = test_r2 - model_r2\n",
    "                tol_r2_diff = test_tol_r2 - model_tol_r2\n",
    "                \n",
    "                print(f\"  vs {model_name}:\")\n",
    "                print(f\"    R² 差异: {r2_diff:.4f} ({'+' if r2_diff > 0 else ''}{r2_diff/max(0.0001, abs(model_r2))*100:.2f}%)\")\n",
    "                print(f\"    容忍度R² 差异: {tol_r2_diff:.4f} ({'+' if tol_r2_diff > 0 else ''}{tol_r2_diff/max(0.0001, abs(model_tol_r2))*100:.2f}%)\")\n",
    "            \n",
    "            # 创建加权平均集成模型\n",
    "            class WeightedAverageEnsemble:\n",
    "                def __init__(self, models_dict, model_names, weights, model_datasets):\n",
    "                    self.models_dict = models_dict\n",
    "                    self.model_names = model_names\n",
    "                    self.weights = weights\n",
    "                    self.model_datasets = model_datasets\n",
    "                    \n",
    "                def predict(self, X):\n",
    "                    predictions = []\n",
    "                    \n",
    "                    for i, model_name in enumerate(self.model_names):\n",
    "                        model = self.models_dict[model_name]\n",
    "                        \n",
    "                        # 获取适当的数据格式\n",
    "                        if model_name in ['XGBoost', 'LightGBM', 'HistGradientBoosting']:\n",
    "                            if isinstance(X, pd.DataFrame):\n",
    "                                # 假设X是原始数据框，需要应用适当的预处理\n",
    "                                X_model = X  # 应该在实际应用中进行适当的预处理转换\n",
    "                            else:\n",
    "                                X_model = X\n",
    "                        elif model_name == 'RandomForest':\n",
    "                            if isinstance(X, pd.DataFrame):\n",
    "                                # 对于RandomForest需要填充NaN\n",
    "                                X_model = X.fillna(0)\n",
    "                            else:\n",
    "                                X_model = X\n",
    "                        elif model_name == 'GaussianProcess':\n",
    "                            if isinstance(X, pd.DataFrame):\n",
    "                                # 假设X是原始数据框，需要应用适当的预处理\n",
    "                                X_model = X  # 应该在实际应用中进行适当的预处理转换\n",
    "                            else:\n",
    "                                X_model = X\n",
    "                        else:\n",
    "                            X_model = X\n",
    "                            \n",
    "                        model_pred = model.predict(X_model)\n",
    "                        predictions.append(model_pred)\n",
    "                    \n",
    "                    # 应用权重\n",
    "                    weighted_preds = np.zeros(predictions[0].shape)\n",
    "                    for i, preds in enumerate(predictions):\n",
    "                        weighted_preds += self.weights[i] * preds\n",
    "                        \n",
    "                    return weighted_preds\n",
    "            \n",
    "            # 创建模型数据集字典\n",
    "            model_datasets = {\n",
    "                'XGBoost': 'tree',\n",
    "                'LightGBM': 'tree',\n",
    "                'HistGradientBoosting': 'tree',\n",
    "                'RandomForest': 'tree_filled',\n",
    "                'GaussianProcess': 'linear'\n",
    "            }\n",
    "            \n",
    "            # 实例化加权平均集成模型\n",
    "            weighted_model = WeightedAverageEnsemble(\n",
    "                models_dict=models[target],\n",
    "                model_names=available_models,\n",
    "                weights=optimal_weights,\n",
    "                model_datasets=model_datasets\n",
    "            )\n",
    "            \n",
    "            # 保存模型\n",
    "            models[target]['WeightedEnsemble'] = weighted_model\n",
    "            # 使用pickle保存加权平均集成模型\n",
    "            weighted_model_file = os.path.join(model_folder, f'{target}_加权平均集成模型.pkl')\n",
    "            with open(weighted_model_file, 'wb') as f:\n",
    "                pickle.dump(weighted_model, f)\n",
    "            print(f\"加权平均集成模型已保存至 {weighted_model_file}\")\n",
    "\n",
    "            # 保存训练集和测试集的预测结果\n",
    "            train_prediction = pd.DataFrame({\n",
    "                '实际值': y_train[target],\n",
    "                '加权平均预测值': train_weighted_preds,\n",
    "                '误差': np.abs(y_train[target] - train_weighted_preds)\n",
    "            })\n",
    "\n",
    "            test_prediction = pd.DataFrame({\n",
    "                '实际值': y_test[target],\n",
    "                '加权平均预测值': test_weighted_preds,\n",
    "                '误差': np.abs(y_test[target] - test_weighted_preds)\n",
    "            })\n",
    "\n",
    "            # 添加各基础模型的预测结果以便比较\n",
    "            for model_name in available_models:\n",
    "                train_prediction[f'{model_name}预测值'] = train_predictions[model_name]\n",
    "                test_prediction[f'{model_name}预测值'] = test_predictions[model_name]\n",
    "\n",
    "            # 保存到文件\n",
    "            train_file = os.path.join(save_folder, f'{target}_加权平均集成训练集预测结果.csv')\n",
    "            test_file = os.path.join(save_folder, f'{target}_加权平均集成测试集预测结果.csv')\n",
    "\n",
    "            train_prediction.to_csv(train_file, index=False)\n",
    "            test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "            print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "            print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "            # 可视化: 预测vs实际值散点图 (训练集和测试集)\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # 训练集散点图\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(y_train[target], train_weighted_preds, alpha=0.5)\n",
    "            plt.plot([y_train[target].min(), y_train[target].max()], [y_train[target].min(), y_train[target].max()], 'r--')\n",
    "            plt.xlabel('实际值')\n",
    "            plt.ylabel('预测值')\n",
    "            plt.title(f'训练集: R²={train_r2:.4f}')\n",
    "            \n",
    "            # 测试集散点图\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.scatter(y_test[target], test_weighted_preds, alpha=0.5)\n",
    "            plt.plot([y_test[target].min(), y_test[target].max()], [y_test[target].min(), y_test[target].max()], 'r--')\n",
    "            plt.xlabel('实际值')\n",
    "            plt.ylabel('预测值')\n",
    "            plt.title(f'测试集: R²={test_r2:.4f}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 绘制误差分布\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # 训练集误差\n",
    "            plt.subplot(1, 2, 1)\n",
    "            train_errors = y_train[target] - train_weighted_preds\n",
    "            plt.hist(train_errors, bins=30, alpha=0.7)\n",
    "            plt.axvline(x=0, color='r', linestyle='--')\n",
    "            plt.xlabel('预测误差')\n",
    "            plt.ylabel('频次')\n",
    "            plt.title(f'训练集误差分布 (MAE={np.abs(train_errors).mean():.4f})')\n",
    "            \n",
    "            # 测试集误差\n",
    "            plt.subplot(1, 2, 2)\n",
    "            test_errors = y_test[target] - test_weighted_preds\n",
    "            plt.hist(test_errors, bins=30, alpha=0.7)\n",
    "            plt.axvline(x=0, color='r', linestyle='--')\n",
    "            plt.xlabel('预测误差')\n",
    "            plt.ylabel('频次')\n",
    "            plt.title(f'测试集误差分布 (MAE={np.abs(test_errors).mean():.4f})')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 绘制权重条形图\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.bar(available_models, optimal_weights)\n",
    "            plt.xlabel('模型')\n",
    "            plt.ylabel('权重')\n",
    "            plt.title(f'{target} - 加权平均集成模型权重分布')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(True, axis='y')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 绘制各模型与加权平均模型的预测对比图\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            model_data = [test_weighted_preds] + [test_predictions[model] for model in available_models]\n",
    "            model_labels = ['加权平均'] + available_models\n",
    "            \n",
    "            plt.boxplot(model_data, labels=model_labels)\n",
    "            plt.ylabel('预测值')\n",
    "            plt.title('加权平均模型与各基础模型预测分布对比')\n",
    "            plt.grid(True, axis='y')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "            print(\"权重优化失败:\", result.message)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"创建加权平均集成模型失败: {str(e)}\")\n",
    "        print(f\"错误详情: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于区间的集成模型 - 每个模型在不同区间的最佳表现\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"训练 {target} 的基于区间的集成模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 创建有效模型列表及其预测结果\n",
    "available_models = []\n",
    "train_predictions = {}\n",
    "test_predictions = {}\n",
    "model_datasets = {}\n",
    "\n",
    "if 'XGBoost' in models[target]:\n",
    "    model = models[target]['XGBoost']\n",
    "    train_pred = model.predict(X_train_tree[target])\n",
    "    test_pred = model.predict(X_test_tree[target])\n",
    "    available_models.append('XGBoost')\n",
    "    train_predictions['XGBoost'] = train_pred\n",
    "    test_predictions['XGBoost'] = test_pred\n",
    "    model_datasets['XGBoost'] = 'tree'\n",
    "\n",
    "if 'LightGBM' in models[target]:\n",
    "    model = models[target]['LightGBM']\n",
    "    train_pred = model.predict(X_train_tree[target])\n",
    "    test_pred = model.predict(X_test_tree[target])\n",
    "    available_models.append('LightGBM')\n",
    "    train_predictions['LightGBM'] = train_pred\n",
    "    test_predictions['LightGBM'] = test_pred\n",
    "    model_datasets['LightGBM'] = 'tree'\n",
    "    \n",
    "if 'HistGradientBoosting' in models[target]:\n",
    "    model = models[target]['HistGradientBoosting']\n",
    "    train_pred = model.predict(X_train_tree[target])\n",
    "    test_pred = model.predict(X_test_tree[target])\n",
    "    available_models.append('HistGradientBoosting')\n",
    "    train_predictions['HistGradientBoosting'] = train_pred\n",
    "    test_predictions['HistGradientBoosting'] = test_pred\n",
    "    model_datasets['HistGradientBoosting'] = 'tree'\n",
    "    \n",
    "if 'RandomForest' in models[target]:\n",
    "    model = models[target]['RandomForest']\n",
    "    train_pred = model.predict(X_train_tree_filled[target])\n",
    "    test_pred = model.predict(X_test_tree_filled[target])\n",
    "    available_models.append('RandomForest')\n",
    "    train_predictions['RandomForest'] = train_pred\n",
    "    test_predictions['RandomForest'] = test_pred\n",
    "    model_datasets['RandomForest'] = 'tree_filled'\n",
    "    \n",
    "if 'GaussianProcess' in models[target]:\n",
    "    model = models[target]['GaussianProcess']\n",
    "    train_pred = model.predict(X_train_linear[target])\n",
    "    test_pred = model.predict(X_test_linear[target])\n",
    "    available_models.append('GaussianProcess')\n",
    "    train_predictions['GaussianProcess'] = train_pred\n",
    "    test_predictions['GaussianProcess'] = test_pred\n",
    "    model_datasets['GaussianProcess'] = 'linear'\n",
    "\n",
    "print(f\"可用模型: {available_models}\")\n",
    "\n",
    "if len(available_models) < 2:\n",
    "    print(\"基于区间的集成至少需要两个模型，目前可用模型不足\")\n",
    "else:\n",
    "    try:\n",
    "        # 步骤1: 确定区间划分策略\n",
    "        # 方法1: 基于目标变量值的分位数划分\n",
    "        # 方法2: 基于预测误差的分布划分\n",
    "        # 方法3: 自适应区间划分\n",
    "        \n",
    "        print(\"分析目标变量分布并确定区间划分...\")\n",
    "        \n",
    "        # 分析目标变量的分布\n",
    "        y_train_values = y_train[target].values\n",
    "        y_min, y_max = y_train_values.min(), y_train_values.max()\n",
    "        \n",
    "        # 尝试不同的区间数量，找到最佳的\n",
    "        best_num_intervals = 3\n",
    "        best_overall_r2 = -np.inf\n",
    "        best_interval_config = None\n",
    "        \n",
    "        print(\"尝试不同的区间划分数量...\")\n",
    "        for num_intervals in range(3, 8):  # 尝试3到7个区间\n",
    "            print(f\"\\n尝试 {num_intervals} 个区间:\")\n",
    "            \n",
    "            # 使用分位数划分区间\n",
    "            quantiles = np.linspace(0, 1, num_intervals + 1)\n",
    "            interval_boundaries = np.quantile(y_train_values, quantiles)\n",
    "            \n",
    "            # 确保边界值不重复\n",
    "            interval_boundaries = np.unique(interval_boundaries)\n",
    "            actual_num_intervals = len(interval_boundaries) - 1\n",
    "            \n",
    "            if actual_num_intervals < 2:\n",
    "                continue\n",
    "                \n",
    "            # 为每个区间找到最佳模型\n",
    "            interval_best_models = {}\n",
    "            interval_performances = {}\n",
    "            \n",
    "            valid_intervals = 0\n",
    "            total_weighted_r2 = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for i in range(actual_num_intervals):\n",
    "                lower_bound = interval_boundaries[i]\n",
    "                upper_bound = interval_boundaries[i + 1] \n",
    "                \n",
    "                # 找到属于当前区间的样本\n",
    "                if i == actual_num_intervals - 1:  # 最后一个区间包含上界\n",
    "                    mask = (y_train_values >= lower_bound) & (y_train_values <= upper_bound)\n",
    "                else:\n",
    "                    mask = (y_train_values >= lower_bound) & (y_train_values < upper_bound)\n",
    "                \n",
    "                interval_samples = np.sum(mask)\n",
    "                if interval_samples < 10:  # 区间内样本太少，跳过\n",
    "                    continue\n",
    "                \n",
    "                # 在当前区间内评估所有模型\n",
    "                y_interval = y_train_values[mask]\n",
    "                best_r2 = -np.inf\n",
    "                best_model = None\n",
    "                \n",
    "                interval_r2_scores = {}\n",
    "                for model_name in available_models:\n",
    "                    pred_interval = train_predictions[model_name][mask]\n",
    "                    r2 = r2_score(y_interval, pred_interval)\n",
    "                    interval_r2_scores[model_name] = r2\n",
    "                    \n",
    "                    if r2 > best_r2:\n",
    "                        best_r2 = r2\n",
    "                        best_model = model_name\n",
    "                \n",
    "                if best_model:\n",
    "                    interval_best_models[i] = best_model\n",
    "                    interval_performances[i] = {\n",
    "                        'best_model': best_model,\n",
    "                        'best_r2': best_r2,\n",
    "                        'bounds': (lower_bound, upper_bound),\n",
    "                        'samples': interval_samples,\n",
    "                        'all_r2': interval_r2_scores\n",
    "                    }\n",
    "                    \n",
    "                    # 计算加权R²\n",
    "                    total_weighted_r2 += best_r2 * interval_samples\n",
    "                    total_samples += interval_samples\n",
    "                    valid_intervals += 1\n",
    "            \n",
    "            if valid_intervals >= 2 and total_samples > 0:\n",
    "                overall_r2 = total_weighted_r2 / total_samples\n",
    "                print(f\"  有效区间数: {valid_intervals}, 总体加权R²: {overall_r2:.4f}\")\n",
    "                \n",
    "                if overall_r2 > best_overall_r2:\n",
    "                    best_overall_r2 = overall_r2\n",
    "                    best_num_intervals = actual_num_intervals\n",
    "                    best_interval_config = {\n",
    "                        'boundaries': interval_boundaries,\n",
    "                        'models': interval_best_models,\n",
    "                        'performances': interval_performances,\n",
    "                        'num_intervals': actual_num_intervals\n",
    "                    }\n",
    "        \n",
    "        if best_interval_config is None:\n",
    "            print(\"无法找到合适的区间划分，回退到简单集成\")\n",
    "        else:\n",
    "            print(f\"\\n选择最佳配置: {best_num_intervals} 个区间, 总体R²: {best_overall_r2:.4f}\")\n",
    "            \n",
    "            # 使用最佳配置\n",
    "            interval_boundaries = best_interval_config['boundaries']\n",
    "            interval_best_models = best_interval_config['models']\n",
    "            interval_performances = best_interval_config['performances']\n",
    "            \n",
    "            # 打印每个区间的详细信息\n",
    "            print(\"\\n区间划分详情:\")\n",
    "            for i, perf in interval_performances.items():\n",
    "                bounds = perf['bounds']\n",
    "                best_model = perf['best_model']\n",
    "                best_r2 = perf['best_r2']\n",
    "                samples = perf['samples']\n",
    "                \n",
    "                print(f\"区间 {i+1}: [{bounds[0]:.3f}, {bounds[1]:.3f}]\")\n",
    "                print(f\"  最佳模型: {best_model} (R²: {best_r2:.4f})\")\n",
    "                print(f\"  样本数: {samples}\")\n",
    "                print(f\"  所有模型R²: {perf['all_r2']}\")\n",
    "                print()\n",
    "            \n",
    "            # 步骤2: 基于区间的预测函数\n",
    "            def interval_based_prediction(y_values, is_training=True):\n",
    "                \"\"\"基于区间选择模型进行预测\"\"\"\n",
    "                predictions = np.zeros(len(y_values))\n",
    "                model_usage = {model: 0 for model in available_models}\n",
    "                \n",
    "                for i in range(len(y_values)):\n",
    "                    y_val = y_values[i]\n",
    "                    \n",
    "                    # 找到当前值属于哪个区间\n",
    "                    selected_model = None\n",
    "                    for interval_idx, perf in interval_performances.items():\n",
    "                        lower_bound, upper_bound = perf['bounds']\n",
    "                        \n",
    "                        if interval_idx == max(interval_performances.keys()):  # 最后一个区间\n",
    "                            if lower_bound <= y_val <= upper_bound:\n",
    "                                selected_model = perf['best_model']\n",
    "                                break\n",
    "                        else:\n",
    "                            if lower_bound <= y_val < upper_bound:\n",
    "                                selected_model = perf['best_model']\n",
    "                                break\n",
    "                    \n",
    "                    # 如果没有找到合适的区间，使用全局最佳模型\n",
    "                    if selected_model is None:\n",
    "                        # 找到全局R²最高的模型\n",
    "                        global_r2_scores = {}\n",
    "                        for model_name in available_models:\n",
    "                            if is_training:\n",
    "                                pred_all = train_predictions[model_name]\n",
    "                                y_all = y_train[target].values\n",
    "                            else:\n",
    "                                pred_all = test_predictions[model_name] \n",
    "                                y_all = y_test[target].values\n",
    "                            global_r2_scores[model_name] = r2_score(y_all, pred_all)\n",
    "                        \n",
    "                        selected_model = max(global_r2_scores, key=global_r2_scores.get)\n",
    "                    \n",
    "                    # 使用选定的模型进行预测\n",
    "                    if is_training:\n",
    "                        predictions[i] = train_predictions[selected_model][i]\n",
    "                    else:\n",
    "                        predictions[i] = test_predictions[selected_model][i]\n",
    "                    \n",
    "                    model_usage[selected_model] += 1\n",
    "                \n",
    "                return predictions, model_usage\n",
    "            \n",
    "            # 步骤3: 在训练集和测试集上应用基于区间的预测\n",
    "            print(\"应用基于区间的预测...\")\n",
    "            \n",
    "            # 训练集预测（用真实值确定区间）\n",
    "            train_interval_preds, train_model_usage = interval_based_prediction(\n",
    "                y_train[target].values, is_training=True\n",
    "            )\n",
    "            \n",
    "            # 测试集预测 - 这里需要特殊处理，因为我们不知道真实值\n",
    "            # 方法1: 使用预测值的平均来估计区间\n",
    "            # 方法2: 使用特征来预测区间\n",
    "            # 方法3: 使用所有模型的平均预测值来估计区间\n",
    "            \n",
    "            print(\"为测试集确定区间...\")\n",
    "            # 使用所有模型预测的平均值来估计测试样本的区间\n",
    "            test_avg_predictions = np.mean([test_predictions[model] for model in available_models], axis=0)\n",
    "            test_interval_preds, test_model_usage = interval_based_prediction(\n",
    "                test_avg_predictions, is_training=False\n",
    "            )\n",
    "            \n",
    "            # 步骤4: 评估性能\n",
    "            train_r2 = r2_score(y_train[target], train_interval_preds)\n",
    "            train_tol_r2 = tolerance_r2_score(y_train[target], train_interval_preds, tolerance=current_tolerance, target=target)\n",
    "            train_within_tol = prediction_within_tolerance(y_train[target], train_interval_preds, tolerance=current_tolerance, target=target)\n",
    "            \n",
    "            test_r2 = r2_score(y_test[target], test_interval_preds)\n",
    "            test_tol_r2 = tolerance_r2_score(y_test[target], test_interval_preds, tolerance=current_tolerance, target=target)\n",
    "            test_within_tol = prediction_within_tolerance(y_test[target], test_interval_preds, tolerance=current_tolerance, target=target)\n",
    "            \n",
    "            print(f\"\\n基于区间的集成模型性能:\")\n",
    "            print(f\"训练集: R²={train_r2:.4f}, 容忍度R²={train_tol_r2:.4f}, 在容忍范围内比例={train_within_tol:.2%}\")\n",
    "            print(f\"测试集: R²={test_r2:.4f}, 容忍度R²={test_tol_r2:.4f}, 在容忍范围内比例={test_within_tol:.2%}\")\n",
    "            \n",
    "            # 步骤5: 分析模型使用频率\n",
    "            print(\"\\n训练集中各模型使用频率:\")\n",
    "            for model, count in train_model_usage.items():\n",
    "                percentage = count / len(y_train[target]) * 100\n",
    "                print(f\"  {model}: {count} 次 ({percentage:.2f}%)\")\n",
    "            \n",
    "            print(\"\\n测试集中各模型使用频率:\")\n",
    "            for model, count in test_model_usage.items():\n",
    "                percentage = count / len(y_test[target]) * 100\n",
    "                print(f\"  {model}: {count} 次 ({percentage:.2f}%)\")\n",
    "            \n",
    "            # 步骤6: 与各基础模型比较性能\n",
    "            print(\"\\n与各基础模型性能比较:\")\n",
    "            for model_name in available_models:\n",
    "                model_test_pred = test_predictions[model_name]\n",
    "                model_r2 = r2_score(y_test[target], model_test_pred)\n",
    "                r2_diff = test_r2 - model_r2\n",
    "                \n",
    "                print(f\"  vs {model_name}:\")\n",
    "                print(f\"    测试集R²: {test_r2:.4f} vs {model_r2:.4f} (差异: {r2_diff:.4f})\")\n",
    "            \n",
    "            # 步骤7: 创建基于区间的集成模型类\n",
    "            class IntervalBasedEnsemble:\n",
    "                def __init__(self, models_dict, interval_config, available_models, model_datasets):\n",
    "                    self.models_dict = models_dict\n",
    "                    self.interval_boundaries = interval_config['boundaries']\n",
    "                    self.interval_performances = interval_config['performances']\n",
    "                    self.available_models = available_models\n",
    "                    self.model_datasets = model_datasets\n",
    "                    \n",
    "                    # 预计算全局最佳模型（用作后备）\n",
    "                    self.global_best_model = max(available_models, \n",
    "                        key=lambda m: np.mean([perf['all_r2'][m] for perf in interval_config['performances'].values()]))\n",
    "                \n",
    "                def predict(self, X):\n",
    "                    # 首先获取所有模型的预测\n",
    "                    all_predictions = {}\n",
    "                    \n",
    "                    for model_name in self.available_models:\n",
    "                        model = self.models_dict[model_name]\n",
    "                        \n",
    "                        # 根据模型类型准备数据\n",
    "                        if model_name in ['XGBoost', 'LightGBM', 'HistGradientBoosting']:\n",
    "                            X_model = X\n",
    "                        elif model_name == 'RandomForest':\n",
    "                            X_model = X.fillna(0) if isinstance(X, pd.DataFrame) else X\n",
    "                        elif model_name == 'GaussianProcess':\n",
    "                            X_model = X\n",
    "                        else:\n",
    "                            X_model = X\n",
    "                        \n",
    "                        all_predictions[model_name] = model.predict(X_model)\n",
    "                    \n",
    "                    # 使用所有模型的平均预测来估计区间\n",
    "                    avg_predictions = np.mean(list(all_predictions.values()), axis=0)\n",
    "                    \n",
    "                    # 为每个样本选择合适的模型\n",
    "                    final_predictions = np.zeros(len(avg_predictions))\n",
    "                    \n",
    "                    for i in range(len(avg_predictions)):\n",
    "                        pred_val = avg_predictions[i]\n",
    "                        \n",
    "                        # 找到预测值属于哪个区间\n",
    "                        selected_model = None\n",
    "                        for interval_idx, perf in self.interval_performances.items():\n",
    "                            lower_bound, upper_bound = perf['bounds']\n",
    "                            \n",
    "                            if interval_idx == max(self.interval_performances.keys()):\n",
    "                                if lower_bound <= pred_val <= upper_bound:\n",
    "                                    selected_model = perf['best_model']\n",
    "                                    break\n",
    "                            else:\n",
    "                                if lower_bound <= pred_val < upper_bound:\n",
    "                                    selected_model = perf['best_model']\n",
    "                                    break\n",
    "                        \n",
    "                        # 如果没有找到，使用全局最佳模型\n",
    "                        if selected_model is None:\n",
    "                            selected_model = self.global_best_model\n",
    "                        \n",
    "                        final_predictions[i] = all_predictions[selected_model][i]\n",
    "                    \n",
    "                    return final_predictions\n",
    "                \n",
    "                def get_interval_info(self):\n",
    "                    \"\"\"返回区间配置信息\"\"\"\n",
    "                    return {\n",
    "                        'boundaries': self.interval_boundaries,\n",
    "                        'performances': self.interval_performances,\n",
    "                        'global_best': self.global_best_model\n",
    "                    }\n",
    "            \n",
    "            # 创建基于区间的集成模型\n",
    "            interval_model = IntervalBasedEnsemble(\n",
    "                models_dict=models[target],\n",
    "                interval_config=best_interval_config,\n",
    "                available_models=available_models,\n",
    "                model_datasets=model_datasets\n",
    "            )\n",
    "            \n",
    "            # 保存模型\n",
    "            models[target]['IntervalEnsemble'] = interval_model\n",
    "            \n",
    "            # 使用pickle保存模型\n",
    "            interval_model_file = os.path.join(model_folder, f'{target}_基于区间集成模型.pkl')\n",
    "            with open(interval_model_file, 'wb') as f:\n",
    "                pickle.dump(interval_model, f)\n",
    "            print(f\"基于区间的集成模型已保存至 {interval_model_file}\")\n",
    "            \n",
    "            # 保存预测结果\n",
    "            train_prediction = pd.DataFrame({\n",
    "                '实际值': y_train[target],\n",
    "                '区间集成预测值': train_interval_preds,\n",
    "                '误差': np.abs(y_train[target] - train_interval_preds)\n",
    "            })\n",
    "\n",
    "            test_prediction = pd.DataFrame({\n",
    "                '实际值': y_test[target],\n",
    "                '区间集成预测值': test_interval_preds,\n",
    "                '误差': np.abs(y_test[target] - test_interval_preds)\n",
    "            })\n",
    "\n",
    "            # 添加各基础模型的预测结果\n",
    "            for model_name in available_models:\n",
    "                train_prediction[f'{model_name}预测值'] = train_predictions[model_name]\n",
    "                test_prediction[f'{model_name}预测值'] = test_predictions[model_name]\n",
    "\n",
    "            # 保存到文件\n",
    "            train_file = os.path.join(save_folder, f'{target}_基于区间集成训练集预测结果.csv')\n",
    "            test_file = os.path.join(save_folder, f'{target}_基于区间集成测试集预测结果.csv')\n",
    "\n",
    "            train_prediction.to_csv(train_file, index=False)\n",
    "            test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "            print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "            print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "            \n",
    "            # 可视化部分\n",
    "            # 1. 预测vs实际值散点图\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(y_train[target], train_interval_preds, alpha=0.5)\n",
    "            plt.plot([y_train[target].min(), y_train[target].max()], [y_train[target].min(), y_train[target].max()], 'r--')\n",
    "            plt.xlabel('实际值')\n",
    "            plt.ylabel('预测值')\n",
    "            plt.title(f'训练集: R²={train_r2:.4f}')\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.scatter(y_test[target], test_interval_preds, alpha=0.5)\n",
    "            plt.plot([y_test[target].min(), y_test[target].max()], [y_test[target].min(), y_test[target].max()], 'r--')\n",
    "            plt.xlabel('实际值')\n",
    "            plt.ylabel('预测值')\n",
    "            plt.title(f'测试集: R²={test_r2:.4f}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 2. 区间划分和模型选择可视化\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            \n",
    "            # 子图1: 区间划分\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.hist(y_train[target], bins=30, alpha=0.7, label='目标变量分布')\n",
    "            for boundary in interval_boundaries[1:-1]:  # 不画最小和最大边界\n",
    "                plt.axvline(x=boundary, color='red', linestyle='--', alpha=0.8)\n",
    "            plt.xlabel('目标变量值')\n",
    "            plt.ylabel('频次')\n",
    "            plt.title('区间划分')\n",
    "            plt.legend()\n",
    "            \n",
    "            # 子图2: 每个区间的最佳模型\n",
    "            plt.subplot(2, 2, 2)\n",
    "            interval_nums = list(interval_performances.keys())\n",
    "            best_models = [interval_performances[i]['best_model'] for i in interval_nums]\n",
    "            best_r2s = [interval_performances[i]['best_r2'] for i in interval_nums]\n",
    "            \n",
    "            bars = plt.bar(range(len(interval_nums)), best_r2s)\n",
    "            plt.xlabel('区间编号')\n",
    "            plt.ylabel('最佳R²')\n",
    "            plt.title('各区间最佳模型性能')\n",
    "            plt.xticks(range(len(interval_nums)), [f'区间{i+1}\\n{best_models[i]}' for i in range(len(best_models))], rotation=45)\n",
    "            \n",
    "            # 子图3: 模型使用频率对比\n",
    "            plt.subplot(2, 2, 3)\n",
    "            models_list = list(train_model_usage.keys())\n",
    "            train_usage = [train_model_usage[m] for m in models_list]\n",
    "            test_usage = [test_model_usage[m] for m in models_list]\n",
    "            \n",
    "            x = np.arange(len(models_list))\n",
    "            width = 0.35\n",
    "            \n",
    "            plt.bar(x - width/2, train_usage, width, label='训练集', alpha=0.8)\n",
    "            plt.bar(x + width/2, test_usage, width, label='测试集', alpha=0.8)\n",
    "            plt.xlabel('模型')\n",
    "            plt.ylabel('使用次数')\n",
    "            plt.title('各模型使用频率对比')\n",
    "            plt.xticks(x, models_list, rotation=45)\n",
    "            plt.legend()\n",
    "            \n",
    "            # 子图4: 误差分布\n",
    "            plt.subplot(2, 2, 4)\n",
    "            test_errors = y_test[target] - test_interval_preds\n",
    "            plt.hist(test_errors, bins=30, alpha=0.7)\n",
    "            plt.axvline(x=0, color='r', linestyle='--')\n",
    "            plt.xlabel('预测误差')\n",
    "            plt.ylabel('频次')\n",
    "            plt.title(f'测试集误差分布 (MAE={np.abs(test_errors).mean():.4f})')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 3. 各区间内所有模型的R²对比热力图\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            # 准备热力图数据\n",
    "            interval_labels = [f\"区间{i+1}\\n[{perf['bounds'][0]:.2f}, {perf['bounds'][1]:.2f}]\" \n",
    "                             for i, perf in interval_performances.items()]\n",
    "            \n",
    "            r2_matrix = []\n",
    "            for i, perf in interval_performances.items():\n",
    "                r2_row = [perf['all_r2'][model] for model in available_models]\n",
    "                r2_matrix.append(r2_row)\n",
    "            \n",
    "            r2_matrix = np.array(r2_matrix)\n",
    "            \n",
    "            im = plt.imshow(r2_matrix, cmap='RdYlGn', aspect='auto')\n",
    "            plt.colorbar(im, label='R² Score')\n",
    "            \n",
    "            plt.xlabel('模型')\n",
    "            plt.ylabel('区间')\n",
    "            plt.title('各区间内所有模型的R²表现热力图')\n",
    "            \n",
    "            plt.xticks(range(len(available_models)), available_models, rotation=45)\n",
    "            plt.yticks(range(len(interval_labels)), interval_labels)\n",
    "            \n",
    "            # 在每个格子中标注数值\n",
    "            for i in range(len(interval_labels)):\n",
    "                for j in range(len(available_models)):\n",
    "                    plt.text(j, i, f'{r2_matrix[i, j]:.3f}', \n",
    "                           ha=\"center\", va=\"center\", color=\"black\" if r2_matrix[i, j] < 0.5 else \"white\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"创建基于区间的集成模型失败: {str(e)}\")\n",
    "        print(f\"错误详情: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 循环使用次数XGBoost贝叶斯超参数优化\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 创建保存数据的文件夹\n",
    "save_folder = '模型可视化数据'\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "    print(f\"已创建文件夹：{save_folder}\")\n",
    "\n",
    "# 选择目标变量\n",
    "target = '循环使用次数'\n",
    "print(f\"训练 {target} 的XGBoost模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.03)\n",
    "\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_tree[target]\n",
    "X_test_model = X_test_tree[target]\n",
    "\n",
    "# 从您的代码中复制的完整函数定义\n",
    "def tolerance_r2_score(y_true, y_pred, tolerance=0.15, target=None):\n",
    "    \"\"\"\n",
    "    计算容忍度R²评分，允许一定误差范围内的预测被视为准确\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    \n",
    "    if target and target in target_tolerance:\n",
    "        tolerance = target_tolerance[target]\n",
    "    \n",
    "    tolerance_values = tolerance * np.abs(y_true)\n",
    "    residuals = np.abs(y_true - y_pred)\n",
    "    adjusted_residuals = np.maximum(0, residuals - tolerance_values)\n",
    "    \n",
    "    y_true_mean = np.mean(y_true)\n",
    "    tss = np.sum((y_true - y_true_mean) ** 2)\n",
    "    rss = np.sum(adjusted_residuals ** 2)\n",
    "    \n",
    "    if tss == 0:\n",
    "        return 0\n",
    "    \n",
    "    tolerance_r2 = 1 - (rss / tss)\n",
    "    return tolerance_r2\n",
    "\n",
    "def prediction_within_tolerance(y_true, y_pred, tolerance=0.15, target=None):\n",
    "    \"\"\"\n",
    "    计算预测值在目标值±容忍范围内的比例\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    if target and target in target_tolerance:\n",
    "        tolerance = target_tolerance[target]\n",
    "    \n",
    "    tolerance_values = tolerance * np.abs(y_true)\n",
    "    within_tolerance = np.abs(y_true - y_pred) <= tolerance_values\n",
    "    \n",
    "    return np.mean(within_tolerance)\n",
    "\n",
    "def make_tolerance_scorer(target_name):\n",
    "    def tolerance_score(y_true, y_pred):\n",
    "        tolerance = target_tolerance.get(target_name, 0.03)\n",
    "        relative_errors = np.abs(y_true - y_pred) / np.abs(y_true)\n",
    "        within_tolerance = np.mean(relative_errors <= tolerance)\n",
    "        return within_tolerance\n",
    "    return tolerance_score\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, target, model_name):\n",
    "    \"\"\"\n",
    "    评估模型在训练集和测试集上的性能，包括标准R²和容忍度R²\n",
    "    \"\"\"\n",
    "    current_tolerance = target_tolerance.get(target, 0.15)\n",
    "    \n",
    "    # 在训练集上评估\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    if len(y_train_pred.shape) > 1 and y_train_pred.shape[1] == 1:\n",
    "        y_train_pred = y_train_pred.flatten()\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    train_tol_r2 = tolerance_r2_score(y_train, y_train_pred, tolerance=current_tolerance, target=target)\n",
    "    train_within_tol = prediction_within_tolerance(y_train, y_train_pred, tolerance=current_tolerance, target=target)\n",
    "    \n",
    "    # 在测试集上评估\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    if len(y_test_pred.shape) > 1 and y_test_pred.shape[1] == 1:\n",
    "        y_test_pred = y_test_pred.flatten()\n",
    "        \n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_tol_r2 = tolerance_r2_score(y_test, y_test_pred, tolerance=current_tolerance, target=target)\n",
    "    test_within_tol = prediction_within_tolerance(y_test, y_test_pred, tolerance=current_tolerance, target=target)\n",
    "    \n",
    "    print(f\"\\n{model_name} 在 {target} 上的评估结果:\")\n",
    "    print(f\"训练集: R²={train_r2:.4f}, 容忍度R²={train_tol_r2:.4f}, 在容忍范围内比例={train_within_tol:.2%}\")\n",
    "    print(f\"测试集: R²={test_r2:.4f}, 容忍度R²={test_tol_r2:.4f}, 在容忍范围内比例={test_within_tol:.2%}\")\n",
    "    \n",
    "    # 绘制预测值与实际值的对比散点图\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 训练集散点图\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_train, y_train_pred, alpha=0.6, s=30)\n",
    "    min_val = min(min(y_train), min(y_train_pred))\n",
    "    max_val = max(max(y_train), max(y_train_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "    plt.xlabel('实际值')\n",
    "    plt.ylabel('预测值')\n",
    "    plt.title(f'训练集: R²={train_r2:.4f}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 测试集散点图\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_test, y_test_pred, alpha=0.6, s=30)\n",
    "    min_val = min(min(y_test), min(y_test_pred))\n",
    "    max_val = max(max(y_test), max(y_test_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "    plt.xlabel('实际值')\n",
    "    plt.ylabel('预测值')\n",
    "    plt.title(f'测试集: R²={test_r2:.4f}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'train_r2': train_r2,\n",
    "        'train_tol_r2': train_tol_r2,\n",
    "        'train_within_tol': train_within_tol,\n",
    "        'test_r2': test_r2,\n",
    "        'test_tol_r2': test_tol_r2,\n",
    "        'test_within_tol': test_within_tol\n",
    "    }\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# 设置基础参数\n",
    "base_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'hist',\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42,\n",
    "    'missing': np.nan\n",
    "}\n",
    "\n",
    "# 定义贝叶斯优化的搜索空间（基于您原始代码的参数范围）\n",
    "dimensions = [\n",
    "    Integer(50, 100, name='n_estimators'),           # 您原始代码：[100, 50, 90, 80]\n",
    "    Real(0.5, 0.8, name='learning_rate'),            # 您原始代码：[0.7, 0.8, 0.6, 0.5]\n",
    "    Integer(3, 6, name='max_depth'),                 # 您原始代码：[4, 5, 6, 3]\n",
    "    Integer(2, 6, name='min_child_weight'),          # 您原始代码：[5, 4, 6, 3, 2]\n",
    "    Real(0.0, 0.2, name='gamma'),                    # 您原始代码：[0, 0.1, 0.2]\n",
    "    Real(0.5, 0.6, name='subsample'),               # 您原始代码：[0.6, 0.5]\n",
    "    Real(0.8, 1.0, name='colsample_bytree'),        # 您原始代码：[0.9, 1.0, 0.8]\n",
    "    Real(0.0, 1.0, name='reg_alpha'),               # 您原始代码：[0, 0.4, 0.5, 0.6, 1.0]\n",
    "    Real(0.5, 1.0, name='reg_lambda')               # 您原始代码：[1.0, 0.5, 0.7, 0.8]\n",
    "]\n",
    "\n",
    "# 交叉验证设置\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 定义目标函数\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def objective(**params):\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=params['max_depth'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        gamma=params['gamma'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        **base_params\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        cv_scores = cross_val_score(\n",
    "            model, X_train_model, y_train[target], \n",
    "            cv=kf, scoring=tol_scorer_wrapped\n",
    "        )\n",
    "        return -cv_scores.mean()\n",
    "    except:\n",
    "        return 1.0\n",
    "\n",
    "# 执行贝叶斯优化\n",
    "print(\"执行贝叶斯优化...\")\n",
    "result = gp_minimize(\n",
    "    func=objective,\n",
    "    dimensions=dimensions,\n",
    "    n_calls=50,\n",
    "    n_initial_points=10,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 获取最佳参数\n",
    "best_params = dict(zip([dim.name for dim in dimensions], result.x))\n",
    "print(f\"最佳参数: {best_params}\")\n",
    "print(f\"最佳CV得分: {-result.fun:.4f}\")\n",
    "\n",
    "# 使用最佳参数创建最终模型\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_child_weight=best_params['min_child_weight'],\n",
    "    gamma=best_params['gamma'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree'],\n",
    "    reg_alpha=best_params['reg_alpha'],\n",
    "    reg_lambda=best_params['reg_lambda'],\n",
    "    **base_params\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_model, y_train[target])\n",
    "\n",
    "# 交叉验证\n",
    "cv_scores = cross_val_score(xgb_model, X_train_model, y_train[target], cv=kf, scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    xgb_model, X_train_model, y_train[target], cv=kf, \n",
    "    scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "models[target]['XGBoost'] = xgb_model\n",
    "\n",
    "# 创建模型保存文件夹\n",
    "model_folder = '训练模型文件'\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    print(f\"已创建模型文件夹：{model_folder}\")\n",
    "\n",
    "xgb_model_file = os.path.join(model_folder, f'{target}_XGBoost模型.pkl')\n",
    "with open(xgb_model_file, 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "print(f\"XGBoost 模型已保存至 {xgb_model_file}\")\n",
    "\n",
    "# 评估模型\n",
    "results = evaluate_model(xgb_model, X_train_model, y_train[target], X_test_model, y_test[target], target, \"XGBoost\")\n",
    "\n",
    "# 获取预测值\n",
    "y_pred_train = xgb_model.predict(X_train_model)\n",
    "y_pred_test = xgb_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到单独的文件\n",
    "train_file = os.path.join(save_folder, f'{target}_XGBoost训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_XGBoost测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "# 特征重要性\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - XGBoost特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_XGBoost特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "\n",
    "# 绘制优化收敛图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_convergence(result)\n",
    "plt.title('贝叶斯优化收敛过程')\n",
    "plt.show()\n",
    "\n",
    "# 保存优化结果\n",
    "optimization_history = pd.DataFrame({\n",
    "    '迭代次数': range(1, len(result.func_vals) + 1),\n",
    "    '目标函数值': result.func_vals,\n",
    "    '最佳目标函数值': [min(result.func_vals[:i+1]) for i in range(len(result.func_vals))]\n",
    "})\n",
    "\n",
    "param_names = [dim.name for dim in dimensions]\n",
    "for i, param_name in enumerate(param_names):\n",
    "    optimization_history[f'参数_{param_name}'] = [x[i] for x in result.x_iters]\n",
    "\n",
    "optimization_history_file = os.path.join(save_folder, f'{target}_贝叶斯优化历史.csv')\n",
    "optimization_history.to_csv(optimization_history_file, index=False)\n",
    "print(f\"优化历史数据已保存至 {optimization_history_file}\")\n",
    "\n",
    "print(f\"\\n贝叶斯优化完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 前置准备工作 (请确保这些变量在您的代码中已定义) ---\n",
    "# 假设您已经加载并准备好了以下数据和变量:\n",
    "# X_train_tree, X_test_tree: 特征数据，类型为 pd.DataFrame\n",
    "# y_train, y_test: 目标数据，类型为 pd.DataFrame 或 pd.Series\n",
    "# target_tolerance: 一个字典，包含每个目标的容忍度，例如 {'循环使用次数': 0.03}\n",
    "# make_tolerance_scorer: 一个函数，用于创建您的自定义评分器\n",
    "# evaluate_model: 一个函数，用于评估模型并返回结果\n",
    "# kf: 一个交叉验证折叠器，例如 KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 示例占位符 (如果您的代码中没有这些，请取消注释并按需修改)\n",
    "# from sklearn.model_selection import KFold\n",
    "# def make_tolerance_scorer(target, tolerance=0.03):\n",
    "#     def tolerance_scorer(y_true, y_pred):\n",
    "#         return np.mean(np.abs(y_true - y_pred) <= np.abs(y_true * tolerance))\n",
    "#     return make_scorer(tolerance_scorer, greater_is_better=True)\n",
    "#\n",
    "# def evaluate_model(model, X_train, y_train, X_test, y_test, target_name, model_name):\n",
    "#     y_pred_train = model.predict(X_train)\n",
    "#     y_pred_test = model.predict(X_test)\n",
    "#     r2_train = r2_score(y_train, y_pred_train)\n",
    "#     r2_test = r2_score(y_test, y_pred_test)\n",
    "#     print(f\"--- {model_name} for {target_name} 评估结果 ---\")\n",
    "#     print(f\"训练集 R²: {r2_train:.4f}\")\n",
    "#     print(f\"测试集 R²: {r2_test:.4f}\")\n",
    "#     return {\"R2_Train\": r2_train, \"R2_Test\": r2_test}\n",
    "#\n",
    "# # 假设的数据 (用作示例)\n",
    "# from sklearn.datasets import make_regression\n",
    "# X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "# X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(20)])\n",
    "# y = pd.Series(y, name='循环使用次数')\n",
    "# y = (y - y.min()) * 10 # 确保y值为正\n",
    "# X_train, X_test, y_train_full, y_test_full = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train_tree = {'循环使用次数': X_train}\n",
    "# X_test_tree = {'循环使用次数': X_test}\n",
    "# y_train = pd.DataFrame({'循环使用次数': y_train_full})\n",
    "# y_test = pd.DataFrame({'循环使用次数': y_test_full})\n",
    "# target_tolerance = {'循环使用次数': 0.05}\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# # --- 前置准备工作结束 ---\n",
    "\n",
    "\n",
    "# --- 主要训练流程 ---\n",
    "\n",
    "# 1. 初始化设置\n",
    "target = '循环使用次数'\n",
    "save_folder = '模型可视化数据'\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "    print(f\"已创建文件夹：{save_folder}\")\n",
    "\n",
    "print(f\"开始训练 {target} 的XGBoost模型...\")\n",
    "\n",
    "# 2. 数据和评分器准备\n",
    "current_tolerance = target_tolerance.get(target, 0.03)\n",
    "X_train_model = X_train_tree[target]\n",
    "X_test_model = X_test_tree[target]\n",
    "y_train_target = y_train[target]\n",
    "y_test_target = y_test[target]\n",
    "\n",
    "# 为当前目标创建自定义评分器\n",
    "# tol_scorer = make_tolerance_scorer(target, tolerance=current_tolerance) # 如果您的函数需要传入target和tolerance\n",
    "tol_scorer = make_tolerance_scorer(target) # 根据您原始代码的调用方式\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "\n",
    "# 3. 为提前停止(Early Stopping)准备验证集\n",
    "# 从原始训练集中分出一部分作为验证集，用于在超参数搜索中监控模型性能\n",
    "X_train_fit, X_val, y_train_fit, y_val = train_test_split(\n",
    "    X_train_model, y_train_target, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"训练集大小: {X_train_fit.shape}, 验证集大小: {X_val.shape}\")\n",
    "\n",
    "# 4. 定义基础模型和超参数搜索空间\n",
    "# 设置基础参数\n",
    "base_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'hist',\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42,\n",
    "    'missing': np.nan,\n",
    "}\n",
    "\n",
    "# 创建基础模型\n",
    "# 注意：n_estimators 设置得比较大，因为我们将使用 early_stopping 来自动找到最佳值\n",
    "base_model = XGBRegressor(n_estimators=1000, **base_params)\n",
    "\n",
    "# 设置超参数搜索空间 (移除了 n_estimators，扩展了 max_depth)\n",
    "param_dist = {\n",
    "    'learning_rate': [0.01, 0.02, 0.03, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0.5, 1.0, 1.5, 2.0]\n",
    "}\n",
    "\n",
    "# 5. 执行带提前停止的超参数优化\n",
    "print(\"\\n执行带有提前停止的超参数优化...\")\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=150,  # 减少迭代次数，150次在多数情况下已足够\n",
    "    cv=kf,\n",
    "    scoring=tol_scorer_wrapped,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 使用 fit_params 将提前停止应用到 RandomizedSearchCV 的每一次训练中\n",
    "fit_params = {\n",
    "    'early_stopping_rounds': 50,  # 如果验证集分数在50轮内没有提升，则停止训练\n",
    "    'eval_set': [(X_val, y_val)],\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "search.fit(X_train_model, y_train_target, **fit_params)\n",
    "\n",
    "# 获取最佳估算器\n",
    "# search.best_estimator_ 已经是用最佳参数和最佳迭代次数训练好的模型\n",
    "xgb_model = search.best_estimator_\n",
    "\n",
    "print(f\"\\n最佳参数: {search.best_params_}\")\n",
    "print(f\"通过提前停止找到的最佳迭代次数: {xgb_model.n_estimators}\")\n",
    "print(f\"最佳CV得分 (使用自定义容忍度评分): {search.best_score_:.4f}\")\n",
    "\n",
    "# 6. 使用最佳模型进行最终评估和可视化\n",
    "# 交叉验证 R² 分数\n",
    "cv_scores_r2 = cross_val_score(xgb_model, X_train_model, y_train_target, cv=kf, scoring='r2')\n",
    "print(f\"交叉验证 R² 分数: {cv_scores_r2.mean():.4f} ± {cv_scores_r2.std():.4f}\")\n",
    "\n",
    "# 交叉验证容忍度分数\n",
    "cv_scores_tol = cross_val_score(xgb_model, X_train_model, y_train_target, cv=kf, scoring=tol_scorer_wrapped)\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {cv_scores_tol.mean():.4f} ± {cv_scores_tol.std():.4f}\")\n",
    "\n",
    "# 评估模型在训练集和测试集上的最终表现\n",
    "results = evaluate_model(xgb_model, X_train_model, y_train_target, X_test_model, y_test_target, target, \"XGBoost\")\n",
    "\n",
    "# 7. 保存模型和预测结果\n",
    "model_folder = '训练模型文件'\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "xgb_model_file = os.path.join(model_folder, f'{target}_XGBoost模型.pkl')\n",
    "with open(xgb_model_file, 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "print(f\"\\nXGBoost 模型已保存至 {xgb_model_file}\")\n",
    "\n",
    "# 获取并保存预测结果\n",
    "y_pred_train = xgb_model.predict(X_train_model)\n",
    "y_pred_test = xgb_model.predict(X_test_model)\n",
    "\n",
    "train_prediction = pd.DataFrame({'实际值': y_train_target, '预测值': y_pred_train, '误差': np.abs(y_train_target - y_pred_train), '数据集': '训练集'})\n",
    "test_prediction = pd.DataFrame({'实际值': y_test_target, '预测值': y_pred_test, '误差': np.abs(y_test_target - y_pred_test), '数据集': '测试集'})\n",
    "\n",
    "train_file = os.path.join(save_folder, f'{target}_XGBoost训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_XGBoost测试集预测结果.csv')\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "\n",
    "# 8. 特征重要性分析\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - XGBoost特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.gca().invert_yaxis() # 让最重要的特征显示在顶部\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_XGBoost特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "\n",
    "\n",
    "# 9. 绘制最终模型的训练过程曲线 (学习曲线)\n",
    "# 为了得到清晰的训练/测试曲线，我们用找到的最佳参数重新训练一次模型\n",
    "# 这次我们用完整的训练集进行训练，并用测试集作为评估集来观察过拟合情况\n",
    "print(\"\\n正在绘制最终模型的学习曲线...\")\n",
    "final_model_for_plot = XGBRegressor(**xgb_model.get_params())\n",
    "eval_set_plot = [(X_train_model, y_train_target), (X_test_model, y_test_target)]\n",
    "\n",
    "final_model_for_plot.fit(X_train_model, y_train_target,\n",
    "                         eval_set=eval_set_plot,\n",
    "                         eval_metric='rmse',\n",
    "                         verbose=False)\n",
    "\n",
    "results_plot = final_model_for_plot.evals_result()\n",
    "epochs = len(results_plot['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_axis, results_plot['validation_0']['rmse'], label='训练集RMSE')\n",
    "plt.plot(x_axis, results_plot['validation_1']['rmse'], label='测试集RMSE')\n",
    "plt.legend()\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('迭代次数')\n",
    "plt.title('XGBoost 最终模型训练进度')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "training_progress_data = pd.DataFrame({\n",
    "    '迭代次数': x_axis,\n",
    "    '训练集RMSE': results_plot['validation_0']['rmse'],\n",
    "    '测试集RMSE': results_plot['validation_1']['rmse']\n",
    "})\n",
    "training_progress_file = os.path.join(save_folder, f'{target}_XGBoost训练进度.csv')\n",
    "training_progress_data.to_csv(training_progress_file, index=False)\n",
    "print(f\"训练进度数据已保存至 {training_progress_file}\")\n",
    "\n",
    "\n",
    "# 10. 学习率影响分析 (此部分为独立分析，保持不变)\n",
    "print(\"\\n正在执行学习率影响分析...\")\n",
    "learning_rates = [0.01, 0.03, 0.05, 0.1, 0.2]\n",
    "plt.figure(figsize=(10, 6))\n",
    "lr_analysis_data = pd.DataFrame()\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model_lr = XGBRegressor(\n",
    "        learning_rate=lr,\n",
    "        n_estimators=500, # 固定迭代次数用于比较\n",
    "        max_depth=5,       # 使用一个合理的深度\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        **base_params\n",
    "    )\n",
    "    eval_set_lr = [(X_test_model, y_test_target)]\n",
    "    model_lr.fit(X_train_model, y_train_target, eval_set=eval_set_lr, eval_metric='rmse', verbose=False)\n",
    "    \n",
    "    results_lr = model_lr.evals_result()\n",
    "    test_rmse = results_lr['validation_0']['rmse']\n",
    "    \n",
    "    plt.plot(test_rmse, label=f'学习率: {lr}')\n",
    "    \n",
    "    # 准备数据以供保存\n",
    "    temp_df = pd.DataFrame({f'学习率_{lr}': test_rmse})\n",
    "    lr_analysis_data = pd.concat([lr_analysis_data, temp_df], axis=1)\n",
    "\n",
    "lr_analysis_data.insert(0, '迭代次数', range(len(lr_analysis_data)))\n",
    "\n",
    "plt.title('不同学习率对测试集RMSE的影响')\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('测试集 RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "lr_analysis_file = os.path.join(save_folder, f'{target}_XGBoost学习率分析.csv')\n",
    "lr_analysis_data.to_csv(lr_analysis_file, index=False)\n",
    "print(f\"学习率分析数据已保存至 {lr_analysis_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# lightGBM\n",
    "print(f\"训练 {target} 的LightGBM模型...\")\n",
    "target = '循环使用次数'\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.03)\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_tree[target]\n",
    "X_test_model = X_test_tree[target]\n",
    "# 设置基础参数\n",
    "base_params = {\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'rmse',\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "# 设置特定参数\n",
    "lgb_params = {\n",
    "    'n_estimators': 6000,\n",
    "    'learning_rate': 0.001,\n",
    "    'num_leaves': 20,\n",
    "    'max_depth': 8,#8shi0.72\n",
    "    'min_child_samples': 1,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree':1,\n",
    "    'reg_alpha': 10,\n",
    "    'reg_lambda': 1.0,\n",
    "    **base_params\n",
    "}\n",
    "\n",
    "print(\"使用自定义LightGBM包装器训练模型\")\n",
    "# 确保保存当前使用的特征列\n",
    "feature_cols = X_train_model.columns.tolist() if hasattr(X_train_model, 'columns') else None\n",
    "\n",
    "# 创建并训练模型\n",
    "lgb_model = CustomLGBMRegressor(**lgb_params)\n",
    "lgb_model.fit(X_train_model, y_train[target])\n",
    "\n",
    "# 保存模型\n",
    "models[target]['LightGBM'] = lgb_model\n",
    "# 创建模型保存文件夹（如果已存在则不会重复创建）\n",
    "model_folder = '训练模型文件'\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    print(f\"已创建模型文件夹：{model_folder}\")\n",
    "\n",
    "lgb_model_file = os.path.join(model_folder, f'{target}_LightGBM模型.pkl')\n",
    "with open(lgb_model_file, 'wb') as f:\n",
    "    pickle.dump(lgb_model, f)\n",
    "print(f\"LightGBM 模型已保存至 {lgb_model_file}\")\n",
    "# 评估模型\n",
    "results = evaluate_model(lgb_model, X_train_model, y_train[target], X_test_model, y_test[target], target, \"LightGBM\")\n",
    "\n",
    "print(\"LightGBM模型训练成功\")\n",
    "# 获取预测值\n",
    "y_pred_train = lgb_model.predict(X_train_model)\n",
    "y_pred_test = lgb_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到单独的文件\n",
    "train_file = os.path.join(save_folder, f'{target}_LightGBM训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_LightGBM测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "# 特征重要性可视化\n",
    "if hasattr(lgb_model.model, 'feature_importance') and feature_cols is not None:\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': lgb_model.model.feature_importance(importance_type='gain')\n",
    "    })\n",
    "    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "    plt.xlabel('增益重要性')\n",
    "    plt.ylabel('特征')\n",
    "    plt.title(f'{target} - LightGBM特征重要性')\n",
    "    plt.grid(True, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_LightGBM特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "\n",
    "\n",
    "# 添加预测不准确样本分析\n",
    "# 获取测试集预测值\n",
    "y_pred = lgb_model.predict(X_test_model)\n",
    "\n",
    "# 将y_test转换为numpy数组格式进行处理\n",
    "if hasattr(y_test[target], 'values'):\n",
    "    y_true_values = y_test[target].values\n",
    "else:\n",
    "    y_true_values = y_test[target]\n",
    "\n",
    "# 计算绝对误差\n",
    "errors = np.abs(y_true_values - y_pred)\n",
    "\n",
    "# 设置容忍度阈值\n",
    "tolerance = 5.0  # 可以根据需要调整\n",
    "\n",
    "# 找出误差超过容忍度的样本\n",
    "inaccurate_mask = errors > tolerance\n",
    "inaccurate_indices = np.where(inaccurate_mask)[0]\n",
    "\n",
    "print(f\"\\n预测不准确的样本数量: {len(inaccurate_indices)} (占测试集的 {len(inaccurate_indices)/len(y_test)*100:.2f}%)\")\n",
    "print(f\"使用的容忍度阈值: {tolerance}\")\n",
    "\n",
    "# 创建预测不准确样本的分析数据\n",
    "if len(inaccurate_indices) > 0:\n",
    "    # 尝试获取原始索引，如果不可用则使用数组位置索引\n",
    "    try:\n",
    "        if hasattr(y_test, 'index'):\n",
    "            original_indices = [y_test.index[i] for i in inaccurate_indices]\n",
    "        elif isinstance(X_test_model, pd.DataFrame) and hasattr(X_test_model, 'index'):\n",
    "            original_indices = [X_test_model.index[i] for i in inaccurate_indices]\n",
    "        else:\n",
    "            # 如果无法获取原始索引，使用数组位置作为标识\n",
    "            original_indices = inaccurate_indices\n",
    "    except Exception as e:\n",
    "        print(f\"无法获取原始索引: {str(e)}\")\n",
    "        original_indices = inaccurate_indices\n",
    "    \n",
    "    # 创建包含预测不准确样本信息的DataFrame\n",
    "    inaccurate_samples = []\n",
    "    for i, idx in enumerate(inaccurate_indices):\n",
    "        # 安全地获取实际值\n",
    "        if hasattr(y_test[target], 'iloc'):\n",
    "            actual = y_test[target].iloc[idx]\n",
    "        else:\n",
    "            actual = y_true_values[idx]\n",
    "        \n",
    "        inaccurate_samples.append({\n",
    "            '样本索引': original_indices[i],\n",
    "            '实际值': actual,\n",
    "            '预测值': y_pred[idx],\n",
    "            '绝对误差': errors[idx],\n",
    "            '相对误差(%)': (errors[idx] / np.abs(actual)) * 100 if actual != 0 else float('inf')\n",
    "        })\n",
    "    \n",
    "    inaccurate_df = pd.DataFrame(inaccurate_samples)\n",
    "    # 按误差降序排列\n",
    "    inaccurate_df = inaccurate_df.sort_values('绝对误差', ascending=False)\n",
    "    \n",
    "    # 打印预测不准确的样本信息\n",
    "    print(\"\\n预测不准确的样本详情 (按误差降序排列):\")\n",
    "    print(inaccurate_df)\n",
    "    \n",
    "    # 保存结果到文件\n",
    "    inaccurate_df.to_csv(f'{target}_不准确预测.csv', index=False)\n",
    "else:\n",
    "    print(f\"\\n没有发现预测不准确的样本 (容忍度阈值: {tolerance})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#HistGradientBoosting\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "print(f\"训练 {target} 的HistGradientBoosting模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.03)\n",
    "\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_tree[target]\n",
    "X_test_model = X_test_tree[target]\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# 设置超参数min_samples_leaf': 4, 'max_iter': 400, 'max_depth': 11, 'learning_rate': 0.011, 'l2_regularization': 0}\n",
    "#{'min_samples_leaf': 4, 'max_iter': 460, 'max_depth': 7, 'learning_rate': 0.015, 'l2_regularization': 0}\n",
    "param_dist = {\n",
    "    'max_iter': [430, 400,380,440,460,600,500,800,900],\n",
    "    'learning_rate': [0.01, 0.008, 0.011,0.012,0.015,0.009,0.1,0.02],\n",
    "    'max_depth': [9, 11, 10,8,7,6,5,12,13],\n",
    "    'min_samples_leaf': [1, 2, 4,5,6,3],\n",
    "    'l2_regularization': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# 创建基础模型\n",
    "base_model = HistGradientBoostingRegressor(\n",
    "    max_iter=4000,\n",
    "    learning_rate=1,\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=4,\n",
    "    l2_regularization=0.5,\n",
    "    loss='squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 执行超参数优化\n",
    "if len(X_train_model) >= 90:\n",
    "    print(\"执行超参数优化...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=8000,\n",
    "        cv=kf,\n",
    "        scoring=tol_scorer_wrapped,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train_model, y_train[target])\n",
    "    hgb_model = search.best_estimator_\n",
    "    print(f\"最佳参数: {search.best_params_}\")\n",
    "    print(f\"最佳CV得分: {search.best_score_:.4f}\")\n",
    "else:\n",
    "    # 使用预定义的参数\n",
    "    print(\"使用预定义参数...\")\n",
    "    hgb_model = base_model\n",
    "    hgb_model.fit(X_train_model, y_train[target])\n",
    "\n",
    "# 交叉验证\n",
    "cv_scores = cross_val_score(hgb_model, X_train_model, y_train[target], cv=kf, scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    hgb_model, X_train_model, y_train[target], cv=kf, \n",
    "    scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "models[target]['HistGradientBoosting'] = hgb_model\n",
    "# 使用pickle保存HistGradientBoosting模型\n",
    "hgb_model_file = os.path.join(model_folder, f'{target}_HistGradientBoosting模型.pkl')\n",
    "with open(hgb_model_file, 'wb') as f:\n",
    "    pickle.dump(hgb_model, f)\n",
    "print(f\"HistGradientBoosting模型已保存至 {hgb_model_file}\")\n",
    "# 评估模型\n",
    "results = evaluate_model(hgb_model, X_train_model, y_train[target], X_test_model, y_test[target], target, \"HistGradientBoosting\")\n",
    "# 获取预测值\n",
    "y_pred_train = hgb_model.predict(X_train_model)\n",
    "y_pred_test = hgb_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到文件\n",
    "train_file = os.path.join(save_folder, f'{target}_HistGradientBoosting训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_HistGradientBoosting测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "# 由于HistGradientBoosting不直接提供特征重要性，使用permutation importance评估\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(\n",
    "    hgb_model, X_test_model, y_test[target], \n",
    "    n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('置换重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - HistGradientBoosting特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_HistGradientBoosting特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "# 不同学习率和迭代次数的影响分析\n",
    "learning_rates = [0.01, 0.008, 0.011,0.009]\n",
    "max_iters = [50, 100, 200, 300]\n",
    "fig, axs = plt.subplots(len(learning_rates), 1, figsize=(10, 4*len(learning_rates)), sharex=True)\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    for iter_count in max_iters:\n",
    "        model = HistGradientBoostingRegressor(\n",
    "            max_iter=iter_count,\n",
    "            learning_rate=lr,\n",
    "            max_depth=3,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train_model, y_train[target])\n",
    "        train_score = r2_score(y_train[target], model.predict(X_train_model))\n",
    "        test_score = r2_score(y_test[target], model.predict(X_test_model))\n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "    \n",
    "    axs[i].plot(max_iters, train_scores, 'o-', label='训练集 R²')\n",
    "    axs[i].plot(max_iters, test_scores, 'o-', label='测试集 R²')\n",
    "    axs[i].set_title(f'学习率 = {lr}')\n",
    "    axs[i].set_ylabel('R²')\n",
    "    axs[i].grid(True)\n",
    "    axs[i].legend()\n",
    "plt.xlabel('迭代次数')\n",
    "plt.suptitle('HistGradientBoosting - 学习率和迭代次数影响')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 准备保存学习率和迭代次数影响分析数据\n",
    "analysis_data = []\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    for j, iter_count in enumerate(max_iters):\n",
    "        analysis_data.append({\n",
    "            '学习率': lr,\n",
    "            '迭代次数': iter_count,\n",
    "            '训练集R²': train_scores[j],\n",
    "            '测试集R²': test_scores[j]\n",
    "        })\n",
    "\n",
    "# 转换为DataFrame并保存\n",
    "lr_iter_analysis = pd.DataFrame(analysis_data)\n",
    "lr_analysis_file = os.path.join(save_folder, f'{target}_HistGradientBoosting学习率迭代分析.csv')\n",
    "lr_iter_analysis.to_csv(lr_analysis_file, index=False)\n",
    "print(f\"学习率和迭代次数分析数据已保存至 {lr_analysis_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# 选择目标变量\n",
    "print(f\"训练 {target} 的随机森林模型...\")\n",
    "target = '循环使用次数'\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.03)\n",
    "\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_tree_filled[target]\n",
    "X_test_model = X_test_tree_filled[target]\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# 设置超参数\n",
    "param_dist = {\n",
    "    'n_estimators': [ 900,1000,1200,1100],\n",
    "    'max_depth': [4, 5, 6,8,9,10],\n",
    "    'min_samples_split': [ 5,4,6,7,8],\n",
    "    'min_samples_leaf': [7, 4,3,2,1,5,6],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# 创建基础模型\n",
    "base_model = RandomForestRegressor(\n",
    "    n_estimators=800,\n",
    "    max_depth=None,\n",
    "    min_samples_split=3,\n",
    "    min_samples_leaf=4,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 执行超参数优化\n",
    "if len(X_train_model) >= 90:\n",
    "    print(\"执行超参数优化...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=1000,#变大后训练集R方变大但是时间可能很长2000/0.3569；4000/0.2091；3000/0.1221；1000/0.1577\n",
    "        cv=kf,\n",
    "        scoring=tol_scorer_wrapped,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train_model, y_train[target])\n",
    "    rf_model = search.best_estimator_\n",
    "    print(f\"最佳参数: {search.best_params_}\")\n",
    "    print(f\"最佳CV得分: {search.best_score_:.4f}\")\n",
    "else:\n",
    "    # 使用预定义的参数\n",
    "    print(\"使用预定义参数...\")\n",
    "    rf_model = base_model\n",
    "    rf_model.fit(X_train_model, y_train[target])\n",
    "\n",
    "# 交叉验证\n",
    "cv_scores = cross_val_score(rf_model, X_train_model, y_train[target], cv=kf, scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    rf_model, X_train_model, y_train[target], cv=kf, \n",
    "    scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "\n",
    "# 如果模型有oob_score属性，输出oob分数\n",
    "if hasattr(rf_model, 'oob_score_'):\n",
    "    print(f\"袋外评分 (OOB score): {rf_model.oob_score_:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "models[target]['RandomForest'] = rf_model\n",
    "# 使用pickle保存随机森林模型\n",
    "rf_model_file = os.path.join(model_folder, f'{target}_随机森林模型.pkl')\n",
    "with open(rf_model_file, 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "print(f\"随机森林模型已保存至 {rf_model_file}\")\n",
    "# 评估模型\n",
    "results = evaluate_model(rf_model, X_train_model, y_train[target], X_test_model, y_test[target], target, \"RandomForest\")\n",
    "# 获取预测值\n",
    "y_pred_train = rf_model.predict(X_train_model)\n",
    "y_pred_test = rf_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到文件\n",
    "train_file = os.path.join(save_folder, f'{target}_随机森林训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_随机森林测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "# 特征重要性\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - RandomForest特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_随机森林特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "# 不同参数组合的影响\n",
    "n_estimators_range = [10, 50, 100, 200, 300, 400,800,1000]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "oob_scores = []\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=n_est,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=4,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf.fit(X_train_model, y_train[target])\n",
    "    train_scores.append(r2_score(y_train[target], rf.predict(X_train_model)))\n",
    "    test_scores.append(r2_score(y_test[target], rf.predict(X_test_model)))\n",
    "    oob_scores.append(rf.oob_score_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators_range, train_scores, 'o-', label='训练集 R²')\n",
    "plt.plot(n_estimators_range, test_scores, 'o-', label='测试集 R²')\n",
    "plt.plot(n_estimators_range, oob_scores, 'o-', label='OOB R²')\n",
    "plt.xlabel('树的数量')\n",
    "plt.ylabel('R²')\n",
    "plt.title('RandomForest - 树数量对性能的影响')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# 保存树数量影响分析数据\n",
    "trees_analysis_data = pd.DataFrame({\n",
    "    '树的数量': n_estimators_range,\n",
    "    '训练集R²': train_scores,\n",
    "    '测试集R²': test_scores,\n",
    "    '袋外评分': oob_scores\n",
    "})\n",
    "trees_analysis_file = os.path.join(save_folder, f'{target}_随机森林树数量分析.csv')\n",
    "trees_analysis_data.to_csv(trees_analysis_file, index=False)\n",
    "print(f\"树数量影响分析数据已保存至 {trees_analysis_file}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度神经网络回归模型\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "target=\"水接触角\"\n",
    "# 选择目标变量\n",
    "print(f\"训练 {target} 的深度神经网络模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_nn[target]\n",
    "X_test_model = X_test_nn[target]\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# 标准化特征数据\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_model)\n",
    "X_test_scaled = scaler.transform(X_test_model)\n",
    "\n",
    "print(f\"训练数据形状: {X_train_scaled.shape}\")\n",
    "print(f\"测试数据形状: {X_test_scaled.shape}\")\n",
    "\n",
    "# 创建神经网络模型的函数\n",
    "def create_nn_model(hidden_layers=[128, 64, 32], dropout_rate=0.3, learning_rate=0.001):\n",
    "    \"\"\"创建深度神经网络模型\"\"\"\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # 输入层\n",
    "    model.add(layers.Dense(hidden_layers[0], \n",
    "                          activation='relu', \n",
    "                          input_shape=(X_train_scaled.shape[1],)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # 隐藏层\n",
    "    for units in hidden_layers[1:]:\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # 输出层\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    # 编译模型\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                 loss='mse', \n",
    "                 metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 包装器类以兼容sklearn接口\n",
    "class KerasRegressorWrapper:\n",
    "    def __init__(self, hidden_layers=[128, 64, 32], dropout_rate=0.3, \n",
    "                 learning_rate=0.001, epochs=100, batch_size=32):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.model = create_nn_model(self.hidden_layers, \n",
    "                                   self.dropout_rate, \n",
    "                                   self.learning_rate)\n",
    "        \n",
    "        # 早停和学习率调度\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        # 训练模型\n",
    "        self.history = self.model.fit(\n",
    "            X, y,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X, verbose=0).flatten()\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'hidden_layers': self.hidden_layers,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "# 设置超参数搜索空间\n",
    "param_dist = {\n",
    "    'hidden_layers': [\n",
    "        [64, 32],\n",
    "        [128, 64],\n",
    "        [128, 64, 32],\n",
    "        [256, 128, 64],\n",
    "        [128, 64, 32, 16],\n",
    "        [256, 128, 64, 32]\n",
    "    ],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4, 0.5],\n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.0005],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [150, 200, 250]\n",
    "}\n",
    "\n",
    "# 创建基础模型\n",
    "base_model = KerasRegressorWrapper(\n",
    "    hidden_layers=[128, 64, 32],\n",
    "    dropout_rate=0.3,\n",
    "    learning_rate=0.001,\n",
    "    epochs=200,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# 执行超参数优化\n",
    "if len(X_train_model) >= 80:\n",
    "    print(\"执行超参数优化...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,  # 由于神经网络训练时间长，减少迭代次数\n",
    "        cv=min(3, cv_folds),\n",
    "        scoring='r2',\n",
    "        n_jobs=1,  # 神经网络使用单进程避免冲突\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train_scaled, y_train[target])\n",
    "    nn_model = search.best_estimator_\n",
    "    print(f\"最佳参数: {search.best_params_}\")\n",
    "    print(f\"最佳CV得分: {search.best_score_:.4f}\")\n",
    "else:\n",
    "    # 使用预定义的参数\n",
    "    print(\"使用预定义参数...\")\n",
    "    nn_model = base_model\n",
    "    nn_model.fit(X_train_scaled, y_train[target])\n",
    "\n",
    "# 交叉验证\n",
    "print(\"执行交叉验证...\")\n",
    "cv_scores = cross_val_score(nn_model, X_train_scaled, y_train[target], \n",
    "                           cv=min(3, cv_folds), scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    nn_model, X_train_scaled, y_train[target], \n",
    "    cv=min(3, cv_folds), scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "\n",
    "# 重新训练最终模型\n",
    "print(\"重新训练最终模型...\")\n",
    "nn_model.fit(X_train_scaled, y_train[target])\n",
    "\n",
    "# 保存模型\n",
    "models[target]['DeepNN'] = {'model': nn_model, 'scaler': scaler}\n",
    "# 保存神经网络模型\n",
    "nn_model_file = os.path.join(model_folder, f'{target}_神经网络模型.pkl')\n",
    "with open(nn_model_file, 'wb') as f:\n",
    "    pickle.dump({'model': nn_model, 'scaler': scaler}, f)\n",
    "print(f\"神经网络模型已保存至 {nn_model_file}\")\n",
    "\n",
    "# 评估模型\n",
    "class NNEvaluationWrapper:\n",
    "    def __init__(self, model, scaler):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "\n",
    "nn_eval_model = NNEvaluationWrapper(nn_model, scaler)\n",
    "results = evaluate_model(nn_eval_model, X_train_model, y_train[target], \n",
    "                        X_test_model, y_test[target], target, \"DeepNN\")\n",
    "\n",
    "# 获取预测值\n",
    "y_pred_train = nn_eval_model.predict(X_train_model)\n",
    "y_pred_test = nn_eval_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到文件\n",
    "train_file = os.path.join(save_folder, f'{target}_神经网络训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_神经网络测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# 训练损失可视化\n",
    "if hasattr(nn_model, 'history') and nn_model.history is not None:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(nn_model.history.history['loss'], label='训练损失')\n",
    "    plt.plot(nn_model.history.history['val_loss'], label='验证损失')\n",
    "    plt.title('模型损失')\n",
    "    plt.xlabel('轮次')\n",
    "    plt.ylabel('损失')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(nn_model.history.history['mae'], label='训练MAE')\n",
    "    plt.plot(nn_model.history.history['val_mae'], label='验证MAE')\n",
    "    plt.title('模型MAE')\n",
    "    plt.xlabel('轮次')\n",
    "    plt.ylabel('平均绝对误差')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 不同网络结构的影响分析\n",
    "print(\"分析不同网络结构的影响...\")\n",
    "network_structures = [\n",
    "    [32],\n",
    "    [64, 32],\n",
    "    [128, 64],\n",
    "    [128, 64, 32],\n",
    "    [256, 128, 64],\n",
    "    [128, 64, 32, 16]\n",
    "]\n",
    "\n",
    "structure_train_scores = []\n",
    "structure_test_scores = []\n",
    "structure_names = []\n",
    "\n",
    "for structure in network_structures:\n",
    "    structure_name = '-'.join(map(str, structure))\n",
    "    structure_names.append(structure_name)\n",
    "    print(f\"测试网络结构: {structure_name}\")\n",
    "    \n",
    "    test_nn = KerasRegressorWrapper(\n",
    "        hidden_layers=structure,\n",
    "        dropout_rate=0.3,\n",
    "        learning_rate=0.001,\n",
    "        epochs=100,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    test_nn.fit(X_train_scaled, y_train[target])\n",
    "    \n",
    "    train_pred = test_nn.predict(X_train_scaled)\n",
    "    test_pred = test_nn.predict(X_test_scaled)\n",
    "    \n",
    "    train_r2 = r2_score(y_train[target], train_pred)\n",
    "    test_r2 = r2_score(y_test[target], test_pred)\n",
    "    \n",
    "    structure_train_scores.append(train_r2)\n",
    "    structure_test_scores.append(test_r2)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(structure_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, structure_train_scores, width, label='训练集 R²')\n",
    "plt.bar(x + width/2, structure_test_scores, width, label='测试集 R²')\n",
    "\n",
    "plt.xlabel('网络结构')\n",
    "plt.ylabel('R²')\n",
    "plt.title('DeepNN - 网络结构对性能的影响')\n",
    "plt.xticks(x, structure_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 保存网络结构影响分析数据\n",
    "structure_analysis_data = pd.DataFrame({\n",
    "    '网络结构': structure_names,\n",
    "    '训练集R²': structure_train_scores,\n",
    "    '测试集R²': structure_test_scores\n",
    "})\n",
    "structure_analysis_file = os.path.join(save_folder, f'{target}_神经网络结构分析.csv')\n",
    "structure_analysis_data.to_csv(structure_analysis_file, index=False)\n",
    "print(f\"网络结构影响分析数据已保存至 {structure_analysis_file}\")\n",
    "\n",
    "print(\"神经网络模型训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 支持向量机回归模型\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "\n",
    "# 选择目标变量\n",
    "print(f\"训练 {target} 的支持向量机回归模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 选择适当的数据集 - SVR对特征尺度敏感，使用线性预处理数据\n",
    "X_train_model = X_train_linear[target]\n",
    "X_test_model = X_test_linear[target]\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# SVR需要标准化处理\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_model)\n",
    "X_test_scaled = scaler.transform(X_test_model)\n",
    "\n",
    "print(f\"训练数据形状: {X_train_scaled.shape}\")\n",
    "print(f\"测试数据形状: {X_test_scaled.shape}\")\n",
    "\n",
    "# 设置超参数搜索空间\n",
    "param_dist = {\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'epsilon': [0.01, 0.1, 0.2, 0.5, 1.0],\n",
    "    'degree': [2, 3, 4]  # 仅对poly核有效\n",
    "}\n",
    "\n",
    "# 创建基础模型\n",
    "base_model = SVR(\n",
    "    kernel='rbf',\n",
    "    C=100,\n",
    "    gamma='scale',\n",
    "    epsilon=0.1,\n",
    "    max_iter=5000\n",
    ")\n",
    "\n",
    "# 执行超参数优化\n",
    "if len(X_train_model) >= 80:\n",
    "    print(\"执行超参数优化...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,\n",
    "        cv=min(5, cv_folds),\n",
    "        scoring=tol_scorer_wrapped,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train_scaled, y_train[target])\n",
    "    svr_model = search.best_estimator_\n",
    "    print(f\"最佳参数: {search.best_params_}\")\n",
    "    print(f\"最佳CV得分: {search.best_score_:.4f}\")\n",
    "    \n",
    "    # 如果找到的最佳核函数是poly，进行更精细的优化\n",
    "    if svr_model.kernel == 'poly':\n",
    "        print(\"对多项式核进行精细优化...\")\n",
    "        poly_param_grid = {\n",
    "            'C': [svr_model.C * 0.5, svr_model.C, svr_model.C * 2],\n",
    "            'gamma': [svr_model.gamma] if isinstance(svr_model.gamma, str) else [svr_model.gamma * 0.5, svr_model.gamma, svr_model.gamma * 2],\n",
    "            'degree': [max(2, svr_model.degree-1), svr_model.degree, svr_model.degree+1],\n",
    "            'epsilon': [svr_model.epsilon * 0.5, svr_model.epsilon, svr_model.epsilon * 2]\n",
    "        }\n",
    "        \n",
    "        fine_search = GridSearchCV(\n",
    "            SVR(kernel='poly'),\n",
    "            poly_param_grid,\n",
    "            cv=min(3, cv_folds),\n",
    "            scoring=tol_scorer_wrapped,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        fine_search.fit(X_train_scaled, y_train[target])\n",
    "        svr_model = fine_search.best_estimator_\n",
    "        print(f\"精细优化后的最佳参数: {fine_search.best_params_}\")\n",
    "        \n",
    "else:\n",
    "    # 使用预定义的参数\n",
    "    print(\"使用预定义参数...\")\n",
    "    svr_model = base_model\n",
    "    svr_model.fit(X_train_scaled, y_train[target])\n",
    "\n",
    "# 交叉验证\n",
    "cv_scores = cross_val_score(svr_model, X_train_scaled, y_train[target], \n",
    "                           cv=min(5, cv_folds), scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    svr_model, X_train_scaled, y_train[target], \n",
    "    cv=min(5, cv_folds), scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "\n",
    "# 输出最终模型信息\n",
    "print(f\"最终模型参数:\")\n",
    "print(f\"  核函数: {svr_model.kernel}\")\n",
    "print(f\"  C参数: {svr_model.C}\")\n",
    "print(f\"  gamma参数: {svr_model.gamma}\")\n",
    "print(f\"  epsilon参数: {svr_model.epsilon}\")\n",
    "if svr_model.kernel == 'poly':\n",
    "    print(f\"  多项式度数: {svr_model.degree}\")\n",
    "print(f\"  支持向量数量: {svr_model.n_support_}\")\n",
    "\n",
    "# 保存模型\n",
    "models[target]['SVR'] = {'model': svr_model, 'scaler': scaler}\n",
    "# 使用pickle保存SVR模型\n",
    "svr_model_file = os.path.join(model_folder, f'{target}_支持向量机模型.pkl')\n",
    "with open(svr_model_file, 'wb') as f:\n",
    "    pickle.dump({'model': svr_model, 'scaler': scaler}, f)\n",
    "print(f\"支持向量机模型已保存至 {svr_model_file}\")\n",
    "\n",
    "# 评估模型 - 创建包装器以处理标准化\n",
    "class SVREvaluationWrapper:\n",
    "    def __init__(self, model, scaler):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "\n",
    "svr_eval_model = SVREvaluationWrapper(svr_model, scaler)\n",
    "results = evaluate_model(svr_eval_model, X_train_model, y_train[target], \n",
    "                        X_test_model, y_test[target], target, \"SVR\")\n",
    "\n",
    "# 获取预测值\n",
    "y_pred_train = svr_eval_model.predict(X_train_model)\n",
    "y_pred_test = svr_eval_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到文件\n",
    "train_file = os.path.join(save_folder, f'{target}_支持向量机训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_支持向量机测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "# 特征重要性（使用置换重要性）\n",
    "print(\"计算特征重要性...\")\n",
    "perm_importance = permutation_importance(\n",
    "    svr_eval_model, X_test_model, y_test[target], \n",
    "    n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('置换重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - SVR特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_支持向量机特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "\n",
    "# 不同C参数的影响分析\n",
    "print(\"分析不同C参数的影响...\")\n",
    "C_range = [0.1, 1, 10, 100, 1000, 10000]\n",
    "c_train_scores = []\n",
    "c_test_scores = []\n",
    "c_support_vectors = []\n",
    "\n",
    "for C_val in C_range:\n",
    "    print(f\"测试C参数: {C_val}\")\n",
    "    test_svr = SVR(\n",
    "        kernel=svr_model.kernel,\n",
    "        C=C_val,\n",
    "        gamma=svr_model.gamma,\n",
    "        epsilon=svr_model.epsilon,\n",
    "        degree=svr_model.degree if svr_model.kernel == 'poly' else 3,\n",
    "        max_iter=5000\n",
    "    )\n",
    "    \n",
    "    test_svr.fit(X_train_scaled, y_train[target])\n",
    "    \n",
    "    train_pred = test_svr.predict(X_train_scaled)\n",
    "    test_pred = test_svr.predict(X_test_scaled)\n",
    "    \n",
    "    train_r2 = r2_score(y_train[target], train_pred)\n",
    "    test_r2 = r2_score(y_test[target], test_pred)\n",
    "    \n",
    "    c_train_scores.append(train_r2)\n",
    "    c_test_scores.append(test_r2)\n",
    "    c_support_vectors.append(test_svr.n_support_.sum())\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.semilogx(C_range, c_train_scores, 'o-', label='训练集 R²')\n",
    "plt.semilogx(C_range, c_test_scores, 'o-', label='测试集 R²')\n",
    "plt.xlabel('C参数')\n",
    "plt.ylabel('R²')\n",
    "plt.title('SVR - C参数对性能的影响')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.semilogx(C_range, c_support_vectors, 'o-', color='green')\n",
    "plt.xlabel('C参数')\n",
    "plt.ylabel('支持向量数量')\n",
    "plt.title('SVR - C参数对支持向量数量的影响')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 不同核函数的性能比较\n",
    "print(\"比较不同核函数的性能...\")\n",
    "kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "kernel_scores = []\n",
    "kernel_train_scores = []\n",
    "kernel_names = []\n",
    "\n",
    "for kernel in kernels:\n",
    "    print(f\"测试核函数: {kernel}\")\n",
    "    try:\n",
    "        if kernel == 'poly':\n",
    "            test_svr = SVR(kernel=kernel, C=100, gamma='scale', epsilon=0.1, degree=3, max_iter=5000)\n",
    "        else:\n",
    "            test_svr = SVR(kernel=kernel, C=100, gamma='scale', epsilon=0.1, max_iter=5000)\n",
    "        \n",
    "        test_svr.fit(X_train_scaled, y_train[target])\n",
    "        \n",
    "        train_pred = test_svr.predict(X_train_scaled)\n",
    "        test_pred = test_svr.predict(X_test_scaled)\n",
    "        \n",
    "        train_r2 = r2_score(y_train[target], train_pred)\n",
    "        test_r2 = r2_score(y_test[target], test_pred)\n",
    "        \n",
    "        kernel_train_scores.append(train_r2)\n",
    "        kernel_scores.append(test_r2)\n",
    "        kernel_names.append(kernel)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"核函数 {kernel} 训练失败: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(kernel_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, kernel_train_scores, width, label='训练集 R²')\n",
    "plt.bar(x + width/2, kernel_scores, width, label='测试集 R²')\n",
    "\n",
    "plt.xlabel('核函数')\n",
    "plt.ylabel('R²')\n",
    "plt.title('SVR - 不同核函数性能比较')\n",
    "plt.xticks(x, kernel_names)\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 保存C参数影响分析数据\n",
    "c_analysis_data = pd.DataFrame({\n",
    "    'C参数': C_range,\n",
    "    '训练集R²': c_train_scores,\n",
    "    '测试集R²': c_test_scores,\n",
    "    '支持向量数量': c_support_vectors\n",
    "})\n",
    "c_analysis_file = os.path.join(save_folder, f'{target}_支持向量机C参数分析.csv')\n",
    "c_analysis_data.to_csv(c_analysis_file, index=False)\n",
    "print(f\"C参数影响分析数据已保存至 {c_analysis_file}\")\n",
    "\n",
    "# 保存核函数比较数据\n",
    "kernel_analysis_data = pd.DataFrame({\n",
    "    '核函数': kernel_names,\n",
    "    '训练集R²': kernel_train_scores,\n",
    "    '测试集R²': kernel_scores\n",
    "})\n",
    "kernel_analysis_file = os.path.join(save_folder, f'{target}_支持向量机核函数分析.csv')\n",
    "kernel_analysis_data.to_csv(kernel_analysis_file, index=False)\n",
    "print(f\"核函数比较分析数据已保存至 {kernel_analysis_file}\")\n",
    "\n",
    "print(\"支持向量机模型训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel, RationalQuadratic\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "import traceback\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# 选择目标变量\n",
    "print(f\"训练 {target} 的高斯过程回归模型...\")\n",
    "\n",
    "\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# 针对原始尺度数据调整超参数搜索空间\n",
    "param_grid = {\n",
    "    'alpha': [1e-6, 1e-4, 1e-2, 1e-1, 1.0],  # 适应更大特征值的正则化范围\n",
    "    'normalize_y': [True],\n",
    "    'n_restarts_optimizer': [2, 5]\n",
    "}\n",
    "\n",
    "# 第一步：不同核函数的比较\n",
    "print(\"步骤1: 比较不同核函数的性能...\")\n",
    "\n",
    "# 修改核函数定义，设置长度尺度的合理范围\n",
    "kernels = [\n",
    "    1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)),\n",
    "    1.0 * Matern(length_scale=1.0, nu=1.5, length_scale_bounds=(1e-2, 1e3)),\n",
    "    1.0 * RationalQuadratic(length_scale=1.0, alpha=0.5, length_scale_bounds=(1e-2, 1e3)),\n",
    "    1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(noise_level=0.1),\n",
    "    1.0 * RationalQuadratic(length_scale=1.0, alpha=0.5, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(noise_level=0.1)\n",
    "]\n",
    "kernel_names = ['RBF', 'Matern', 'RationalQuadratic', 'RBF + WhiteNoise', 'RationalQuadratic + WhiteNoise']\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "fit_times = []\n",
    "\n",
    "for kernel, name in zip(kernels, kernel_names):\n",
    "    print(f\"训练核函数: {name}\")\n",
    "    start_time = time.time()\n",
    "    gp = GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        alpha=1e-10,  # 先使用较小值进行比较\n",
    "        normalize_y=True,\n",
    "        n_restarts_optimizer=2,\n",
    "        random_state=42\n",
    "    )\n",
    "    gp.fit(X_train_model, y_train[target])\n",
    "    fit_time = time.time() - start_time\n",
    "    \n",
    "    train_score = r2_score(y_train[target], gp.predict(X_train_model))\n",
    "    test_score = r2_score(y_test[target], gp.predict(X_test_model))\n",
    "    \n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    fit_times.append(fit_time)\n",
    "    \n",
    "    print(f\"  训练时间: {fit_time:.2f}秒, 训练集R²: {train_score:.4f}, 测试集R²: {test_score:.4f}\")\n",
    "\n",
    "# 可视化比较结果\n",
    "plt.figure(figsize=(12, 6))\n",
    "width = 0.35\n",
    "x = np.arange(len(kernel_names))\n",
    "plt.bar(x - width/2, train_scores, width, label='训练集 R²')\n",
    "plt.bar(x + width/2, test_scores, width, label='测试集 R²')\n",
    "plt.xticks(x, kernel_names, rotation=45, ha='right')\n",
    "plt.xlabel('核函数')\n",
    "plt.ylabel('R²')\n",
    "plt.title('不同核函数的性能比较')\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 第二步：选择最佳核函数\n",
    "best_kernel_index = np.argmax(test_scores)\n",
    "best_kernel = kernels[best_kernel_index]\n",
    "best_kernel_name = kernel_names[best_kernel_index]\n",
    "print(f\"\\n步骤2: 选择最佳核函数\")\n",
    "print(f\"最佳核函数: {best_kernel_name}, 测试集R²: {test_scores[best_kernel_index]:.4f}\")\n",
    "\n",
    "# 第三步：使用最佳核函数并进行超参数优化\n",
    "print(f\"\\n步骤3: 使用最佳核函数 {best_kernel_name} 进行超参数优化...\")\n",
    "\n",
    "# 使用最佳核函数创建基础模型\n",
    "gpr_model = GaussianProcessRegressor(\n",
    "    kernel=best_kernel,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 使用GridSearchCV进行超参数优化\n",
    "grid_search = GridSearchCV(\n",
    "    gpr_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=min(3, cv_folds),\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"开始网格搜索...\")\n",
    "grid_search.fit(X_train_model, y_train[target])\n",
    "print(\"网格搜索完成\")\n",
    "\n",
    "# 获取最佳模型和参数\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"最佳参数: {grid_search.best_params_}\")\n",
    "print(f\"最佳交叉验证分数: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 第四步：评估最终模型\n",
    "print(f\"\\n步骤4: 评估最终优化模型...\")\n",
    "\n",
    "# 使用最佳模型进行交叉验证评估\n",
    "cv_scores = cross_val_score(best_model, X_train_model, y_train[target], cv=min(3, cv_folds), scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    best_model, X_train_model, y_train[target], cv=min(3, cv_folds),\n",
    "    scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "\n",
    "# 重新拟合最佳模型，以便后续使用\n",
    "best_model.fit(X_train_model, y_train[target])\n",
    "\n",
    "# 保存模型\n",
    "models[target]['GaussianProcess'] = best_model\n",
    "# 使用pickle保存高斯过程回归模型\n",
    "gp_model_file = os.path.join(model_folder, f'{target}_高斯过程模型.pkl')\n",
    "with open(gp_model_file, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"高斯过程回归模型已保存至 {gp_model_file}\")\n",
    "\n",
    "# 评估模型\n",
    "results = evaluate_model(best_model, X_train_model, y_train[target], X_test_model, y_test[target], target, \"GaussianProcess\")\n",
    "\n",
    "# 获取预测值\n",
    "y_pred_train = best_model.predict(X_train_model)\n",
    "y_pred_test = best_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到文件\n",
    "train_file = os.path.join(save_folder, f'{target}_高斯过程训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_高斯过程测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "# 打印核函数参数\n",
    "print(\"最终核函数参数:\")\n",
    "print(best_model.kernel_)\n",
    "\n",
    "# 计算特征重要性\n",
    "perm_importance = permutation_importance(\n",
    "    best_model, X_test_model, y_test[target], \n",
    "    n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('置换重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - GaussianProcess特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_高斯过程特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "\n",
    "# 第六步: 添加预测不准确样本分析\n",
    "print(f\"\\n步骤6: 分析预测不准确的样本...\")\n",
    "\n",
    "# 已经获取了测试集预测值，但需要转换为标准格式\n",
    "if not isinstance(y_pred_test, np.ndarray):\n",
    "    y_pred = np.array(y_pred_test)\n",
    "else:\n",
    "    y_pred = y_pred_test\n",
    "\n",
    "# 将y_test转换为numpy数组格式进行处理\n",
    "if hasattr(y_test[target], 'values'):\n",
    "    y_true_values = y_test[target].values\n",
    "else:\n",
    "    y_true_values = y_test[target]\n",
    "\n",
    "# 计算绝对误差\n",
    "errors = np.abs(y_true_values - y_pred)\n",
    "\n",
    "# 设置容忍度阈值\n",
    "tolerance = 5.0  # 可以根据需要调整\n",
    "\n",
    "# 找出误差超过容忍度的样本\n",
    "inaccurate_mask = errors > tolerance\n",
    "inaccurate_indices = np.where(inaccurate_mask)[0]\n",
    "\n",
    "print(f\"\\n预测不准确的样本数量: {len(inaccurate_indices)} (占测试集的 {len(inaccurate_indices)/len(y_test[target])*100:.2f}%)\")\n",
    "print(f\"使用的容忍度阈值: {tolerance}\")\n",
    "\n",
    "# 创建预测不准确样本的分析数据\n",
    "if len(inaccurate_indices) > 0:\n",
    "    # 尝试获取原始索引，如果不可用则使用数组位置索引\n",
    "    try:\n",
    "        if hasattr(y_test[target], 'index'):\n",
    "            original_indices = [y_test[target].index[i] for i in inaccurate_indices]\n",
    "        elif isinstance(X_test_model, pd.DataFrame) and hasattr(X_test_model, 'index'):\n",
    "            original_indices = [X_test_model.index[i] for i in inaccurate_indices]\n",
    "        else:\n",
    "            # 如果无法获取原始索引，使用数组位置作为标识\n",
    "            original_indices = inaccurate_indices\n",
    "    except Exception as e:\n",
    "        print(f\"无法获取原始索引: {str(e)}\")\n",
    "        original_indices = inaccurate_indices\n",
    "    \n",
    "    # 创建包含预测不准确样本信息的DataFrame\n",
    "    inaccurate_samples = []\n",
    "    for i, idx in enumerate(inaccurate_indices):\n",
    "        # 安全地获取实际值\n",
    "        if hasattr(y_test[target], 'iloc'):\n",
    "            actual = y_test[target].iloc[idx]\n",
    "        else:\n",
    "            actual = y_true_values[idx]\n",
    "        \n",
    "        # 计算预测置信区间\n",
    "        prediction = y_pred[idx]\n",
    "\n",
    "        \n",
    "        inaccurate_samples.append({\n",
    "            '样本索引': original_indices[i],\n",
    "            '实际值': actual,\n",
    "            '预测值': prediction,\n",
    "            '绝对误差': errors[idx],\n",
    "            '相对误差(%)': (errors[idx] / np.abs(actual)) * 100 if actual != 0 else float('inf'),\n",
    "\n",
    "        })\n",
    "    \n",
    "    inaccurate_df = pd.DataFrame(inaccurate_samples)\n",
    "    # 按误差降序排列\n",
    "    inaccurate_df = inaccurate_df.sort_values('绝对误差', ascending=False)\n",
    "    \n",
    "    # 打印预测不准确的样本信息\n",
    "    print(\"\\n预测不准确的样本详情 (按误差降序排列):\")\n",
    "    print(inaccurate_df)\n",
    "    \n",
    "    # 保存结果到文件\n",
    "    inaccurate_file = os.path.join(save_folder, f'{target}_GP不准确预测.csv')\n",
    "    inaccurate_df.to_csv(inaccurate_file, index=False)\n",
    "    print(f\"不准确预测分析已保存至 {inaccurate_file}\")\n",
    "else:\n",
    "    print(f\"\\n没有发现预测不准确的样本 (容忍度阈值: {tolerance})\")\n",
    "\n",
    "print(\"\\n高斯过程回归模型训练、评估和异常分析完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接运行的模型加载代码\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 如果需要加载神经网络模型，需要先定义KerasRegressorWrapper类\n",
    "class KerasRegressorWrapper:\n",
    "    def __init__(self, hidden_layers=[128, 64, 32], dropout_rate=0.3, \n",
    "                 learning_rate=0.001, epochs=100, batch_size=32):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X, verbose=0).flatten()\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'hidden_layers': self.hidden_layers,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "# 设置模型文件夹路径\n",
    "model_folder = '训练模型文件'\n",
    "\n",
    "# 初始化models字典\n",
    "if 'models' not in locals():\n",
    "    models = {}\n",
    "\n",
    "print(f\"为目标变量 {target} 加载模型...\")\n",
    "\n",
    "# 查找模型文件，排除Keras模型\n",
    "model_files = [f for f in os.listdir(model_folder) \n",
    "              if f.startswith(f'{target}_') and f.endswith('.pkl') \n",
    "              and not f.endswith('_features.pkl')\n",
    "              and 'Ensemble' not in f and '集成' not in f\n",
    "              and 'Keras' not in f and 'Neural' not in f and 'NN' not in f]\n",
    "\n",
    "print(f\"找到 {len(model_files)} 个模型文件: {model_files}\")\n",
    "\n",
    "# 初始化目标变量的模型字典\n",
    "if target not in models:\n",
    "    models[target] = {}\n",
    "\n",
    "# 加载每个模型\n",
    "for model_file in model_files:\n",
    "    # 从文件名提取模型名称\n",
    "    model_name = model_file.replace(f'{target}_', '').replace('模型.pkl', '')\n",
    "    \n",
    "    print(f\"  加载模型: {model_name}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    model_path = os.path.join(model_folder, model_file)\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    models[target][model_name] = model\n",
    "    print(f\"    {model_name} 加载成功\")\n",
    "\n",
    "print(f\"成功加载 {len(models[target])} 个模型\")\n",
    "print(f\"可用模型: {list(models[target].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VotingEnsemble - 基于模型标准R²性能分配权重\n",
    "print(f\"训练 {target} 的VotingEnsemble集成模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 使用已有模型和已知性能 - 无需重新评估\n",
    "base_models = []\n",
    "model_scores = {}  # 存储标准R²分数\n",
    "model_datasets = {}  # 存储每个模型对应的数据集\n",
    "\n",
    "print(\"收集已有模型的性能评估结果...\")\n",
    "# 使用原始训练代码计算的标准R²\n",
    "if 'XGBoost' in models[target]:\n",
    "    # 不重新评估，而是计算一次标准R²\n",
    "    model = models[target]['XGBoost']\n",
    "    y_pred = model.predict(X_test_tree[target])\n",
    "    r2 = r2_score(y_test[target], y_pred)\n",
    "    base_models.append(('xgb', model))\n",
    "    model_scores['xgb'] = r2\n",
    "    model_datasets['xgb'] = 'tree'\n",
    "    print(f\"  XGBoost - R²: {r2:.4f}\")\n",
    "\n",
    "if 'LightGBM' in models[target]:\n",
    "    model = models[target]['LightGBM']\n",
    "    y_pred = model.predict(X_test_tree[target])\n",
    "    r2 = r2_score(y_test[target], y_pred)\n",
    "    base_models.append(('lgb', model))\n",
    "    model_scores['lgb'] = r2\n",
    "    model_datasets['lgb'] = 'tree'\n",
    "    print(f\"  LightGBM - R²: {r2:.4f}\")\n",
    "\n",
    "if 'HistGradientBoosting' in models[target]:\n",
    "    model = models[target]['HistGradientBoosting']\n",
    "    y_pred = model.predict(X_test_tree[target])\n",
    "    r2 = r2_score(y_test[target], y_pred)\n",
    "    base_models.append(('hgb', model))\n",
    "    model_scores['hgb'] = r2\n",
    "    model_datasets['hgb'] = 'tree'\n",
    "    print(f\"  HistGradientBoosting - R²: {r2:.4f}\")\n",
    "\n",
    "if 'RandomForest' in models[target]:\n",
    "    model = models[target]['RandomForest']\n",
    "    y_pred = model.predict(X_test_tree_filled[target])\n",
    "    r2 = r2_score(y_test[target], y_pred)\n",
    "    base_models.append(('rf', model))\n",
    "    model_scores['rf'] = r2\n",
    "    model_datasets['rf'] = 'tree_filled'\n",
    "    print(f\"  RandomForest - R²: {r2:.4f}\")\n",
    "\n",
    "if 'GaussianProcess' in models[target]:\n",
    "    model = models[target]['GaussianProcess']\n",
    "    y_pred = model.predict(X_test_linear[target])\n",
    "    r2 = r2_score(y_test[target], y_pred)\n",
    "    base_models.append(('gp', model))\n",
    "    model_scores['gp'] = r2\n",
    "    model_datasets['gp'] = 'linear'\n",
    "    print(f\"  GaussianProcess - R²: {r2:.4f}\")\n",
    "\n",
    "# 改为基于标准R²性能计算权重，而不是容忍度R²\n",
    "print(\"\\n根据标准R²模型性能分配权重...\")\n",
    "\n",
    "# 基于标准R²计算权重\n",
    "total_score = sum(model_scores.values())\n",
    "if total_score > 0:  # 防止除以零错误\n",
    "    weights = [model_scores[name] / total_score * len(model_scores) for name, _ in base_models]\n",
    "else:\n",
    "    weights = [1.0 for _ in base_models]  # 如果总分为0，则均等分配权重\n",
    "\n",
    "print(\"  基于标准R²分配权重\")\n",
    "\n",
    "# 确保权重至少为0.5，防止某些模型权重过低\n",
    "min_weight = 0.5\n",
    "weights = [max(w, min_weight) for w in weights]\n",
    "\n",
    "# 打印权重\n",
    "for i, (name, _) in enumerate(base_models):\n",
    "    print(f\"  {name} 权重: {weights[i]:.4f}\")\n",
    "\n",
    "# 检查是否有足够的模型可用于集成\n",
    "if len(base_models) >= 2:\n",
    "    try:\n",
    "        # 创建自定义投票回归器的封装，确保使用正确的数据集\n",
    "        class EnhancedVotingRegressor:\n",
    "            def __init__(self, estimators, weights, datasets, target_name):\n",
    "                self.estimators = estimators\n",
    "                self.weights = weights\n",
    "                self.datasets = datasets\n",
    "                self.target_name = target_name\n",
    "                \n",
    "                # 归一化权重\n",
    "                self.weights = np.array(self.weights)\n",
    "                self.weights = self.weights / np.sum(self.weights)\n",
    "                \n",
    "            def predict(self, X):\n",
    "                # 对每个模型获取预测，并根据模型类型使用适当的数据预处理\n",
    "                predictions = []\n",
    "                \n",
    "                for i, (name, model) in enumerate(self.estimators):\n",
    "                    # 选择合适的数据格式\n",
    "                    dataset_type = self.datasets.get(name, 'standard')\n",
    "                    \n",
    "                    if dataset_type == 'tree':\n",
    "                        # 对于支持NaN的树模型，直接使用X\n",
    "                        X_model = X\n",
    "                    elif dataset_type == 'tree_filled':\n",
    "                        # 对于不支持NaN的树模型，需要填充X\n",
    "                        if isinstance(X, pd.DataFrame):\n",
    "                            X_model = X.fillna(0)\n",
    "                        else:\n",
    "                            X_model = X\n",
    "                    elif dataset_type == 'linear':\n",
    "                        # 对于线性模型，使用线性预处理的X\n",
    "                        X_model = X\n",
    "                    else:\n",
    "                        # 默认情况下直接使用X\n",
    "                        X_model = X\n",
    "                    \n",
    "                    # 获取当前模型的预测\n",
    "                    pred = model.predict(X_model)\n",
    "                    predictions.append(pred)\n",
    "                \n",
    "                # 加权平均所有预测\n",
    "                weighted_pred = np.zeros(predictions[0].shape)\n",
    "                for i, pred in enumerate(predictions):\n",
    "                    weighted_pred += self.weights[i] * pred\n",
    "                \n",
    "                return weighted_pred\n",
    "        \n",
    "        # 创建投票集成模型\n",
    "        print(\"创建投票集成模型...\")\n",
    "        voting_model = EnhancedVotingRegressor(\n",
    "            estimators=base_models,\n",
    "            weights=weights,\n",
    "            datasets=model_datasets,\n",
    "            target_name=target\n",
    "        )\n",
    "        \n",
    "        # 获取训练集和测试集预测\n",
    "        train_predictions = {}\n",
    "        test_predictions = {}\n",
    "        \n",
    "        # 获取每个基础模型的预测\n",
    "        for name, model in base_models:\n",
    "            if model_datasets[name] == 'tree':\n",
    "                train_predictions[name] = model.predict(X_train_tree[target])\n",
    "                test_predictions[name] = model.predict(X_test_tree[target])\n",
    "            elif model_datasets[name] == 'tree_filled':\n",
    "                train_predictions[name] = model.predict(X_train_tree_filled[target])\n",
    "                test_predictions[name] = model.predict(X_test_tree_filled[target])\n",
    "            elif model_datasets[name] == 'linear':\n",
    "                train_predictions[name] = model.predict(X_train_linear[target])\n",
    "                test_predictions[name] = model.predict(X_test_linear[target])\n",
    "        \n",
    "        # 计算加权预测\n",
    "        y_train_pred = np.zeros(len(y_train[target]))\n",
    "        y_test_pred = np.zeros(len(y_test[target]))\n",
    "        \n",
    "        for i, (name, _) in enumerate(base_models):\n",
    "            y_train_pred += weights[i] * train_predictions[name]\n",
    "            y_test_pred += weights[i] * test_predictions[name]\n",
    "        \n",
    "        # 归一化权重\n",
    "        total_weight = sum(weights)\n",
    "        y_train_pred /= total_weight\n",
    "        y_test_pred /= total_weight\n",
    "        \n",
    "        # 计算性能指标\n",
    "        train_r2 = r2_score(y_train[target], y_train_pred)\n",
    "        test_r2 = r2_score(y_test[target], y_test_pred)\n",
    "        \n",
    "        train_tol_r2 = tolerance_r2_score(y_train[target], y_train_pred, tolerance=current_tolerance, target=target)\n",
    "        test_tol_r2 = tolerance_r2_score(y_test[target], y_test_pred, tolerance=current_tolerance, target=target)\n",
    "        \n",
    "        train_within_tol = prediction_within_tolerance(y_train[target], y_train_pred, tolerance=current_tolerance, target=target)\n",
    "        test_within_tol = prediction_within_tolerance(y_test[target], y_test_pred, tolerance=current_tolerance, target=target)\n",
    "        # 保存训练集和测试集的预测结果\n",
    "        train_prediction = pd.DataFrame({\n",
    "            '实际值': y_train[target],\n",
    "            '集成预测值': y_train_pred,\n",
    "            '误差': np.abs(y_train[target] - y_train_pred)\n",
    "        })\n",
    "\n",
    "        test_prediction = pd.DataFrame({\n",
    "            '实际值': y_test[target],\n",
    "            '集成预测值': y_test_pred,\n",
    "            '误差': np.abs(y_test[target] - y_test_pred)\n",
    "        })\n",
    "\n",
    "        # 添加各基础模型的预测结果\n",
    "        for name, _ in base_models:\n",
    "            train_prediction[f'{name}预测值'] = train_predictions[name]\n",
    "            test_prediction[f'{name}预测值'] = test_predictions[name]\n",
    "\n",
    "        # 保存到文件\n",
    "        train_file = os.path.join(save_folder, f'{target}_投票集成模型训练集预测结果.csv')\n",
    "        test_file = os.path.join(save_folder, f'{target}_投票集成模型测试集预测结果.csv')\n",
    "\n",
    "        train_prediction.to_csv(train_file, index=False)\n",
    "        test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "        print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "        print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "        # 输出性能指标\n",
    "        print(f\"\\n投票集成模型性能:\")\n",
    "        print(f\"训练集: R²={train_r2:.4f}, 容忍度R²={train_tol_r2:.4f}, 在容忍范围内比例={train_within_tol:.2%}\")\n",
    "        print(f\"测试集: R²={test_r2:.4f}, 容忍度R²={test_tol_r2:.4f}, 在容忍范围内比例={test_within_tol:.2%}\")\n",
    "        \n",
    "        # 与各个基础模型比较性能\n",
    "        print(\"\\n与各基础模型性能比较:\")\n",
    "        for name, _ in base_models:\n",
    "            base_train_pred = train_predictions[name]\n",
    "            base_test_pred = test_predictions[name]\n",
    "            \n",
    "            base_train_r2 = r2_score(y_train[target], base_train_pred)\n",
    "            base_test_r2 = r2_score(y_test[target], base_test_pred)\n",
    "            \n",
    "            print(f\"  vs {name}:\")\n",
    "            print(f\"    训练集R²: {train_r2:.4f} vs {base_train_r2:.4f} (差异: {train_r2-base_train_r2:.4f})\")\n",
    "            print(f\"    测试集R²: {test_r2:.4f} vs {base_test_r2:.4f} (差异: {test_r2-base_test_r2:.4f})\")\n",
    "        \n",
    "        # 保存模型\n",
    "        models[target]['VotingEnsemble'] = voting_model\n",
    "        # 使用pickle保存投票集成模型\n",
    "        ensemble_model_file = os.path.join(model_folder, f'{target}_投票集成模型.pkl')\n",
    "        with open(ensemble_model_file, 'wb') as f:\n",
    "            pickle.dump(voting_model, f)\n",
    "        print(f\"投票集成模型已保存至 {ensemble_model_file}\")\n",
    "        # 可视化: 预测vs实际值散点图 (训练集和测试集)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 训练集散点图\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(y_train[target], y_train_pred, alpha=0.5)\n",
    "        plt.plot([y_train[target].min(), y_train[target].max()], [y_train[target].min(), y_train[target].max()], 'r--')\n",
    "        plt.xlabel('实际值')\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title(f'训练集: R²={train_r2:.4f}')\n",
    "        \n",
    "        # 测试集散点图\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(y_test[target], y_test_pred, alpha=0.5)\n",
    "        plt.plot([y_test[target].min(), y_test[target].max()], [y_test[target].min(), y_test[target].max()], 'r--')\n",
    "        plt.xlabel('实际值')\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title(f'测试集: R²={test_r2:.4f}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 绘制误差分布\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 训练集误差\n",
    "        plt.subplot(1, 2, 1)\n",
    "        train_errors = y_train[target] - y_train_pred\n",
    "        plt.hist(train_errors, bins=30, alpha=0.7)\n",
    "        plt.axvline(x=0, color='r', linestyle='--')\n",
    "        plt.xlabel('预测误差')\n",
    "        plt.ylabel('频次')\n",
    "        plt.title(f'训练集误差分布 (MAE={np.abs(train_errors).mean():.4f})')\n",
    "        \n",
    "        # 测试集误差\n",
    "        plt.subplot(1, 2, 2)\n",
    "        test_errors = y_test[target] - y_test_pred\n",
    "        plt.hist(test_errors, bins=30, alpha=0.7)\n",
    "        plt.axvline(x=0, color='r', linestyle='--')\n",
    "        plt.xlabel('预测误差')\n",
    "        plt.ylabel('频次')\n",
    "        plt.title(f'测试集误差分布 (MAE={np.abs(test_errors).mean():.4f})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 绘制权重分布\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        model_names = [name for name, _ in base_models]\n",
    "        plt.bar(model_names, weights)\n",
    "        plt.xlabel('模型')\n",
    "        plt.ylabel('权重')\n",
    "        plt.title(f'{target} - 投票集成模型权重分布')\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 对比各模型预测分布\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        model_preds = [y_test_pred] + [test_predictions[name] for name, _ in base_models]\n",
    "        model_labels = ['Voting'] + [name for name, _ in base_models]\n",
    "        \n",
    "        plt.boxplot(model_preds, labels=model_labels)\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title('投票集成模型与各基础模型预测分布对比')\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"创建投票集成模型失败: {str(e)}\")\n",
    "        print(f\"错误详情: {traceback.format_exc()}\")\n",
    "else:\n",
    "    print(\"没有足够的基础模型来创建投票集成\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 自适应集成模型 - 根据样本特征动态选择最佳模型\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(f\"训练 {target} 的自适应集成模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 创建有效模型列表及其对应的数据集\n",
    "available_models = []\n",
    "model_input_data = {}\n",
    "\n",
    "if 'XGBoost' in models[target]:\n",
    "    available_models.append('XGBoost')\n",
    "    model_input_data['XGBoost'] = {\n",
    "        'train': X_train_tree[target],\n",
    "        'test': X_test_tree[target]\n",
    "    }\n",
    "\n",
    "if 'LightGBM' in models[target]:\n",
    "    available_models.append('LightGBM')\n",
    "    model_input_data['LightGBM'] = {\n",
    "        'train': X_train_tree[target],\n",
    "        'test': X_test_tree[target]\n",
    "    }\n",
    "    \n",
    "if 'HistGradientBoosting' in models[target]:\n",
    "    available_models.append('HistGradientBoosting')\n",
    "    model_input_data['HistGradientBoosting'] = {\n",
    "        'train': X_train_tree[target],\n",
    "        'test': X_test_tree[target]\n",
    "    }\n",
    "    \n",
    "if 'RandomForest' in models[target]:\n",
    "    available_models.append('RandomForest')\n",
    "    model_input_data['RandomForest'] = {\n",
    "        'train': X_train_tree_filled[target],\n",
    "        'test': X_test_tree_filled[target]\n",
    "    }\n",
    "    \n",
    "if 'GaussianProcess' in models[target]:\n",
    "    available_models.append('GaussianProcess')\n",
    "    model_input_data['GaussianProcess'] = {\n",
    "        'train': X_train_linear[target],\n",
    "        'test': X_test_linear[target]\n",
    "    }\n",
    "\n",
    "print(f\"可用模型: {available_models}\")\n",
    "\n",
    "if len(available_models) < 2:\n",
    "    print(\"自适应集成至少需要两个模型，目前可用模型不足\")\n",
    "else:\n",
    "    try:\n",
    "        # 步骤1: 为每个样本生成各模型的预测\n",
    "        print(\"为每个样本生成所有模型的预测...\")\n",
    "        train_predictions = {}\n",
    "        test_predictions = {}\n",
    "        \n",
    "        for model_name in available_models:\n",
    "            model = models[target][model_name]\n",
    "            # 使用适当的数据集进行预测\n",
    "            train_data = model_input_data[model_name]['train']\n",
    "            test_data = model_input_data[model_name]['test']\n",
    "            \n",
    "            train_pred = model.predict(train_data)\n",
    "            test_pred = model.predict(test_data)\n",
    "            \n",
    "            train_predictions[model_name] = train_pred\n",
    "            test_predictions[model_name] = test_pred\n",
    "        \n",
    "        # 步骤2: 计算每个样本的每个模型预测误差\n",
    "        print(\"计算各模型在每个样本上的预测误差...\")\n",
    "        train_errors = {}\n",
    "        for model_name in available_models:\n",
    "            pred = train_predictions[model_name]\n",
    "            error = np.abs(y_train[target].values - pred)\n",
    "            train_errors[model_name] = error\n",
    "        \n",
    "        # 步骤3: 创建一个元模型，学习如何根据特征选择最佳模型\n",
    "        print(\"训练元模型来决定每个样本应使用哪个模型...\")\n",
    "        \n",
    "        # 为每个样本找出表现最好的模型\n",
    "        best_model_indices = np.zeros(len(y_train[target]), dtype=int)\n",
    "        model_name_to_idx = {name: idx for idx, name in enumerate(available_models)}\n",
    "        \n",
    "        for i in range(len(y_train[target])):\n",
    "            model_errors = [train_errors[model_name][i] for model_name in available_models]\n",
    "            best_model_idx = np.argmin(model_errors)\n",
    "            best_model_indices[i] = best_model_idx\n",
    "        \n",
    "        # 用原始特征训练一个分类器来预测最佳模型\n",
    "        meta_classifier = RandomForestClassifier(\n",
    "            n_estimators=200, \n",
    "            max_depth=4,\n",
    "            min_samples_split=2,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        meta_classifier.fit(X_train[target], best_model_indices)\n",
    "        \n",
    "        # 步骤4: 在训练集和测试集上使用元模型选择最佳模型\n",
    "        print(\"在训练集和测试集上应用元模型...\")\n",
    "        train_best_models = meta_classifier.predict(X_train[target])\n",
    "        test_best_models = meta_classifier.predict(X_test[target])\n",
    "        \n",
    "        # 步骤5: 根据元模型的选择，为每个样本选择相应的预测\n",
    "        train_adaptive_predictions = np.zeros(len(y_train[target]))\n",
    "        test_adaptive_predictions = np.zeros(len(y_test[target]))\n",
    "        \n",
    "        # 为训练集计算自适应预测\n",
    "        for i in range(len(y_train[target])):\n",
    "            selected_model = available_models[train_best_models[i]]\n",
    "            train_adaptive_predictions[i] = train_predictions[selected_model][i]\n",
    "        \n",
    "        # 为测试集计算自适应预测\n",
    "        for i in range(len(y_test[target])):\n",
    "            selected_model = available_models[test_best_models[i]]\n",
    "            test_adaptive_predictions[i] = test_predictions[selected_model][i]\n",
    "        \n",
    "        # 步骤6: 评估自适应集成的性能\n",
    "        train_r2 = r2_score(y_train[target], train_adaptive_predictions)\n",
    "        train_tol_r2 = tolerance_r2_score(y_train[target], train_adaptive_predictions, tolerance=current_tolerance, target=target)\n",
    "        train_within_tol = prediction_within_tolerance(y_train[target], train_adaptive_predictions, tolerance=current_tolerance, target=target)\n",
    "        \n",
    "        test_r2 = r2_score(y_test[target], test_adaptive_predictions)\n",
    "        test_tol_r2 = tolerance_r2_score(y_test[target], test_adaptive_predictions, tolerance=current_tolerance, target=target)\n",
    "        test_within_tol = prediction_within_tolerance(y_test[target], test_adaptive_predictions, tolerance=current_tolerance, target=target)\n",
    "        \n",
    "        print(f\"\\n自适应集成模型性能:\")\n",
    "        print(f\"训练集: R²={train_r2:.4f}, 容忍度R²={train_tol_r2:.4f}, 在容忍范围内比例={train_within_tol:.2%}\")\n",
    "        print(f\"测试集: R²={test_r2:.4f}, 容忍度R²={test_tol_r2:.4f}, 在容忍范围内比例={test_within_tol:.2%}\")\n",
    "        \n",
    "        # 步骤7: 比较自适应集成与各个基础模型的性能\n",
    "        print(\"\\n与各基础模型性能比较:\")\n",
    "        for model_name in available_models:\n",
    "            model_train_pred = train_predictions[model_name]\n",
    "            model_test_pred = test_predictions[model_name]\n",
    "            \n",
    "            model_train_r2 = r2_score(y_train[target], model_train_pred)\n",
    "            model_test_r2 = r2_score(y_test[target], model_test_pred)\n",
    "            \n",
    "            train_r2_diff = train_r2 - model_train_r2\n",
    "            test_r2_diff = test_r2 - model_test_r2\n",
    "            \n",
    "            print(f\"  vs {model_name}:\")\n",
    "            print(f\"    训练集R²: {train_r2:.4f} vs {model_train_r2:.4f} (差异: {train_r2_diff:.4f})\")\n",
    "            print(f\"    测试集R²: {test_r2:.4f} vs {model_test_r2:.4f} (差异: {test_r2_diff:.4f})\")\n",
    "        \n",
    "        # 步骤8: 分析各模型被选择的频率\n",
    "        train_model_selection_counts = np.bincount(train_best_models, minlength=len(available_models))\n",
    "        train_model_selection_percent = train_model_selection_counts / len(train_best_models) * 100\n",
    "        \n",
    "        test_model_selection_counts = np.bincount(test_best_models, minlength=len(available_models))\n",
    "        test_model_selection_percent = test_model_selection_counts / len(test_best_models) * 100\n",
    "        \n",
    "        print(\"\\n各模型在训练集上的选择频率:\")\n",
    "        for i, model_name in enumerate(available_models):\n",
    "            print(f\"  {model_name}: {train_model_selection_counts[i]} 次 ({train_model_selection_percent[i]:.2f}%)\")\n",
    "        \n",
    "        print(\"\\n各模型在测试集上的选择频率:\")\n",
    "        for i, model_name in enumerate(available_models):\n",
    "            print(f\"  {model_name}: {test_model_selection_counts[i]} 次 ({test_model_selection_percent[i]:.2f}%)\")\n",
    "        \n",
    "        # 步骤9: 创建并保存自适应集成模型\n",
    "        class AdaptiveEnsembleModel:\n",
    "            def __init__(self, meta_classifier, models_dict, available_models, model_input_data):\n",
    "                self.meta_classifier = meta_classifier\n",
    "                self.models_dict = models_dict\n",
    "                self.available_models = available_models\n",
    "                self.model_input_data = model_input_data\n",
    "                \n",
    "                # 添加数据类型映射\n",
    "                self.data_type_map = {\n",
    "                    'XGBoost': 'tree',\n",
    "                    'LightGBM': 'tree',\n",
    "                    'HistGradientBoosting': 'tree',\n",
    "                    'RandomForest': 'tree_filled',\n",
    "                    'GaussianProcess': 'linear'\n",
    "                }\n",
    "                \n",
    "            def predict(self, X):\n",
    "                # 确保X是DataFrame格式，保持列名\n",
    "                if not isinstance(X, pd.DataFrame):\n",
    "                    if hasattr(X, 'shape') and len(X.shape) == 2:\n",
    "                        if hasattr(X_train[target], 'columns'):\n",
    "                            X = pd.DataFrame(X, columns=X_train[target].columns)\n",
    "                        else:\n",
    "                            X = pd.DataFrame(X)\n",
    "                \n",
    "                # 首先预测每个样本应使用哪个模型\n",
    "                model_choices = self.meta_classifier.predict(X)\n",
    "                \n",
    "                # 初始化预测结果数组\n",
    "                predictions = np.zeros(len(X))\n",
    "                \n",
    "                # 为每个样本获取相应模型的预测\n",
    "                for i in range(len(X)):\n",
    "                    # 获取为当前样本选择的模型\n",
    "                    model_idx = model_choices[i]\n",
    "                    model_name = self.available_models[model_idx]\n",
    "                    model = self.models_dict[model_name]\n",
    "                    \n",
    "                    # 准备单个样本的数据\n",
    "                    if isinstance(X, pd.DataFrame):\n",
    "                        x_sample = X.iloc[[i]]\n",
    "                    else:\n",
    "                        if len(X.shape) == 1:\n",
    "                            x_sample = X.reshape(1, -1)\n",
    "                        else:\n",
    "                            x_sample = X[[i]]\n",
    "                    \n",
    "                    # 根据模型类型进行预处理\n",
    "                    data_type = self.data_type_map.get(model_name, 'standard')\n",
    "                    \n",
    "                    if data_type == 'tree':\n",
    "                        # 支持NaN值的树模型，不需要特殊处理\n",
    "                        x_processed = x_sample\n",
    "                    elif data_type == 'tree_filled':\n",
    "                        # 不支持NaN的树模型，需要填充\n",
    "                        if isinstance(x_sample, pd.DataFrame):\n",
    "                            x_processed = x_sample.fillna(0)\n",
    "                        else:\n",
    "                            x_processed = np.nan_to_num(x_sample, 0)\n",
    "                    elif data_type == 'linear':\n",
    "                        # 线性模型的特殊处理，如果有需要\n",
    "                        x_processed = x_sample\n",
    "                    else:\n",
    "                        # 默认情况\n",
    "                        x_processed = x_sample\n",
    "                    \n",
    "                    # 获取预测\n",
    "                    pred = model.predict(x_processed)\n",
    "                    predictions[i] = pred[0] if hasattr(pred, '__len__') else pred\n",
    "                \n",
    "                return predictions\n",
    "                \n",
    "            def get_feature_importances(self):\n",
    "                # 获取元分类器的特征重要性\n",
    "                if hasattr(self.meta_classifier, 'feature_importances_'):\n",
    "                    return self.meta_classifier.feature_importances_\n",
    "                return None\n",
    "        \n",
    "        # 创建自适应集成模型实例\n",
    "        adaptive_model = AdaptiveEnsembleModel(\n",
    "            meta_classifier=meta_classifier,\n",
    "            models_dict=models[target],\n",
    "            available_models=available_models,\n",
    "            model_input_data=model_input_data\n",
    "        )\n",
    "        \n",
    "        # 保存模型\n",
    "        models[target]['AdaptiveEnsemble'] = adaptive_model\n",
    "        # 使用pickle保存自适应集成模型\n",
    "        adaptive_model_file = os.path.join(model_folder, f'{target}_自适应集成模型.pkl')\n",
    "        with open(adaptive_model_file, 'wb') as f:\n",
    "            pickle.dump(adaptive_model, f)\n",
    "        print(f\"自适应集成模型已保存至 {adaptive_model_file}\")\n",
    "\n",
    "        # 保存训练集和测试集的预测结果\n",
    "        train_prediction = pd.DataFrame({\n",
    "            '实际值': y_train[target],\n",
    "            '自适应集成预测值': train_adaptive_predictions,\n",
    "            '误差': np.abs(y_train[target] - train_adaptive_predictions)\n",
    "        })\n",
    "\n",
    "        test_prediction = pd.DataFrame({\n",
    "            '实际值': y_test[target],\n",
    "            '自适应集成预测值': test_adaptive_predictions,\n",
    "            '误差': np.abs(y_test[target] - test_adaptive_predictions)\n",
    "        })\n",
    "\n",
    "        # 添加各基础模型的预测结果以便比较\n",
    "        for model_name in available_models:\n",
    "            train_prediction[f'{model_name}预测值'] = train_predictions[model_name]\n",
    "            test_prediction[f'{model_name}预测值'] = test_predictions[model_name]\n",
    "\n",
    "        # 保存到文件\n",
    "        train_file = os.path.join(save_folder, f'{target}_自适应集成训练集预测结果.csv')\n",
    "        test_file = os.path.join(save_folder, f'{target}_自适应集成测试集预测结果.csv')\n",
    "\n",
    "        train_prediction.to_csv(train_file, index=False)\n",
    "        test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "        print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "        print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "        # 可视化: 预测vs实际值散点图 (训练集和测试集)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 训练集散点图\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(y_train[target], train_adaptive_predictions, alpha=0.5)\n",
    "        plt.plot([y_train[target].min(), y_train[target].max()], [y_train[target].min(), y_train[target].max()], 'r--')\n",
    "        plt.xlabel('实际值')\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title(f'训练集: R²={train_r2:.4f}')\n",
    "        \n",
    "        # 测试集散点图\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(y_test[target], test_adaptive_predictions, alpha=0.5)\n",
    "        plt.plot([y_test[target].min(), y_test[target].max()], [y_test[target].min(), y_test[target].max()], 'r--')\n",
    "        plt.xlabel('实际值')\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title(f'测试集: R²={test_r2:.4f}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 绘制误差分布\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 训练集误差\n",
    "        plt.subplot(1, 2, 1)\n",
    "        train_errors_plot = y_train[target] - train_adaptive_predictions\n",
    "        plt.hist(train_errors_plot, bins=30, alpha=0.7)\n",
    "        plt.axvline(x=0, color='r', linestyle='--')\n",
    "        plt.xlabel('预测误差')\n",
    "        plt.ylabel('频次')\n",
    "        plt.title(f'训练集误差分布 (MAE={np.abs(train_errors_plot).mean():.4f})')\n",
    "        \n",
    "        # 测试集误差\n",
    "        plt.subplot(1, 2, 2)\n",
    "        test_errors_plot = y_test[target] - test_adaptive_predictions\n",
    "        plt.hist(test_errors_plot, bins=30, alpha=0.7)\n",
    "        plt.axvline(x=0, color='r', linestyle='--')\n",
    "        plt.xlabel('预测误差')\n",
    "        plt.ylabel('频次')\n",
    "        plt.title(f'测试集误差分布 (MAE={np.abs(test_errors_plot).mean():.4f})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 保存元分类器的特征重要性\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': X_train[target].columns,\n",
    "            'Importance': meta_classifier.feature_importances_\n",
    "        })\n",
    "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "        # 保存特征重要性数据\n",
    "        importance_file = os.path.join(save_folder, f'{target}_自适应集成特征重要性.csv')\n",
    "        feature_importance.to_csv(importance_file, index=False)\n",
    "        print(f\"特征重要性数据已保存至 {importance_file}\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "        plt.xlabel('重要性')\n",
    "        plt.ylabel('特征')\n",
    "        plt.title(f'{target} - 自适应集成模型选择特征重要性')\n",
    "        plt.grid(True, axis='x')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 绘制模型选择频率饼图\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 训练集上的模型选择频率\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.pie(train_model_selection_counts, labels=available_models, autopct='%1.1f%%')\n",
    "        plt.title(f'训练集 - 模型选择频率')\n",
    "        \n",
    "        # 测试集上的模型选择频率\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.pie(test_model_selection_counts, labels=available_models, autopct='%1.1f%%')\n",
    "        plt.title(f'测试集 - 模型选择频率')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        # 保存模型选择频率数据\n",
    "        model_selection_data = pd.DataFrame({\n",
    "            '模型': available_models,\n",
    "            '训练集选择次数': train_model_selection_counts,\n",
    "            '训练集选择百分比': train_model_selection_percent,\n",
    "            '测试集选择次数': test_model_selection_counts,\n",
    "            '测试集选择百分比': test_model_selection_percent\n",
    "        })\n",
    "\n",
    "        selection_file = os.path.join(save_folder, f'{target}_自适应集成模型选择频率.csv')\n",
    "        model_selection_data.to_csv(selection_file, index=False)\n",
    "        print(f\"模型选择频率数据已保存至 {selection_file}\")\n",
    "        # 绘制误差分布与模型选择关系\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # 对比测试集上各模型的预测结果\n",
    "        model_data = [test_adaptive_predictions] + [test_predictions[model] for model in available_models]\n",
    "        model_labels = ['自适应集成'] + available_models\n",
    "        \n",
    "        plt.boxplot(model_data, labels=model_labels)\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title('自适应集成模型与各基础模型预测分布对比')\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"创建自适应集成模型失败: {str(e)}\")\n",
    "        print(f\"错误详情: {traceback.format_exc()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加权平均集成模型 - 使用标准R²优化权重\n",
    "print(f\"训练 {target} 的加权平均集成模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 创建有效模型列表及其预测结果\n",
    "available_models = []\n",
    "train_predictions = {}\n",
    "test_predictions = {}\n",
    "\n",
    "if 'XGBoost' in models[target]:\n",
    "    model = models[target]['XGBoost']\n",
    "    train_pred = model.predict(X_train_tree[target])\n",
    "    test_pred = model.predict(X_test_tree[target])\n",
    "    available_models.append('XGBoost')\n",
    "    train_predictions['XGBoost'] = train_pred\n",
    "    test_predictions['XGBoost'] = test_pred\n",
    "\n",
    "if 'LightGBM' in models[target]:\n",
    "    model = models[target]['LightGBM']\n",
    "    train_pred = model.predict(X_train_tree[target])\n",
    "    test_pred = model.predict(X_test_tree[target])\n",
    "    available_models.append('LightGBM')\n",
    "    train_predictions['LightGBM'] = train_pred\n",
    "    test_predictions['LightGBM'] = test_pred\n",
    "    \n",
    "if 'HistGradientBoosting' in models[target]:\n",
    "    model = models[target]['HistGradientBoosting']\n",
    "    train_pred = model.predict(X_train_tree[target])\n",
    "    test_pred = model.predict(X_test_tree[target])\n",
    "    available_models.append('HistGradientBoosting')\n",
    "    train_predictions['HistGradientBoosting'] = train_pred\n",
    "    test_predictions['HistGradientBoosting'] = test_pred\n",
    "    \n",
    "if 'RandomForest' in models[target]:\n",
    "    model = models[target]['RandomForest']\n",
    "    train_pred = model.predict(X_train_tree_filled[target])\n",
    "    test_pred = model.predict(X_test_tree_filled[target])\n",
    "    available_models.append('RandomForest')\n",
    "    train_predictions['RandomForest'] = train_pred\n",
    "    test_predictions['RandomForest'] = test_pred\n",
    "    \n",
    "if 'GaussianProcess' in models[target]:\n",
    "    model = models[target]['GaussianProcess']\n",
    "    train_pred = model.predict(X_train_linear[target])\n",
    "    test_pred = model.predict(X_test_linear[target])\n",
    "    available_models.append('GaussianProcess')\n",
    "    train_predictions['GaussianProcess'] = train_pred\n",
    "    test_predictions['GaussianProcess'] = test_pred\n",
    "\n",
    "print(f\"可用模型: {available_models}\")\n",
    "\n",
    "if len(available_models) < 2:\n",
    "    print(\"加权平均集成至少需要两个模型，目前可用模型不足\")\n",
    "else:\n",
    "    try:\n",
    "        # 通过优化找到最优权重\n",
    "        print(\"寻找最优权重组合...\")\n",
    "        from scipy.optimize import minimize\n",
    "        \n",
    "        # 定义自定义加权平均函数\n",
    "        def weighted_prediction(weights, preds_list):\n",
    "            weighted_preds = np.zeros(preds_list[0].shape)\n",
    "            for i, preds in enumerate(preds_list):\n",
    "                weighted_preds += weights[i] * preds\n",
    "            return weighted_preds\n",
    "        \n",
    "        # 定义要优化的损失函数（最大化标准R²）- 修改为使用标准R²而非容忍度R²\n",
    "        def neg_r2(weights, preds_list, y_true):\n",
    "            # 归一化权重确保和为1\n",
    "            weights = np.array(weights)\n",
    "            weights = weights / np.sum(weights)\n",
    "            \n",
    "            weighted_preds = weighted_prediction(weights, preds_list)\n",
    "            r2 = r2_score(y_true, weighted_preds)\n",
    "            return -r2  # 最小化负的R²（即最大化R²）\n",
    "        \n",
    "        # 准备用于优化的预测值列表\n",
    "        train_preds_list = [train_predictions[model_name] for model_name in available_models]\n",
    "        \n",
    "        # 初始权重（均等）\n",
    "        initial_weights = np.ones(len(available_models)) / len(available_models)\n",
    "        \n",
    "        # 约束：权重和为1，所有权重非负\n",
    "        constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "        bounds = [(0, 1) for _ in range(len(available_models))]\n",
    "        \n",
    "        # 使用SLSQP优化算法寻找最优权重\n",
    "        print(\"优化权重中...\")\n",
    "        result = minimize(\n",
    "            neg_r2, \n",
    "            initial_weights, \n",
    "            args=(train_preds_list, y_train[target]),\n",
    "            bounds=bounds,\n",
    "            constraints=constraints,\n",
    "            method='SLSQP'\n",
    "        )\n",
    "        \n",
    "        if result.success:\n",
    "            # 获取最优权重并归一化\n",
    "            optimal_weights = result.x\n",
    "            optimal_weights = optimal_weights / np.sum(optimal_weights)\n",
    "            \n",
    "            print(\"\\n找到最优权重组合:\")\n",
    "            for i, model_name in enumerate(available_models):\n",
    "                print(f\"  {model_name}: {optimal_weights[i]:.4f}\")\n",
    "                \n",
    "            # 使用最优权重在训练集和测试集上评估性能\n",
    "            train_weighted_preds = weighted_prediction(\n",
    "                optimal_weights, \n",
    "                [train_predictions[model_name] for model_name in available_models]\n",
    "            )\n",
    "            \n",
    "            test_weighted_preds = weighted_prediction(\n",
    "                optimal_weights, \n",
    "                [test_predictions[model_name] for model_name in available_models]\n",
    "            )\n",
    "            \n",
    "            # 计算性能指标\n",
    "            train_r2 = r2_score(y_train[target], train_weighted_preds)\n",
    "            train_tol_r2 = tolerance_r2_score(y_train[target], train_weighted_preds, tolerance=current_tolerance, target=target)\n",
    "            train_within_tol = prediction_within_tolerance(y_train[target], train_weighted_preds, tolerance=current_tolerance, target=target)\n",
    "            \n",
    "            test_r2 = r2_score(y_test[target], test_weighted_preds)\n",
    "            test_tol_r2 = tolerance_r2_score(y_test[target], test_weighted_preds, tolerance=current_tolerance, target=target)\n",
    "            test_within_tol = prediction_within_tolerance(y_test[target], test_weighted_preds, tolerance=current_tolerance, target=target)\n",
    "            \n",
    "            print(\"\\n加权平均集成性能:\")\n",
    "            print(f\"  训练集 - R²: {train_r2:.4f}, 容忍度R²: {train_tol_r2:.4f}, 在容忍范围内: {train_within_tol:.2%}\")\n",
    "            print(f\"  测试集 - R²: {test_r2:.4f}, 容忍度R²: {test_tol_r2:.4f}, 在容忍范围内: {test_within_tol:.2%}\")\n",
    "            \n",
    "            # 与各个基础模型比较性能\n",
    "            print(\"\\n与各基础模型性能比较:\")\n",
    "            for model_name in available_models:\n",
    "                model_test_pred = test_predictions[model_name]\n",
    "                model_r2 = r2_score(y_test[target], model_test_pred)\n",
    "                model_tol_r2 = tolerance_r2_score(y_test[target], model_test_pred, tolerance=current_tolerance, target=target)\n",
    "                \n",
    "                r2_diff = test_r2 - model_r2\n",
    "                tol_r2_diff = test_tol_r2 - model_tol_r2\n",
    "                \n",
    "                print(f\"  vs {model_name}:\")\n",
    "                print(f\"    R² 差异: {r2_diff:.4f} ({'+' if r2_diff > 0 else ''}{r2_diff/max(0.0001, abs(model_r2))*100:.2f}%)\")\n",
    "                print(f\"    容忍度R² 差异: {tol_r2_diff:.4f} ({'+' if tol_r2_diff > 0 else ''}{tol_r2_diff/max(0.0001, abs(model_tol_r2))*100:.2f}%)\")\n",
    "            \n",
    "            # 创建加权平均集成模型\n",
    "            class WeightedAverageEnsemble:\n",
    "                def __init__(self, models_dict, model_names, weights, model_datasets):\n",
    "                    self.models_dict = models_dict\n",
    "                    self.model_names = model_names\n",
    "                    self.weights = weights\n",
    "                    self.model_datasets = model_datasets\n",
    "                    \n",
    "                def predict(self, X):\n",
    "                    predictions = []\n",
    "                    \n",
    "                    for i, model_name in enumerate(self.model_names):\n",
    "                        model = self.models_dict[model_name]\n",
    "                        \n",
    "                        # 获取适当的数据格式\n",
    "                        if model_name in ['XGBoost', 'LightGBM', 'HistGradientBoosting']:\n",
    "                            if isinstance(X, pd.DataFrame):\n",
    "                                # 假设X是原始数据框，需要应用适当的预处理\n",
    "                                X_model = X  # 应该在实际应用中进行适当的预处理转换\n",
    "                            else:\n",
    "                                X_model = X\n",
    "                        elif model_name == 'RandomForest':\n",
    "                            if isinstance(X, pd.DataFrame):\n",
    "                                # 对于RandomForest需要填充NaN\n",
    "                                X_model = X.fillna(0)\n",
    "                            else:\n",
    "                                X_model = X\n",
    "                        elif model_name == 'GaussianProcess':\n",
    "                            if isinstance(X, pd.DataFrame):\n",
    "                                # 假设X是原始数据框，需要应用适当的预处理\n",
    "                                X_model = X  # 应该在实际应用中进行适当的预处理转换\n",
    "                            else:\n",
    "                                X_model = X\n",
    "                        else:\n",
    "                            X_model = X\n",
    "                            \n",
    "                        model_pred = model.predict(X_model)\n",
    "                        predictions.append(model_pred)\n",
    "                    \n",
    "                    # 应用权重\n",
    "                    weighted_preds = np.zeros(predictions[0].shape)\n",
    "                    for i, preds in enumerate(predictions):\n",
    "                        weighted_preds += self.weights[i] * preds\n",
    "                        \n",
    "                    return weighted_preds\n",
    "            \n",
    "            # 创建模型数据集字典\n",
    "            model_datasets = {\n",
    "                'XGBoost': 'tree',\n",
    "                'LightGBM': 'tree',\n",
    "                'HistGradientBoosting': 'tree',\n",
    "                'RandomForest': 'tree_filled',\n",
    "                'GaussianProcess': 'linear'\n",
    "            }\n",
    "            \n",
    "            # 实例化加权平均集成模型\n",
    "            weighted_model = WeightedAverageEnsemble(\n",
    "                models_dict=models[target],\n",
    "                model_names=available_models,\n",
    "                weights=optimal_weights,\n",
    "                model_datasets=model_datasets\n",
    "            )\n",
    "            \n",
    "            # 保存模型\n",
    "            models[target]['WeightedEnsemble'] = weighted_model\n",
    "            # 使用pickle保存加权平均集成模型\n",
    "            weighted_model_file = os.path.join(model_folder, f'{target}_加权平均集成模型.pkl')\n",
    "            with open(weighted_model_file, 'wb') as f:\n",
    "                pickle.dump(weighted_model, f)\n",
    "            print(f\"加权平均集成模型已保存至 {weighted_model_file}\")\n",
    "\n",
    "            # 保存训练集和测试集的预测结果\n",
    "            train_prediction = pd.DataFrame({\n",
    "                '实际值': y_train[target],\n",
    "                '加权平均预测值': train_weighted_preds,\n",
    "                '误差': np.abs(y_train[target] - train_weighted_preds)\n",
    "            })\n",
    "\n",
    "            test_prediction = pd.DataFrame({\n",
    "                '实际值': y_test[target],\n",
    "                '加权平均预测值': test_weighted_preds,\n",
    "                '误差': np.abs(y_test[target] - test_weighted_preds)\n",
    "            })\n",
    "\n",
    "            # 添加各基础模型的预测结果以便比较\n",
    "            for model_name in available_models:\n",
    "                train_prediction[f'{model_name}预测值'] = train_predictions[model_name]\n",
    "                test_prediction[f'{model_name}预测值'] = test_predictions[model_name]\n",
    "\n",
    "            # 保存到文件\n",
    "            train_file = os.path.join(save_folder, f'{target}_加权平均集成训练集预测结果.csv')\n",
    "            test_file = os.path.join(save_folder, f'{target}_加权平均集成测试集预测结果.csv')\n",
    "\n",
    "            train_prediction.to_csv(train_file, index=False)\n",
    "            test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "            print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "            print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "            # 可视化: 预测vs实际值散点图 (训练集和测试集)\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # 训练集散点图\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(y_train[target], train_weighted_preds, alpha=0.5)\n",
    "            plt.plot([y_train[target].min(), y_train[target].max()], [y_train[target].min(), y_train[target].max()], 'r--')\n",
    "            plt.xlabel('实际值')\n",
    "            plt.ylabel('预测值')\n",
    "            plt.title(f'训练集: R²={train_r2:.4f}')\n",
    "            \n",
    "            # 测试集散点图\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.scatter(y_test[target], test_weighted_preds, alpha=0.5)\n",
    "            plt.plot([y_test[target].min(), y_test[target].max()], [y_test[target].min(), y_test[target].max()], 'r--')\n",
    "            plt.xlabel('实际值')\n",
    "            plt.ylabel('预测值')\n",
    "            plt.title(f'测试集: R²={test_r2:.4f}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 绘制误差分布\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # 训练集误差\n",
    "            plt.subplot(1, 2, 1)\n",
    "            train_errors = y_train[target] - train_weighted_preds\n",
    "            plt.hist(train_errors, bins=30, alpha=0.7)\n",
    "            plt.axvline(x=0, color='r', linestyle='--')\n",
    "            plt.xlabel('预测误差')\n",
    "            plt.ylabel('频次')\n",
    "            plt.title(f'训练集误差分布 (MAE={np.abs(train_errors).mean():.4f})')\n",
    "            \n",
    "            # 测试集误差\n",
    "            plt.subplot(1, 2, 2)\n",
    "            test_errors = y_test[target] - test_weighted_preds\n",
    "            plt.hist(test_errors, bins=30, alpha=0.7)\n",
    "            plt.axvline(x=0, color='r', linestyle='--')\n",
    "            plt.xlabel('预测误差')\n",
    "            plt.ylabel('频次')\n",
    "            plt.title(f'测试集误差分布 (MAE={np.abs(test_errors).mean():.4f})')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 绘制权重条形图\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.bar(available_models, optimal_weights)\n",
    "            plt.xlabel('模型')\n",
    "            plt.ylabel('权重')\n",
    "            plt.title(f'{target} - 加权平均集成模型权重分布')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(True, axis='y')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 绘制各模型与加权平均模型的预测对比图\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            model_data = [test_weighted_preds] + [test_predictions[model] for model in available_models]\n",
    "            model_labels = ['加权平均'] + available_models\n",
    "            \n",
    "            plt.boxplot(model_data, labels=model_labels)\n",
    "            plt.ylabel('预测值')\n",
    "            plt.title('加权平均模型与各基础模型预测分布对比')\n",
    "            plt.grid(True, axis='y')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "            print(\"权重优化失败:\", result.message)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"创建加权平均集成模型失败: {str(e)}\")\n",
    "        print(f\"错误详情: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 吸油能力XGBoost贝叶斯超参数优化\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 创建保存数据的文件夹\n",
    "save_folder = '模型可视化数据'\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "    print(f\"已创建文件夹：{save_folder}\")\n",
    "\n",
    "# 选择目标变量\n",
    "target = '吸油能力'\n",
    "print(f\"训练 {target} 的XGBoost模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.03)\n",
    "\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_tree[target]\n",
    "X_test_model = X_test_tree[target]\n",
    "\n",
    "# 从您的代码中复制的完整函数定义\n",
    "def tolerance_r2_score(y_true, y_pred, tolerance=0.15, target=None):\n",
    "    \"\"\"\n",
    "    计算容忍度R²评分，允许一定误差范围内的预测被视为准确\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    \n",
    "    if target and target in target_tolerance:\n",
    "        tolerance = target_tolerance[target]\n",
    "    \n",
    "    tolerance_values = tolerance * np.abs(y_true)\n",
    "    residuals = np.abs(y_true - y_pred)\n",
    "    adjusted_residuals = np.maximum(0, residuals - tolerance_values)\n",
    "    \n",
    "    y_true_mean = np.mean(y_true)\n",
    "    tss = np.sum((y_true - y_true_mean) ** 2)\n",
    "    rss = np.sum(adjusted_residuals ** 2)\n",
    "    \n",
    "    if tss == 0:\n",
    "        return 0\n",
    "    \n",
    "    tolerance_r2 = 1 - (rss / tss)\n",
    "    return tolerance_r2\n",
    "\n",
    "def prediction_within_tolerance(y_true, y_pred, tolerance=0.15, target=None):\n",
    "    \"\"\"\n",
    "    计算预测值在目标值±容忍范围内的比例\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    if target and target in target_tolerance:\n",
    "        tolerance = target_tolerance[target]\n",
    "    \n",
    "    tolerance_values = tolerance * np.abs(y_true)\n",
    "    within_tolerance = np.abs(y_true - y_pred) <= tolerance_values\n",
    "    \n",
    "    return np.mean(within_tolerance)\n",
    "\n",
    "def make_tolerance_scorer(target_name):\n",
    "    def tolerance_score(y_true, y_pred):\n",
    "        tolerance = target_tolerance.get(target_name, 0.03)\n",
    "        relative_errors = np.abs(y_true - y_pred) / np.abs(y_true)\n",
    "        within_tolerance = np.mean(relative_errors <= tolerance)\n",
    "        return within_tolerance\n",
    "    return tolerance_score\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, target, model_name):\n",
    "    \"\"\"\n",
    "    评估模型在训练集和测试集上的性能，包括标准R²和容忍度R²\n",
    "    \"\"\"\n",
    "    current_tolerance = target_tolerance.get(target, 0.15)\n",
    "    \n",
    "    # 在训练集上评估\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    if len(y_train_pred.shape) > 1 and y_train_pred.shape[1] == 1:\n",
    "        y_train_pred = y_train_pred.flatten()\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    train_tol_r2 = tolerance_r2_score(y_train, y_train_pred, tolerance=current_tolerance, target=target)\n",
    "    train_within_tol = prediction_within_tolerance(y_train, y_train_pred, tolerance=current_tolerance, target=target)\n",
    "    \n",
    "    # 在测试集上评估\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    if len(y_test_pred.shape) > 1 and y_test_pred.shape[1] == 1:\n",
    "        y_test_pred = y_test_pred.flatten()\n",
    "        \n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_tol_r2 = tolerance_r2_score(y_test, y_test_pred, tolerance=current_tolerance, target=target)\n",
    "    test_within_tol = prediction_within_tolerance(y_test, y_test_pred, tolerance=current_tolerance, target=target)\n",
    "    \n",
    "    print(f\"\\n{model_name} 在 {target} 上的评估结果:\")\n",
    "    print(f\"训练集: R²={train_r2:.4f}, 容忍度R²={train_tol_r2:.4f}, 在容忍范围内比例={train_within_tol:.2%}\")\n",
    "    print(f\"测试集: R²={test_r2:.4f}, 容忍度R²={test_tol_r2:.4f}, 在容忍范围内比例={test_within_tol:.2%}\")\n",
    "    \n",
    "    # 绘制预测值与实际值的对比散点图\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 训练集散点图\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_train, y_train_pred, alpha=0.6, s=30)\n",
    "    min_val = min(min(y_train), min(y_train_pred))\n",
    "    max_val = max(max(y_train), max(y_train_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "    plt.xlabel('实际值')\n",
    "    plt.ylabel('预测值')\n",
    "    plt.title(f'训练集: R²={train_r2:.4f}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 测试集散点图\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_test, y_test_pred, alpha=0.6, s=30)\n",
    "    min_val = min(min(y_test), min(y_test_pred))\n",
    "    max_val = max(max(y_test), max(y_test_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "    plt.xlabel('实际值')\n",
    "    plt.ylabel('预测值')\n",
    "    plt.title(f'测试集: R²={test_r2:.4f}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'train_r2': train_r2,\n",
    "        'train_tol_r2': train_tol_r2,\n",
    "        'train_within_tol': train_within_tol,\n",
    "        'test_r2': test_r2,\n",
    "        'test_tol_r2': test_tol_r2,\n",
    "        'test_within_tol': test_within_tol\n",
    "    }\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# 设置基础参数\n",
    "base_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'hist',\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42,\n",
    "    'missing': np.nan\n",
    "}\n",
    "\n",
    "# 定义贝叶斯优化的搜索空间（基于您原始代码的参数范围）\n",
    "dimensions = [\n",
    "    Integer(50, 100, name='n_estimators'),           # 您原始代码：[100, 50, 90, 80]\n",
    "    Real(0.5, 0.8, name='learning_rate'),            # 您原始代码：[0.7, 0.8, 0.6, 0.5]\n",
    "    Integer(3, 6, name='max_depth'),                 # 您原始代码：[4, 5, 6, 3]\n",
    "    Integer(2, 6, name='min_child_weight'),          # 您原始代码：[5, 4, 6, 3, 2]\n",
    "    Real(0.0, 0.2, name='gamma'),                    # 您原始代码：[0, 0.1, 0.2]\n",
    "    Real(0.5, 0.6, name='subsample'),               # 您原始代码：[0.6, 0.5]\n",
    "    Real(0.8, 1.0, name='colsample_bytree'),        # 您原始代码：[0.9, 1.0, 0.8]\n",
    "    Real(0.0, 1.0, name='reg_alpha'),               # 您原始代码：[0, 0.4, 0.5, 0.6, 1.0]\n",
    "    Real(0.5, 1.0, name='reg_lambda')               # 您原始代码：[1.0, 0.5, 0.7, 0.8]\n",
    "]\n",
    "\n",
    "# 交叉验证设置\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 定义目标函数\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def objective(**params):\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=params['max_depth'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        gamma=params['gamma'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        **base_params\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        cv_scores = cross_val_score(\n",
    "            model, X_train_model, y_train[target], \n",
    "            cv=kf, scoring=tol_scorer_wrapped\n",
    "        )\n",
    "        return -cv_scores.mean()\n",
    "    except:\n",
    "        return 1.0\n",
    "\n",
    "# 执行贝叶斯优化\n",
    "print(\"执行贝叶斯优化...\")\n",
    "result = gp_minimize(\n",
    "    func=objective,\n",
    "    dimensions=dimensions,\n",
    "    n_calls=50,\n",
    "    n_initial_points=10,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 获取最佳参数\n",
    "best_params = dict(zip([dim.name for dim in dimensions], result.x))\n",
    "print(f\"最佳参数: {best_params}\")\n",
    "print(f\"最佳CV得分: {-result.fun:.4f}\")\n",
    "\n",
    "# 使用最佳参数创建最终模型\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_child_weight=best_params['min_child_weight'],\n",
    "    gamma=best_params['gamma'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree'],\n",
    "    reg_alpha=best_params['reg_alpha'],\n",
    "    reg_lambda=best_params['reg_lambda'],\n",
    "    **base_params\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_model, y_train[target])\n",
    "\n",
    "# 交叉验证\n",
    "cv_scores = cross_val_score(xgb_model, X_train_model, y_train[target], cv=kf, scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    xgb_model, X_train_model, y_train[target], cv=kf, \n",
    "    scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "models[target]['XGBoost'] = xgb_model\n",
    "\n",
    "# 创建模型保存文件夹\n",
    "model_folder = '训练模型文件'\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    print(f\"已创建模型文件夹：{model_folder}\")\n",
    "\n",
    "xgb_model_file = os.path.join(model_folder, f'{target}_XGBoost模型.pkl')\n",
    "with open(xgb_model_file, 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "print(f\"XGBoost 模型已保存至 {xgb_model_file}\")\n",
    "\n",
    "# 评估模型\n",
    "results = evaluate_model(xgb_model, X_train_model, y_train[target], X_test_model, y_test[target], target, \"XGBoost\")\n",
    "\n",
    "# 获取预测值\n",
    "y_pred_train = xgb_model.predict(X_train_model)\n",
    "y_pred_test = xgb_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到单独的文件\n",
    "train_file = os.path.join(save_folder, f'{target}_XGBoost训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_XGBoost测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "# 特征重要性\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - XGBoost特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_XGBoost特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "\n",
    "# 绘制优化收敛图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_convergence(result)\n",
    "plt.title('贝叶斯优化收敛过程')\n",
    "plt.show()\n",
    "\n",
    "# 保存优化结果\n",
    "optimization_history = pd.DataFrame({\n",
    "    '迭代次数': range(1, len(result.func_vals) + 1),\n",
    "    '目标函数值': result.func_vals,\n",
    "    '最佳目标函数值': [min(result.func_vals[:i+1]) for i in range(len(result.func_vals))]\n",
    "})\n",
    "\n",
    "param_names = [dim.name for dim in dimensions]\n",
    "for i, param_name in enumerate(param_names):\n",
    "    optimization_history[f'参数_{param_name}'] = [x[i] for x in result.x_iters]\n",
    "\n",
    "optimization_history_file = os.path.join(save_folder, f'{target}_贝叶斯优化历史.csv')\n",
    "optimization_history.to_csv(optimization_history_file, index=False)\n",
    "print(f\"优化历史数据已保存至 {optimization_history_file}\")\n",
    "\n",
    "print(f\"\\n贝叶斯优化完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost\n",
    "from xgboost import XGBRegressor\n",
    "import os\n",
    "\n",
    "# 创建保存数据的文件夹\n",
    "save_folder = '模型可视化数据'\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "    print(f\"已创建文件夹：{save_folder}\")\n",
    "# 选择目标变量\n",
    "target = '吸油能力'\n",
    "print(f\"训练 {target} 的XGBoost模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.03)\n",
    "\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_tree[target]\n",
    "X_test_model = X_test_tree[target]\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# 设置基础参数\n",
    "base_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'hist',\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42,\n",
    "    'missing': np.nan\n",
    "}\n",
    "\n",
    "# 设置超参数\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300,500,600,800,1000],\n",
    "    'learning_rate': [0.01, 0.03, 0.05,0.02,0.1,0.5],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.7, 0.8],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "    'reg_alpha': [0, 0.5, 1.0],\n",
    "    'reg_lambda': [1.0, 1.5, 2.0]\n",
    "}\n",
    "\n",
    "# 创建基础模型\n",
    "base_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1.0,\n",
    "    **base_params\n",
    ")\n",
    "\n",
    "# 执行超参数优化\n",
    "print(\"执行超参数优化...\")\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=1000,\n",
    "    cv=kf,\n",
    "    scoring=tol_scorer_wrapped,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(X_train_model, y_train[target])\n",
    "xgb_model = search.best_estimator_\n",
    "print(f\"最佳参数: {search.best_params_}\")\n",
    "print(f\"最佳CV得分: {search.best_score_:.4f}\")\n",
    "# 交叉验证\n",
    "cv_scores = cross_val_score(xgb_model, X_train_model, y_train[target], cv=kf, scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    xgb_model, X_train_model, y_train[target], cv=kf, \n",
    "    scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "# 保存模型\n",
    "models[target]['XGBoost'] = xgb_model\n",
    "# 创建模型保存文件夹\n",
    "model_folder = '训练模型文件'\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    print(f\"已创建模型文件夹：{model_folder}\")\n",
    "\n",
    "xgb_model_file = os.path.join(model_folder, f'{target}_XGBoost模型.pkl')\n",
    "with open(xgb_model_file, 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "print(f\"XGBoost 模型已保存至 {xgb_model_file}\")\n",
    "# 评估模型\n",
    "results = evaluate_model(xgb_model, X_train_model, y_train[target], X_test_model, y_test[target], target, \"XGBoost\")\n",
    "# 获取预测值\n",
    "y_pred_train = xgb_model.predict(X_train_model)\n",
    "y_pred_test = xgb_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到单独的文件\n",
    "train_file = os.path.join(save_folder, f'{target}_XGBoost训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_XGBoost测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "# 特征重要性\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - XGBoost特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_XGBoost特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "\n",
    "# 绘制训练过程中的损失曲线\n",
    "eval_set = [(X_train_model, y_train[target]), (X_test_model, y_test[target])]\n",
    "model_train = XGBRegressor(**{**base_model.get_params(), 'eval_metric': 'rmse'})\n",
    "model_train.fit(X_train_model, y_train[target], eval_set=eval_set, verbose=False)\n",
    "\n",
    "results = model_train.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_axis, results['validation_0']['rmse'], label='训练集')\n",
    "plt.plot(x_axis, results['validation_1']['rmse'], label='测试集')\n",
    "plt.legend()\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('迭代次数')\n",
    "plt.title('XGBoost训练进度')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# 保存训练进度数据\n",
    "training_progress_data = pd.DataFrame({\n",
    "    '迭代次数': x_axis,\n",
    "    '训练集RMSE': results['validation_0']['rmse'],\n",
    "    '测试集RMSE': results['validation_1']['rmse']\n",
    "})\n",
    "training_progress_file = os.path.join(save_folder, f'{target}_XGBoost训练进度.csv')\n",
    "training_progress_data.to_csv(training_progress_file, index=False)\n",
    "print(f\"训练进度数据已保存至 {training_progress_file}\")\n",
    "# 学习率影响分析\n",
    "learning_rates = [0.005, 0.01, 0.03, 0.05, 0.1, 0.2]\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 创建用于保存学习率分析数据的DataFrame\n",
    "lr_analysis_data = pd.DataFrame()\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = XGBRegressor(\n",
    "        learning_rate=lr,\n",
    "        n_estimators=500,\n",
    "        max_depth=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        tree_method='hist',\n",
    "        random_state=42\n",
    "    )\n",
    "    eval_set = [(X_test_model, y_test[target])]\n",
    "    model.fit(X_train_model, y_train[target], eval_set=eval_set, verbose=False)\n",
    "    results = model.evals_result()\n",
    "    \n",
    "    # 将当前学习率的结果添加到DataFrame\n",
    "    temp_df = pd.DataFrame({\n",
    "        '迭代次数': range(len(results['validation_0']['rmse'])),\n",
    "        f'学习率_{lr}': results['validation_0']['rmse']\n",
    "    })\n",
    "    \n",
    "    if lr_analysis_data.empty:\n",
    "        lr_analysis_data = temp_df\n",
    "    else:\n",
    "        lr_analysis_data = pd.merge(\n",
    "            lr_analysis_data, temp_df, on='迭代次数', how='outer'\n",
    "        )\n",
    "    \n",
    "    plt.plot(results['validation_0']['rmse'], label=f'学习率: {lr}')\n",
    "\n",
    "# 保存学习率分析数据\n",
    "lr_analysis_file = os.path.join(save_folder, f'{target}_XGBoost学习率分析.csv')\n",
    "lr_analysis_data.to_csv(lr_analysis_file, index=False)\n",
    "print(f\"学习率分析数据已保存至 {lr_analysis_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# lightGBM\n",
    "print(f\"训练 {target} 的LightGBM模型...\")\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.03)\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_tree[target]\n",
    "X_test_model = X_test_tree[target]\n",
    "# 设置基础参数\n",
    "base_params = {\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'rmse',\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "# 设置特定参数\n",
    "lgb_params = {\n",
    "    'n_estimators': 12000,\n",
    "    'learning_rate': 0.001,\n",
    "    'num_leaves': 20,\n",
    "    'max_depth': 20,#8shi0.72\n",
    "    'min_child_samples': 1,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree':1,\n",
    "    'reg_alpha': 10,\n",
    "    'reg_lambda': 1.0,\n",
    "    **base_params\n",
    "}\n",
    "\n",
    "print(\"使用自定义LightGBM包装器训练模型\")\n",
    "# 确保保存当前使用的特征列\n",
    "feature_cols = X_train_model.columns.tolist() if hasattr(X_train_model, 'columns') else None\n",
    "\n",
    "# 创建并训练模型\n",
    "lgb_model = CustomLGBMRegressor(**lgb_params)\n",
    "lgb_model.fit(X_train_model, y_train[target])\n",
    "\n",
    "# 保存模型\n",
    "models[target]['LightGBM'] = lgb_model\n",
    "# 创建模型保存文件夹（如果已存在则不会重复创建）\n",
    "model_folder = '训练模型文件'\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    print(f\"已创建模型文件夹：{model_folder}\")\n",
    "\n",
    "lgb_model_file = os.path.join(model_folder, f'{target}_LightGBM模型.pkl')\n",
    "with open(lgb_model_file, 'wb') as f:\n",
    "    pickle.dump(lgb_model, f)\n",
    "print(f\"LightGBM 模型已保存至 {lgb_model_file}\")\n",
    "# 评估模型\n",
    "results = evaluate_model(lgb_model, X_train_model, y_train[target], X_test_model, y_test[target], target, \"LightGBM\")\n",
    "\n",
    "print(\"LightGBM模型训练成功\")\n",
    "# 获取预测值\n",
    "y_pred_train = lgb_model.predict(X_train_model)\n",
    "y_pred_test = lgb_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到单独的文件\n",
    "train_file = os.path.join(save_folder, f'{target}_LightGBM训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_LightGBM测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "# 特征重要性可视化\n",
    "if hasattr(lgb_model.model, 'feature_importance') and feature_cols is not None:\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': lgb_model.model.feature_importance(importance_type='gain')\n",
    "    })\n",
    "    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "    plt.xlabel('增益重要性')\n",
    "    plt.ylabel('特征')\n",
    "    plt.title(f'{target} - LightGBM特征重要性')\n",
    "    plt.grid(True, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_LightGBM特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "\n",
    "\n",
    "# 添加预测不准确样本分析\n",
    "# 获取测试集预测值\n",
    "y_pred = lgb_model.predict(X_test_model)\n",
    "\n",
    "# 将y_test转换为numpy数组格式进行处理\n",
    "if hasattr(y_test[target], 'values'):\n",
    "    y_true_values = y_test[target].values\n",
    "else:\n",
    "    y_true_values = y_test[target]\n",
    "\n",
    "# 计算绝对误差\n",
    "errors = np.abs(y_true_values - y_pred)\n",
    "\n",
    "# 设置容忍度阈值\n",
    "tolerance = 5.0  # 可以根据需要调整\n",
    "\n",
    "# 找出误差超过容忍度的样本\n",
    "inaccurate_mask = errors > tolerance\n",
    "inaccurate_indices = np.where(inaccurate_mask)[0]\n",
    "\n",
    "print(f\"\\n预测不准确的样本数量: {len(inaccurate_indices)} (占测试集的 {len(inaccurate_indices)/len(y_test)*100:.2f}%)\")\n",
    "print(f\"使用的容忍度阈值: {tolerance}\")\n",
    "\n",
    "# 创建预测不准确样本的分析数据\n",
    "if len(inaccurate_indices) > 0:\n",
    "    # 尝试获取原始索引，如果不可用则使用数组位置索引\n",
    "    try:\n",
    "        if hasattr(y_test, 'index'):\n",
    "            original_indices = [y_test.index[i] for i in inaccurate_indices]\n",
    "        elif isinstance(X_test_model, pd.DataFrame) and hasattr(X_test_model, 'index'):\n",
    "            original_indices = [X_test_model.index[i] for i in inaccurate_indices]\n",
    "        else:\n",
    "            # 如果无法获取原始索引，使用数组位置作为标识\n",
    "            original_indices = inaccurate_indices\n",
    "    except Exception as e:\n",
    "        print(f\"无法获取原始索引: {str(e)}\")\n",
    "        original_indices = inaccurate_indices\n",
    "    \n",
    "    # 创建包含预测不准确样本信息的DataFrame\n",
    "    inaccurate_samples = []\n",
    "    for i, idx in enumerate(inaccurate_indices):\n",
    "        # 安全地获取实际值\n",
    "        if hasattr(y_test[target], 'iloc'):\n",
    "            actual = y_test[target].iloc[idx]\n",
    "        else:\n",
    "            actual = y_true_values[idx]\n",
    "        \n",
    "        inaccurate_samples.append({\n",
    "            '样本索引': original_indices[i],\n",
    "            '实际值': actual,\n",
    "            '预测值': y_pred[idx],\n",
    "            '绝对误差': errors[idx],\n",
    "            '相对误差(%)': (errors[idx] / np.abs(actual)) * 100 if actual != 0 else float('inf')\n",
    "        })\n",
    "    \n",
    "    inaccurate_df = pd.DataFrame(inaccurate_samples)\n",
    "    # 按误差降序排列\n",
    "    inaccurate_df = inaccurate_df.sort_values('绝对误差', ascending=False)\n",
    "    \n",
    "    # 打印预测不准确的样本信息\n",
    "    print(\"\\n预测不准确的样本详情 (按误差降序排列):\")\n",
    "    print(inaccurate_df)\n",
    "    \n",
    "    # 保存结果到文件\n",
    "    inaccurate_df.to_csv(f'{target}_不准确预测.csv', index=False)\n",
    "else:\n",
    "    print(f\"\\n没有发现预测不准确的样本 (容忍度阈值: {tolerance})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#HistGradientBoosting\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "print(f\"训练 {target} 的HistGradientBoosting模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.03)\n",
    "\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_tree[target]\n",
    "X_test_model = X_test_tree[target]\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# 设置超参数\n",
    "param_dist = {\n",
    "    'max_iter': [450, 400,350,440,460],\n",
    "    'learning_rate': [0.01, 0.008, 0.011,0.009],\n",
    "    'max_depth': [9, 11, 10,8,7,6,5],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'l2_regularization': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# 创建基础模型\n",
    "base_model = HistGradientBoostingRegressor(\n",
    "    max_iter=3000,\n",
    "    learning_rate=1,\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=4,\n",
    "    l2_regularization=0.5,\n",
    "    loss='squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 执行超参数优化\n",
    "if len(X_train_model) >= 90:\n",
    "    print(\"执行超参数优化...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=2500,\n",
    "        cv=kf,\n",
    "        scoring=tol_scorer_wrapped,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train_model, y_train[target])\n",
    "    hgb_model = search.best_estimator_\n",
    "    print(f\"最佳参数: {search.best_params_}\")\n",
    "    print(f\"最佳CV得分: {search.best_score_:.4f}\")\n",
    "else:\n",
    "    # 使用预定义的参数\n",
    "    print(\"使用预定义参数...\")\n",
    "    hgb_model = base_model\n",
    "    hgb_model.fit(X_train_model, y_train[target])\n",
    "\n",
    "# 交叉验证\n",
    "cv_scores = cross_val_score(hgb_model, X_train_model, y_train[target], cv=kf, scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    hgb_model, X_train_model, y_train[target], cv=kf, \n",
    "    scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "models[target]['HistGradientBoosting'] = hgb_model\n",
    "# 使用pickle保存HistGradientBoosting模型\n",
    "hgb_model_file = os.path.join(model_folder, f'{target}_HistGradientBoosting模型.pkl')\n",
    "with open(hgb_model_file, 'wb') as f:\n",
    "    pickle.dump(hgb_model, f)\n",
    "print(f\"HistGradientBoosting模型已保存至 {hgb_model_file}\")\n",
    "# 评估模型\n",
    "results = evaluate_model(hgb_model, X_train_model, y_train[target], X_test_model, y_test[target], target, \"HistGradientBoosting\")\n",
    "# 获取预测值\n",
    "y_pred_train = hgb_model.predict(X_train_model)\n",
    "y_pred_test = hgb_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到文件\n",
    "train_file = os.path.join(save_folder, f'{target}_HistGradientBoosting训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_HistGradientBoosting测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "# 由于HistGradientBoosting不直接提供特征重要性，使用permutation importance评估\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(\n",
    "    hgb_model, X_test_model, y_test[target], \n",
    "    n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('置换重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - HistGradientBoosting特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_HistGradientBoosting特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "# 不同学习率和迭代次数的影响分析\n",
    "learning_rates = [0.01, 0.008, 0.011,0.009]\n",
    "max_iters = [50, 100, 200, 300]\n",
    "fig, axs = plt.subplots(len(learning_rates), 1, figsize=(10, 4*len(learning_rates)), sharex=True)\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    for iter_count in max_iters:\n",
    "        model = HistGradientBoostingRegressor(\n",
    "            max_iter=iter_count,\n",
    "            learning_rate=lr,\n",
    "            max_depth=3,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train_model, y_train[target])\n",
    "        train_score = r2_score(y_train[target], model.predict(X_train_model))\n",
    "        test_score = r2_score(y_test[target], model.predict(X_test_model))\n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "    \n",
    "    axs[i].plot(max_iters, train_scores, 'o-', label='训练集 R²')\n",
    "    axs[i].plot(max_iters, test_scores, 'o-', label='测试集 R²')\n",
    "    axs[i].set_title(f'学习率 = {lr}')\n",
    "    axs[i].set_ylabel('R²')\n",
    "    axs[i].grid(True)\n",
    "    axs[i].legend()\n",
    "plt.xlabel('迭代次数')\n",
    "plt.suptitle('HistGradientBoosting - 学习率和迭代次数影响')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 准备保存学习率和迭代次数影响分析数据\n",
    "analysis_data = []\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    for j, iter_count in enumerate(max_iters):\n",
    "        analysis_data.append({\n",
    "            '学习率': lr,\n",
    "            '迭代次数': iter_count,\n",
    "            '训练集R²': train_scores[j],\n",
    "            '测试集R²': test_scores[j]\n",
    "        })\n",
    "\n",
    "# 转换为DataFrame并保存\n",
    "lr_iter_analysis = pd.DataFrame(analysis_data)\n",
    "lr_analysis_file = os.path.join(save_folder, f'{target}_HistGradientBoosting学习率迭代分析.csv')\n",
    "lr_iter_analysis.to_csv(lr_analysis_file, index=False)\n",
    "print(f\"学习率和迭代次数分析数据已保存至 {lr_analysis_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# 选择目标变量\n",
    "print(f\"训练 {target} 的随机森林模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.03)\n",
    "\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_tree_filled[target]\n",
    "X_test_model = X_test_tree_filled[target]\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# 设置超参数\n",
    "param_dist = {\n",
    "    'n_estimators': [ 900,1000,1200,1100],\n",
    "    'max_depth': [4, 5, 6,8,9,10],\n",
    "    'min_samples_split': [ 5,4,6,7,8],\n",
    "    'min_samples_leaf': [7, 4,3,2,1,5,6],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# 创建基础模型\n",
    "base_model = RandomForestRegressor(\n",
    "    n_estimators=800,\n",
    "    max_depth=None,\n",
    "    min_samples_split=3,\n",
    "    min_samples_leaf=4,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 执行超参数优化\n",
    "if len(X_train_model) >= 90:\n",
    "    print(\"执行超参数优化...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=500,#变大后训练集R方变大但是时间可能很长2000/0.3569；4000/0.2091；3000/0.1221；1000/0.1577\n",
    "        cv=kf,\n",
    "        scoring=tol_scorer_wrapped,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train_model, y_train[target])\n",
    "    rf_model = search.best_estimator_\n",
    "    print(f\"最佳参数: {search.best_params_}\")\n",
    "    print(f\"最佳CV得分: {search.best_score_:.4f}\")\n",
    "else:\n",
    "    # 使用预定义的参数\n",
    "    print(\"使用预定义参数...\")\n",
    "    rf_model = base_model\n",
    "    rf_model.fit(X_train_model, y_train[target])\n",
    "\n",
    "# 交叉验证\n",
    "cv_scores = cross_val_score(rf_model, X_train_model, y_train[target], cv=kf, scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    rf_model, X_train_model, y_train[target], cv=kf, \n",
    "    scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "\n",
    "# 如果模型有oob_score属性，输出oob分数\n",
    "if hasattr(rf_model, 'oob_score_'):\n",
    "    print(f\"袋外评分 (OOB score): {rf_model.oob_score_:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "models[target]['RandomForest'] = rf_model\n",
    "# 使用pickle保存随机森林模型\n",
    "rf_model_file = os.path.join(model_folder, f'{target}_随机森林模型.pkl')\n",
    "with open(rf_model_file, 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "print(f\"随机森林模型已保存至 {rf_model_file}\")\n",
    "# 评估模型\n",
    "results = evaluate_model(rf_model, X_train_model, y_train[target], X_test_model, y_test[target], target, \"RandomForest\")\n",
    "# 获取预测值\n",
    "y_pred_train = rf_model.predict(X_train_model)\n",
    "y_pred_test = rf_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到文件\n",
    "train_file = os.path.join(save_folder, f'{target}_随机森林训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_随机森林测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "# 特征重要性\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - RandomForest特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_随机森林特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "# 不同参数组合的影响\n",
    "n_estimators_range = [10, 50, 100, 200, 300, 400,800,1000]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "oob_scores = []\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=n_est,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=4,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf.fit(X_train_model, y_train[target])\n",
    "    train_scores.append(r2_score(y_train[target], rf.predict(X_train_model)))\n",
    "    test_scores.append(r2_score(y_test[target], rf.predict(X_test_model)))\n",
    "    oob_scores.append(rf.oob_score_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators_range, train_scores, 'o-', label='训练集 R²')\n",
    "plt.plot(n_estimators_range, test_scores, 'o-', label='测试集 R²')\n",
    "plt.plot(n_estimators_range, oob_scores, 'o-', label='OOB R²')\n",
    "plt.xlabel('树的数量')\n",
    "plt.ylabel('R²')\n",
    "plt.title('RandomForest - 树数量对性能的影响')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# 保存树数量影响分析数据\n",
    "trees_analysis_data = pd.DataFrame({\n",
    "    '树的数量': n_estimators_range,\n",
    "    '训练集R²': train_scores,\n",
    "    '测试集R²': test_scores,\n",
    "    '袋外评分': oob_scores\n",
    "})\n",
    "trees_analysis_file = os.path.join(save_folder, f'{target}_随机森林树数量分析.csv')\n",
    "trees_analysis_data.to_csv(trees_analysis_file, index=False)\n",
    "print(f\"树数量影响分析数据已保存至 {trees_analysis_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度神经网络回归模型\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# 选择目标变量\n",
    "print(f\"训练 {target} 的深度神经网络模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 选择适当的数据集\n",
    "X_train_model = X_train_nn[target]\n",
    "X_test_model = X_test_nn[target]\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# 标准化特征数据\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_model)\n",
    "X_test_scaled = scaler.transform(X_test_model)\n",
    "\n",
    "print(f\"训练数据形状: {X_train_scaled.shape}\")\n",
    "print(f\"测试数据形状: {X_test_scaled.shape}\")\n",
    "\n",
    "# 创建神经网络模型的函数\n",
    "def create_nn_model(hidden_layers=[128, 64, 32], dropout_rate=0.3, learning_rate=0.001):\n",
    "    \"\"\"创建深度神经网络模型\"\"\"\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # 输入层\n",
    "    model.add(layers.Dense(hidden_layers[0], \n",
    "                          activation='relu', \n",
    "                          input_shape=(X_train_scaled.shape[1],)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # 隐藏层\n",
    "    for units in hidden_layers[1:]:\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # 输出层\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    # 编译模型\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                 loss='mse', \n",
    "                 metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 包装器类以兼容sklearn接口\n",
    "class KerasRegressorWrapper:\n",
    "    def __init__(self, hidden_layers=[128, 64, 32], dropout_rate=0.3, \n",
    "                 learning_rate=0.001, epochs=100, batch_size=32):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.model = create_nn_model(self.hidden_layers, \n",
    "                                   self.dropout_rate, \n",
    "                                   self.learning_rate)\n",
    "        \n",
    "        # 早停和学习率调度\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        # 训练模型\n",
    "        self.history = self.model.fit(\n",
    "            X, y,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X, verbose=0).flatten()\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'hidden_layers': self.hidden_layers,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "# 设置超参数搜索空间\n",
    "param_dist = {\n",
    "    'hidden_layers': [\n",
    "        [64, 32],\n",
    "        [128, 64],\n",
    "        [128, 64, 32],\n",
    "        [256, 128, 64],\n",
    "        [128, 64, 32, 16],\n",
    "        [256, 128, 64, 32]\n",
    "    ],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4, 0.5],\n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.0005],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [150, 200, 250]\n",
    "}\n",
    "\n",
    "# 创建基础模型\n",
    "base_model = KerasRegressorWrapper(\n",
    "    hidden_layers=[128, 64, 32],\n",
    "    dropout_rate=0.3,\n",
    "    learning_rate=0.001,\n",
    "    epochs=200,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# 执行超参数优化\n",
    "if len(X_train_model) >= 80:\n",
    "    print(\"执行超参数优化...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,  # 由于神经网络训练时间长，减少迭代次数\n",
    "        cv=min(3, cv_folds),\n",
    "        scoring='r2',\n",
    "        n_jobs=1,  # 神经网络使用单进程避免冲突\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train_scaled, y_train[target])\n",
    "    nn_model = search.best_estimator_\n",
    "    print(f\"最佳参数: {search.best_params_}\")\n",
    "    print(f\"最佳CV得分: {search.best_score_:.4f}\")\n",
    "else:\n",
    "    # 使用预定义的参数\n",
    "    print(\"使用预定义参数...\")\n",
    "    nn_model = base_model\n",
    "    nn_model.fit(X_train_scaled, y_train[target])\n",
    "\n",
    "# 交叉验证\n",
    "print(\"执行交叉验证...\")\n",
    "cv_scores = cross_val_score(nn_model, X_train_scaled, y_train[target], \n",
    "                           cv=min(3, cv_folds), scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    nn_model, X_train_scaled, y_train[target], \n",
    "    cv=min(3, cv_folds), scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "\n",
    "# 重新训练最终模型\n",
    "print(\"重新训练最终模型...\")\n",
    "nn_model.fit(X_train_scaled, y_train[target])\n",
    "\n",
    "# 保存模型\n",
    "models[target]['DeepNN'] = {'model': nn_model, 'scaler': scaler}\n",
    "# 保存神经网络模型\n",
    "nn_model_file = os.path.join(model_folder, f'{target}_神经网络模型.pkl')\n",
    "with open(nn_model_file, 'wb') as f:\n",
    "    pickle.dump({'model': nn_model, 'scaler': scaler}, f)\n",
    "print(f\"神经网络模型已保存至 {nn_model_file}\")\n",
    "\n",
    "# 评估模型\n",
    "class NNEvaluationWrapper:\n",
    "    def __init__(self, model, scaler):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "\n",
    "nn_eval_model = NNEvaluationWrapper(nn_model, scaler)\n",
    "results = evaluate_model(nn_eval_model, X_train_model, y_train[target], \n",
    "                        X_test_model, y_test[target], target, \"DeepNN\")\n",
    "\n",
    "# 获取预测值\n",
    "y_pred_train = nn_eval_model.predict(X_train_model)\n",
    "y_pred_test = nn_eval_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到文件\n",
    "train_file = os.path.join(save_folder, f'{target}_神经网络训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_神经网络测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "# 特征重要性（使用置换重要性）\n",
    "print(\"计算特征重要性...\")\n",
    "perm_importance = permutation_importance(\n",
    "    nn_eval_model, X_test_model, y_test[target], \n",
    "    n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('置换重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - DeepNN特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_神经网络特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "\n",
    "# 训练损失可视化\n",
    "if hasattr(nn_model, 'history') and nn_model.history is not None:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(nn_model.history.history['loss'], label='训练损失')\n",
    "    plt.plot(nn_model.history.history['val_loss'], label='验证损失')\n",
    "    plt.title('模型损失')\n",
    "    plt.xlabel('轮次')\n",
    "    plt.ylabel('损失')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(nn_model.history.history['mae'], label='训练MAE')\n",
    "    plt.plot(nn_model.history.history['val_mae'], label='验证MAE')\n",
    "    plt.title('模型MAE')\n",
    "    plt.xlabel('轮次')\n",
    "    plt.ylabel('平均绝对误差')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 不同网络结构的影响分析\n",
    "print(\"分析不同网络结构的影响...\")\n",
    "network_structures = [\n",
    "    [32],\n",
    "    [64, 32],\n",
    "    [128, 64],\n",
    "    [128, 64, 32],\n",
    "    [256, 128, 64],\n",
    "    [128, 64, 32, 16]\n",
    "]\n",
    "\n",
    "structure_train_scores = []\n",
    "structure_test_scores = []\n",
    "structure_names = []\n",
    "\n",
    "for structure in network_structures:\n",
    "    structure_name = '-'.join(map(str, structure))\n",
    "    structure_names.append(structure_name)\n",
    "    print(f\"测试网络结构: {structure_name}\")\n",
    "    \n",
    "    test_nn = KerasRegressorWrapper(\n",
    "        hidden_layers=structure,\n",
    "        dropout_rate=0.3,\n",
    "        learning_rate=0.001,\n",
    "        epochs=100,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    test_nn.fit(X_train_scaled, y_train[target])\n",
    "    \n",
    "    train_pred = test_nn.predict(X_train_scaled)\n",
    "    test_pred = test_nn.predict(X_test_scaled)\n",
    "    \n",
    "    train_r2 = r2_score(y_train[target], train_pred)\n",
    "    test_r2 = r2_score(y_test[target], test_pred)\n",
    "    \n",
    "    structure_train_scores.append(train_r2)\n",
    "    structure_test_scores.append(test_r2)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(structure_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, structure_train_scores, width, label='训练集 R²')\n",
    "plt.bar(x + width/2, structure_test_scores, width, label='测试集 R²')\n",
    "\n",
    "plt.xlabel('网络结构')\n",
    "plt.ylabel('R²')\n",
    "plt.title('DeepNN - 网络结构对性能的影响')\n",
    "plt.xticks(x, structure_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 保存网络结构影响分析数据\n",
    "structure_analysis_data = pd.DataFrame({\n",
    "    '网络结构': structure_names,\n",
    "    '训练集R²': structure_train_scores,\n",
    "    '测试集R²': structure_test_scores\n",
    "})\n",
    "structure_analysis_file = os.path.join(save_folder, f'{target}_神经网络结构分析.csv')\n",
    "structure_analysis_data.to_csv(structure_analysis_file, index=False)\n",
    "print(f\"网络结构影响分析数据已保存至 {structure_analysis_file}\")\n",
    "\n",
    "print(\"神经网络模型训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 支持向量机回归模型\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "\n",
    "# 选择目标变量\n",
    "print(f\"训练 {target} 的支持向量机回归模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 选择适当的数据集 - SVR对特征尺度敏感，使用线性预处理数据\n",
    "X_train_model = X_train_linear[target]\n",
    "X_test_model = X_test_linear[target]\n",
    "\n",
    "# 为当前目标创建特定的评分器\n",
    "tol_scorer = make_tolerance_scorer(target)\n",
    "tol_scorer_wrapped = make_scorer(tol_scorer, greater_is_better=True)\n",
    "\n",
    "# SVR需要标准化处理\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_model)\n",
    "X_test_scaled = scaler.transform(X_test_model)\n",
    "\n",
    "print(f\"训练数据形状: {X_train_scaled.shape}\")\n",
    "print(f\"测试数据形状: {X_test_scaled.shape}\")\n",
    "\n",
    "# 设置超参数搜索空间\n",
    "param_dist = {\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'epsilon': [0.01, 0.1, 0.2, 0.5, 1.0],\n",
    "    'degree': [2, 3, 4]  # 仅对poly核有效\n",
    "}\n",
    "\n",
    "# 创建基础模型\n",
    "base_model = SVR(\n",
    "    kernel='rbf',\n",
    "    C=100,\n",
    "    gamma='scale',\n",
    "    epsilon=0.1,\n",
    "    max_iter=5000\n",
    ")\n",
    "\n",
    "# 执行超参数优化\n",
    "if len(X_train_model) >= 80:\n",
    "    print(\"执行超参数优化...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,\n",
    "        cv=min(5, cv_folds),\n",
    "        scoring=tol_scorer_wrapped,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train_scaled, y_train[target])\n",
    "    svr_model = search.best_estimator_\n",
    "    print(f\"最佳参数: {search.best_params_}\")\n",
    "    print(f\"最佳CV得分: {search.best_score_:.4f}\")\n",
    "    \n",
    "    # 如果找到的最佳核函数是poly，进行更精细的优化\n",
    "    if svr_model.kernel == 'poly':\n",
    "        print(\"对多项式核进行精细优化...\")\n",
    "        poly_param_grid = {\n",
    "            'C': [svr_model.C * 0.5, svr_model.C, svr_model.C * 2],\n",
    "            'gamma': [svr_model.gamma] if isinstance(svr_model.gamma, str) else [svr_model.gamma * 0.5, svr_model.gamma, svr_model.gamma * 2],\n",
    "            'degree': [max(2, svr_model.degree-1), svr_model.degree, svr_model.degree+1],\n",
    "            'epsilon': [svr_model.epsilon * 0.5, svr_model.epsilon, svr_model.epsilon * 2]\n",
    "        }\n",
    "        \n",
    "        fine_search = GridSearchCV(\n",
    "            SVR(kernel='poly'),\n",
    "            poly_param_grid,\n",
    "            cv=min(3, cv_folds),\n",
    "            scoring=tol_scorer_wrapped,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        fine_search.fit(X_train_scaled, y_train[target])\n",
    "        svr_model = fine_search.best_estimator_\n",
    "        print(f\"精细优化后的最佳参数: {fine_search.best_params_}\")\n",
    "        \n",
    "else:\n",
    "    # 使用预定义的参数\n",
    "    print(\"使用预定义参数...\")\n",
    "    svr_model = base_model\n",
    "    svr_model.fit(X_train_scaled, y_train[target])\n",
    "\n",
    "# 交叉验证\n",
    "cv_scores = cross_val_score(svr_model, X_train_scaled, y_train[target], \n",
    "                           cv=min(5, cv_folds), scoring='r2')\n",
    "print(f\"交叉验证R²分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 容忍度R²评分\n",
    "tolerance_cv_scores = cross_val_score(\n",
    "    svr_model, X_train_scaled, y_train[target], \n",
    "    cv=min(5, cv_folds), scoring=tol_scorer_wrapped\n",
    ")\n",
    "print(f\"容忍度R²分数 (容忍度={current_tolerance:.2f}): {tolerance_cv_scores.mean():.4f} ± {tolerance_cv_scores.std():.4f}\")\n",
    "\n",
    "# 输出最终模型信息\n",
    "print(f\"最终模型参数:\")\n",
    "print(f\"  核函数: {svr_model.kernel}\")\n",
    "print(f\"  C参数: {svr_model.C}\")\n",
    "print(f\"  gamma参数: {svr_model.gamma}\")\n",
    "print(f\"  epsilon参数: {svr_model.epsilon}\")\n",
    "if svr_model.kernel == 'poly':\n",
    "    print(f\"  多项式度数: {svr_model.degree}\")\n",
    "print(f\"  支持向量数量: {svr_model.n_support_}\")\n",
    "\n",
    "# 保存模型\n",
    "models[target]['SVR'] = {'model': svr_model, 'scaler': scaler}\n",
    "# 使用pickle保存SVR模型\n",
    "svr_model_file = os.path.join(model_folder, f'{target}_支持向量机模型.pkl')\n",
    "with open(svr_model_file, 'wb') as f:\n",
    "    pickle.dump({'model': svr_model, 'scaler': scaler}, f)\n",
    "print(f\"支持向量机模型已保存至 {svr_model_file}\")\n",
    "\n",
    "# 评估模型 - 创建包装器以处理标准化\n",
    "class SVREvaluationWrapper:\n",
    "    def __init__(self, model, scaler):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "\n",
    "svr_eval_model = SVREvaluationWrapper(svr_model, scaler)\n",
    "results = evaluate_model(svr_eval_model, X_train_model, y_train[target], \n",
    "                        X_test_model, y_test[target], target, \"SVR\")\n",
    "\n",
    "# 获取预测值\n",
    "y_pred_train = svr_eval_model.predict(X_train_model)\n",
    "y_pred_test = svr_eval_model.predict(X_test_model)\n",
    "\n",
    "# 分别创建训练集和测试集的预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "# 保存到文件\n",
    "train_file = os.path.join(save_folder, f'{target}_支持向量机训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_支持向量机测试集预测结果.csv')\n",
    "\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "# 特征重要性（使用置换重要性）\n",
    "print(\"计算特征重要性...\")\n",
    "perm_importance = permutation_importance(\n",
    "    svr_eval_model, X_test_model, y_test[target], \n",
    "    n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('置换重要性')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - SVR特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 保存特征重要性数据\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_支持向量机特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存至 {feature_importance_file}\")\n",
    "\n",
    "# 不同C参数的影响分析\n",
    "print(\"分析不同C参数的影响...\")\n",
    "C_range = [0.1, 1, 10, 100, 1000, 10000]\n",
    "c_train_scores = []\n",
    "c_test_scores = []\n",
    "c_support_vectors = []\n",
    "\n",
    "for C_val in C_range:\n",
    "    print(f\"测试C参数: {C_val}\")\n",
    "    test_svr = SVR(\n",
    "        kernel=svr_model.kernel,\n",
    "        C=C_val,\n",
    "        gamma=svr_model.gamma,\n",
    "        epsilon=svr_model.epsilon,\n",
    "        degree=svr_model.degree if svr_model.kernel == 'poly' else 3,\n",
    "        max_iter=5000\n",
    "    )\n",
    "    \n",
    "    test_svr.fit(X_train_scaled, y_train[target])\n",
    "    \n",
    "    train_pred = test_svr.predict(X_train_scaled)\n",
    "    test_pred = test_svr.predict(X_test_scaled)\n",
    "    \n",
    "    train_r2 = r2_score(y_train[target], train_pred)\n",
    "    test_r2 = r2_score(y_test[target], test_pred)\n",
    "    \n",
    "    c_train_scores.append(train_r2)\n",
    "    c_test_scores.append(test_r2)\n",
    "    c_support_vectors.append(test_svr.n_support_.sum())\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.semilogx(C_range, c_train_scores, 'o-', label='训练集 R²')\n",
    "plt.semilogx(C_range, c_test_scores, 'o-', label='测试集 R²')\n",
    "plt.xlabel('C参数')\n",
    "plt.ylabel('R²')\n",
    "plt.title('SVR - C参数对性能的影响')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.semilogx(C_range, c_support_vectors, 'o-', color='green')\n",
    "plt.xlabel('C参数')\n",
    "plt.ylabel('支持向量数量')\n",
    "plt.title('SVR - C参数对支持向量数量的影响')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 不同核函数的性能比较\n",
    "print(\"比较不同核函数的性能...\")\n",
    "kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "kernel_scores = []\n",
    "kernel_train_scores = []\n",
    "kernel_names = []\n",
    "\n",
    "for kernel in kernels:\n",
    "    print(f\"测试核函数: {kernel}\")\n",
    "    try:\n",
    "        if kernel == 'poly':\n",
    "            test_svr = SVR(kernel=kernel, C=100, gamma='scale', epsilon=0.1, degree=3, max_iter=5000)\n",
    "        else:\n",
    "            test_svr = SVR(kernel=kernel, C=100, gamma='scale', epsilon=0.1, max_iter=5000)\n",
    "        \n",
    "        test_svr.fit(X_train_scaled, y_train[target])\n",
    "        \n",
    "        train_pred = test_svr.predict(X_train_scaled)\n",
    "        test_pred = test_svr.predict(X_test_scaled)\n",
    "        \n",
    "        train_r2 = r2_score(y_train[target], train_pred)\n",
    "        test_r2 = r2_score(y_test[target], test_pred)\n",
    "        \n",
    "        kernel_train_scores.append(train_r2)\n",
    "        kernel_scores.append(test_r2)\n",
    "        kernel_names.append(kernel)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"核函数 {kernel} 训练失败: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(kernel_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, kernel_train_scores, width, label='训练集 R²')\n",
    "plt.bar(x + width/2, kernel_scores, width, label='测试集 R²')\n",
    "\n",
    "plt.xlabel('核函数')\n",
    "plt.ylabel('R²')\n",
    "plt.title('SVR - 不同核函数性能比较')\n",
    "plt.xticks(x, kernel_names)\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 保存C参数影响分析数据\n",
    "c_analysis_data = pd.DataFrame({\n",
    "    'C参数': C_range,\n",
    "    '训练集R²': c_train_scores,\n",
    "    '测试集R²': c_test_scores,\n",
    "    '支持向量数量': c_support_vectors\n",
    "})\n",
    "c_analysis_file = os.path.join(save_folder, f'{target}_支持向量机C参数分析.csv')\n",
    "c_analysis_data.to_csv(c_analysis_file, index=False)\n",
    "print(f\"C参数影响分析数据已保存至 {c_analysis_file}\")\n",
    "\n",
    "# 保存核函数比较数据\n",
    "kernel_analysis_data = pd.DataFrame({\n",
    "    '核函数': kernel_names,\n",
    "    '训练集R²': kernel_train_scores,\n",
    "    '测试集R²': kernel_scores\n",
    "})\n",
    "kernel_analysis_file = os.path.join(save_folder, f'{target}_支持向量机核函数分析.csv')\n",
    "kernel_analysis_data.to_csv(kernel_analysis_file, index=False)\n",
    "print(f\"核函数比较分析数据已保存至 {kernel_analysis_file}\")\n",
    "\n",
    "print(\"支持向量机模型训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C, Matern\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import traceback\n",
    "\n",
    "# 设置Matplotlib正常显示中文\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']  # 'SimHei' 是黑体\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题\n",
    "\n",
    "\n",
    "# --- 代码开始 ---\n",
    "target=\"吸油能力\"\n",
    "print(f\"训练 {target} 的高斯过程回归模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 选择数据集\n",
    "X_train_model = X_train_linear[target]\n",
    "X_test_model = X_test_linear[target]\n",
    "\n",
    "# 步骤1: 对输入特征进行标准化\n",
    "print(\"\\n步骤1: 对输入特征进行标准化...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_model)\n",
    "X_test_scaled = scaler.transform(X_test_model)\n",
    "print(\"特征标准化完成。\")\n",
    "\n",
    "# 步骤2: 扩展核函数库并进行自动化选择与优化 (核心修改)\n",
    "print(\"\\n步骤2: 扩展核函数库，进行更全面的自动化模型选择...\")\n",
    "\n",
    "# 定义一个更丰富的、带优化边界的核函数字典\n",
    "kernels_to_try = {\n",
    "    \"RBF\": \n",
    "        C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) \n",
    "        + WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-10, 1e1)),\n",
    "    \n",
    "    \"Matern (nu=1.5)\": \n",
    "        C(1.0, (1e-3, 1e3)) * Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2), nu=1.5) \n",
    "        + WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-10, 1e1)),\n",
    "        \n",
    "    \"Matern (nu=2.5)\": \n",
    "        C(1.0, (1e-3, 1e3)) * Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2), nu=2.5) \n",
    "        + WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-10, 1e1)),\n",
    "        \n",
    "    \"RationalQuadratic\": \n",
    "        C(1.0, (1e-3, 1e3)) * RationalQuadratic(length_scale=1.0, alpha=0.1, length_scale_bounds=(1e-2, 1e2), alpha_bounds=(1e-2, 1e2)) \n",
    "        + WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-10, 1e1)),\n",
    "\n",
    "    # 复合核示例：RBF + Matern (更复杂的模型，可能需要更多数据来避免过拟合)\n",
    "    \"RBF + Matern\":\n",
    "        C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "        + C(1.0, (1e-3, 1e3)) * Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2), nu=1.5)\n",
    "        + WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-10, 1e1))\n",
    "}\n",
    "\n",
    "best_score = -np.inf\n",
    "best_kernel_name = \"\"\n",
    "best_model_from_cv = None\n",
    "\n",
    "print(\"开始测试多个核函数，优中选优...\")\n",
    "for name, kernel in kernels_to_try.items():\n",
    "    print(f\"  > 正在测试核函数: {name}...\")\n",
    "    gp = GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        n_restarts_optimizer=15, # 保证充分优化\n",
    "        normalize_y=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    try:\n",
    "        # 使用交叉验证来评估当前核函数的性能\n",
    "        score = cross_val_score(gp, X_train_scaled, y_train[target], cv=min(3, cv_folds), scoring='r2').mean()\n",
    "        print(f\"    交叉验证 R² 平均分: {score:.4f}\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_kernel_name = name\n",
    "            # 训练一个模型以备后用\n",
    "            gp.fit(X_train_scaled, y_train[target])\n",
    "            best_model_from_cv = gp\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    核函数 {name} 训练失败: {e}\")\n",
    "        continue\n",
    "\n",
    "if best_model_from_cv is None:\n",
    "    raise RuntimeError(\"所有核函数都训练失败，请检查数据或核函数参数！\")\n",
    "\n",
    "print(f\"\\n[决策] 最佳核函数为: '{best_kernel_name}' (交叉验证最高分: {best_score:.4f})\")\n",
    "\n",
    "# 将选出的最佳模型作为最终模型\n",
    "best_model = best_model_from_cv\n",
    "print(f\"最终选定的模型核函数参数: {best_model.kernel_}\")\n",
    "\n",
    "\n",
    "\n",
    "# 步骤3: 评估并可视化最终模型\n",
    "print(f\"\\n步骤3: 评估并可视化最终模型...\")\n",
    "\n",
    "# 在训练集和测试集上获取预测值\n",
    "y_pred_train = best_model.predict(X_train_scaled)\n",
    "y_pred_test = best_model.predict(X_test_scaled)\n",
    "\n",
    "# 计算最终的R²分数\n",
    "train_r2 = r2_score(y_train[target], y_pred_train)\n",
    "test_r2 = r2_score(y_test[target], y_pred_test)\n",
    "print(f\"训练集最终 R²: {train_r2:.4f}\")\n",
    "print(f\"测试集最终 R²: {test_r2:.4f}\")\n",
    "\n",
    "# 绘制“实际值 vs. 预测值”对比图 (训练集和测试集)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "fig.suptitle(f'\"{target}\" 的模型预测效果 (最佳核: {best_kernel_name})', fontsize=16)\n",
    "\n",
    "# 训练集图\n",
    "axes[0].scatter(y_train[target], y_pred_train, alpha=0.6)\n",
    "min_val_train = min(y_train[target].min(), y_pred_train.min())\n",
    "max_val_train = max(y_train[target].max(), y_pred_train.max())\n",
    "axes[0].plot([min_val_train, max_val_train], [min_val_train, max_val_train], 'r--', lw=2, label='理想情况 (y=x)')\n",
    "axes[0].set_title(f'训练集 (R² = {train_r2:.4f})')\n",
    "axes[0].set_xlabel('实际值')\n",
    "axes[0].set_ylabel('预测值')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "axes[0].axis('equal')\n",
    "\n",
    "# 测试集图\n",
    "axes[1].scatter(y_test[target], y_pred_test, alpha=0.6)\n",
    "min_val_test = min(y_test[target].min(), y_pred_test.min())\n",
    "max_val_test = max(y_test[target].max(), y_pred_test.max())\n",
    "axes[1].plot([min_val_test, max_val_test], [min_val_test, max_val_test], 'r--', lw=2, label='理想情况 (y=x)')\n",
    "axes[1].set_title(f'测试集 (R² = {test_r2:.4f})')\n",
    "axes[1].set_xlabel('实际值')\n",
    "axes[1].set_ylabel('预测值')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "axes[1].axis('equal')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "# 步骤4: 保存模型、结果和特征重要性\n",
    "print(f\"\\n步骤4: 保存模型与分析结果...\")\n",
    "models[target]['GaussianProcess'] = best_model\n",
    "gp_model_file = os.path.join(model_folder, f'{target}_高斯过程模型.pkl')\n",
    "with open(gp_model_file, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"高斯过程回归模型已保存至 {gp_model_file}\")\n",
    "\n",
    "# 创建并保存预测结果DataFrame\n",
    "train_prediction = pd.DataFrame({\n",
    "    '实际值': y_train[target],\n",
    "    '预测值': y_pred_train,\n",
    "    '误差': np.abs(y_train[target] - y_pred_train)\n",
    "})\n",
    "train_prediction['数据集'] = '训练集'\n",
    "\n",
    "test_prediction = pd.DataFrame({\n",
    "    '实际值': y_test[target],\n",
    "    '预测值': y_pred_test,\n",
    "    '误差': np.abs(y_test[target] - y_pred_test)\n",
    "})\n",
    "test_prediction['数据集'] = '测试集'\n",
    "\n",
    "train_file = os.path.join(save_folder, f'{target}_高斯过程训练集预测结果.csv')\n",
    "test_file = os.path.join(save_folder, f'{target}_高斯过程测试集预测结果.csv')\n",
    "train_prediction.to_csv(train_file, index=False)\n",
    "test_prediction.to_csv(test_file, index=False)\n",
    "print(f\"训练集和测试集预测结果已保存。\")\n",
    "\n",
    "\n",
    "# 计算并可视化特征重要性\n",
    "perm_importance = permutation_importance(\n",
    "    best_model, X_test_scaled, y_test[target],\n",
    "    n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_model.columns,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('置换重要性 (Permutation Importance)')\n",
    "plt.ylabel('特征')\n",
    "plt.title(f'{target} - GaussianProcess特征重要性')\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "feature_importance_file = os.path.join(save_folder, f'{target}_高斯过程特征重要性.csv')\n",
    "feature_importance.to_csv(feature_importance_file, index=False)\n",
    "print(f\"特征重要性数据已保存。\")\n",
    "\n",
    "\n",
    "# 步骤5: 分析预测不准确的样本\n",
    "print(f\"\\n步骤5: 分析预测不准确的样本...\")\n",
    "\n",
    "y_true_values = y_test[target].values\n",
    "errors = np.abs(y_true_values - y_pred_test)\n",
    "tolerance = 5.0  # 您可以根据需要调整此阈值\n",
    "\n",
    "inaccurate_mask = errors > tolerance\n",
    "inaccurate_indices = np.where(inaccurate_mask)[0]\n",
    "\n",
    "print(f\"\\n预测不准确的样本数量: {len(inaccurate_indices)} (占测试集的 {len(inaccurate_indices)/len(y_test)*100:.2f}%)，使用的容忍度阈值: {tolerance}\")\n",
    "\n",
    "if len(inaccurate_indices) > 0:\n",
    "    original_indices = [X_test_model.index[i] for i in inaccurate_indices]\n",
    "\n",
    "    inaccurate_samples = []\n",
    "    for i, idx in enumerate(inaccurate_indices):\n",
    "        actual = y_true_values[idx]\n",
    "        prediction = y_pred_test[idx]\n",
    "        inaccurate_samples.append({\n",
    "            '样本索引': original_indices[i],\n",
    "            '实际值': actual,\n",
    "            '预测值': prediction,\n",
    "            '绝对误差': errors[idx],\n",
    "            '相对误差(%)': (errors[idx] / np.abs(actual)) * 100 if actual != 0 else float('inf'),\n",
    "        })\n",
    "    \n",
    "    inaccurate_df = pd.DataFrame(inaccurate_samples).sort_values('绝对误差', ascending=False)\n",
    "    \n",
    "    print(\"\\n预测不准确的样本详情 (按误差降序排列):\")\n",
    "    print(inaccurate_df)\n",
    "    \n",
    "    inaccurate_file_path = os.path.join(save_folder, f'{target}_GP不准确预测.csv')\n",
    "    inaccurate_df.to_csv(inaccurate_file_path, index=False)\n",
    "    print(f\"不准确样本分析已保存至 {inaccurate_file_path}\")\n",
    "else:\n",
    "    print(f\"\\n在容忍度阈值 {tolerance} 内，没有发现预测不准确的样本。\")\n",
    "\n",
    "print(\"\\n高斯过程回归模型训练、评估和分析全部完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接运行的模型加载代码\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 如果需要加载神经网络模型，需要先定义KerasRegressorWrapper类\n",
    "class KerasRegressorWrapper:\n",
    "    def __init__(self, hidden_layers=[128, 64, 32], dropout_rate=0.3, \n",
    "                 learning_rate=0.001, epochs=100, batch_size=32):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X, verbose=0).flatten()\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'hidden_layers': self.hidden_layers,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "# 设置模型文件夹路径\n",
    "model_folder = '训练模型文件'\n",
    "\n",
    "# 初始化models字典\n",
    "if 'models' not in locals():\n",
    "    models = {}\n",
    "target=\"吸油能力\"\n",
    "print(f\"为目标变量 {target} 加载模型...\")\n",
    "\n",
    "# 查找模型文件，排除Keras模型\n",
    "model_files = [f for f in os.listdir(model_folder) \n",
    "              if f.startswith(f'{target}_') and f.endswith('.pkl') \n",
    "              and not f.endswith('_features.pkl')\n",
    "              and 'Ensemble' not in f and '集成' not in f\n",
    "              and 'Keras' not in f and 'Neural' not in f and 'NN' not in f]\n",
    "\n",
    "print(f\"找到 {len(model_files)} 个模型文件: {model_files}\")\n",
    "\n",
    "# 初始化目标变量的模型字典\n",
    "if target not in models:\n",
    "    models[target] = {}\n",
    "\n",
    "# 加载每个模型\n",
    "for model_file in model_files:\n",
    "    # 从文件名提取模型名称\n",
    "    model_name = model_file.replace(f'{target}_', '').replace('模型.pkl', '')\n",
    "    \n",
    "    print(f\"  加载模型: {model_name}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    model_path = os.path.join(model_folder, model_file)\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    models[target][model_name] = model\n",
    "    print(f\"    {model_name} 加载成功\")\n",
    "\n",
    "print(f\"成功加载 {len(models[target])} 个模型\")\n",
    "print(f\"可用模型: {list(models[target].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VotingEnsemble - 基于模型标准R²性能分配权重\n",
    "print(f\"训练 {target} 的VotingEnsemble集成模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 使用已有模型和已知性能 - 无需重新评估\n",
    "base_models = []\n",
    "model_scores = {}  # 存储标准R²分数\n",
    "model_datasets = {}  # 存储每个模型对应的数据集\n",
    "\n",
    "print(\"收集已有模型的性能评估结果...\")\n",
    "# 使用原始训练代码计算的标准R²\n",
    "if 'XGBoost' in models[target]:\n",
    "    # 不重新评估，而是计算一次标准R²\n",
    "    model = models[target]['XGBoost']\n",
    "    y_pred = model.predict(X_test_tree[target])\n",
    "    r2 = r2_score(y_test[target], y_pred)\n",
    "    base_models.append(('xgb', model))\n",
    "    model_scores['xgb'] = r2\n",
    "    model_datasets['xgb'] = 'tree'\n",
    "    print(f\"  XGBoost - R²: {r2:.4f}\")\n",
    "\n",
    "if 'LightGBM' in models[target]:\n",
    "    model = models[target]['LightGBM']\n",
    "    y_pred = model.predict(X_test_tree[target])\n",
    "    r2 = r2_score(y_test[target], y_pred)\n",
    "    base_models.append(('lgb', model))\n",
    "    model_scores['lgb'] = r2\n",
    "    model_datasets['lgb'] = 'tree'\n",
    "    print(f\"  LightGBM - R²: {r2:.4f}\")\n",
    "\n",
    "if 'HistGradientBoosting' in models[target]:\n",
    "    model = models[target]['HistGradientBoosting']\n",
    "    y_pred = model.predict(X_test_tree[target])\n",
    "    r2 = r2_score(y_test[target], y_pred)\n",
    "    base_models.append(('hgb', model))\n",
    "    model_scores['hgb'] = r2\n",
    "    model_datasets['hgb'] = 'tree'\n",
    "    print(f\"  HistGradientBoosting - R²: {r2:.4f}\")\n",
    "\n",
    "if 'RandomForest' in models[target]:\n",
    "    model = models[target]['RandomForest']\n",
    "    y_pred = model.predict(X_test_tree_filled[target])\n",
    "    r2 = r2_score(y_test[target], y_pred)\n",
    "    base_models.append(('rf', model))\n",
    "    model_scores['rf'] = r2\n",
    "    model_datasets['rf'] = 'tree_filled'\n",
    "    print(f\"  RandomForest - R²: {r2:.4f}\")\n",
    "\n",
    "if 'GaussianProcess' in models[target]:\n",
    "    model = models[target]['GaussianProcess']\n",
    "    y_pred = model.predict(X_test_linear[target])\n",
    "    r2 = r2_score(y_test[target], y_pred)\n",
    "    base_models.append(('gp', model))\n",
    "    model_scores['gp'] = r2\n",
    "    model_datasets['gp'] = 'linear'\n",
    "    print(f\"  GaussianProcess - R²: {r2:.4f}\")\n",
    "\n",
    "# 改为基于标准R²性能计算权重，而不是容忍度R²\n",
    "print(\"\\n根据标准R²模型性能分配权重...\")\n",
    "\n",
    "# 基于标准R²计算权重\n",
    "total_score = sum(model_scores.values())\n",
    "if total_score > 0:  # 防止除以零错误\n",
    "    weights = [model_scores[name] / total_score * len(model_scores) for name, _ in base_models]\n",
    "else:\n",
    "    weights = [1.0 for _ in base_models]  # 如果总分为0，则均等分配权重\n",
    "\n",
    "print(\"  基于标准R²分配权重\")\n",
    "\n",
    "# 确保权重至少为0.5，防止某些模型权重过低\n",
    "min_weight = 0.5\n",
    "weights = [max(w, min_weight) for w in weights]\n",
    "\n",
    "# 打印权重\n",
    "for i, (name, _) in enumerate(base_models):\n",
    "    print(f\"  {name} 权重: {weights[i]:.4f}\")\n",
    "\n",
    "# 检查是否有足够的模型可用于集成\n",
    "if len(base_models) >= 2:\n",
    "    try:\n",
    "        # 创建自定义投票回归器的封装，确保使用正确的数据集\n",
    "        class EnhancedVotingRegressor:\n",
    "            def __init__(self, estimators, weights, datasets, target_name):\n",
    "                self.estimators = estimators\n",
    "                self.weights = weights\n",
    "                self.datasets = datasets\n",
    "                self.target_name = target_name\n",
    "                \n",
    "                # 归一化权重\n",
    "                self.weights = np.array(self.weights)\n",
    "                self.weights = self.weights / np.sum(self.weights)\n",
    "                \n",
    "            def predict(self, X):\n",
    "                # 对每个模型获取预测，并根据模型类型使用适当的数据预处理\n",
    "                predictions = []\n",
    "                \n",
    "                for i, (name, model) in enumerate(self.estimators):\n",
    "                    # 选择合适的数据格式\n",
    "                    dataset_type = self.datasets.get(name, 'standard')\n",
    "                    \n",
    "                    if dataset_type == 'tree':\n",
    "                        # 对于支持NaN的树模型，直接使用X\n",
    "                        X_model = X\n",
    "                    elif dataset_type == 'tree_filled':\n",
    "                        # 对于不支持NaN的树模型，需要填充X\n",
    "                        if isinstance(X, pd.DataFrame):\n",
    "                            X_model = X.fillna(0)\n",
    "                        else:\n",
    "                            X_model = X\n",
    "                    elif dataset_type == 'linear':\n",
    "                        # 对于线性模型，使用线性预处理的X\n",
    "                        X_model = X\n",
    "                    else:\n",
    "                        # 默认情况下直接使用X\n",
    "                        X_model = X\n",
    "                    \n",
    "                    # 获取当前模型的预测\n",
    "                    pred = model.predict(X_model)\n",
    "                    predictions.append(pred)\n",
    "                \n",
    "                # 加权平均所有预测\n",
    "                weighted_pred = np.zeros(predictions[0].shape)\n",
    "                for i, pred in enumerate(predictions):\n",
    "                    weighted_pred += self.weights[i] * pred\n",
    "                \n",
    "                return weighted_pred\n",
    "        \n",
    "        # 创建投票集成模型\n",
    "        print(\"创建投票集成模型...\")\n",
    "        voting_model = EnhancedVotingRegressor(\n",
    "            estimators=base_models,\n",
    "            weights=weights,\n",
    "            datasets=model_datasets,\n",
    "            target_name=target\n",
    "        )\n",
    "        \n",
    "        # 获取训练集和测试集预测\n",
    "        train_predictions = {}\n",
    "        test_predictions = {}\n",
    "        \n",
    "        # 获取每个基础模型的预测\n",
    "        for name, model in base_models:\n",
    "            if model_datasets[name] == 'tree':\n",
    "                train_predictions[name] = model.predict(X_train_tree[target])\n",
    "                test_predictions[name] = model.predict(X_test_tree[target])\n",
    "            elif model_datasets[name] == 'tree_filled':\n",
    "                train_predictions[name] = model.predict(X_train_tree_filled[target])\n",
    "                test_predictions[name] = model.predict(X_test_tree_filled[target])\n",
    "            elif model_datasets[name] == 'linear':\n",
    "                train_predictions[name] = model.predict(X_train_linear[target])\n",
    "                test_predictions[name] = model.predict(X_test_linear[target])\n",
    "        \n",
    "        # 计算加权预测\n",
    "        y_train_pred = np.zeros(len(y_train[target]))\n",
    "        y_test_pred = np.zeros(len(y_test[target]))\n",
    "        \n",
    "        for i, (name, _) in enumerate(base_models):\n",
    "            y_train_pred += weights[i] * train_predictions[name]\n",
    "            y_test_pred += weights[i] * test_predictions[name]\n",
    "        \n",
    "        # 归一化权重\n",
    "        total_weight = sum(weights)\n",
    "        y_train_pred /= total_weight\n",
    "        y_test_pred /= total_weight\n",
    "        \n",
    "        # 计算性能指标\n",
    "        train_r2 = r2_score(y_train[target], y_train_pred)\n",
    "        test_r2 = r2_score(y_test[target], y_test_pred)\n",
    "        \n",
    "        train_tol_r2 = tolerance_r2_score(y_train[target], y_train_pred, tolerance=current_tolerance, target=target)\n",
    "        test_tol_r2 = tolerance_r2_score(y_test[target], y_test_pred, tolerance=current_tolerance, target=target)\n",
    "        \n",
    "        train_within_tol = prediction_within_tolerance(y_train[target], y_train_pred, tolerance=current_tolerance, target=target)\n",
    "        test_within_tol = prediction_within_tolerance(y_test[target], y_test_pred, tolerance=current_tolerance, target=target)\n",
    "        # 保存训练集和测试集的预测结果\n",
    "        train_prediction = pd.DataFrame({\n",
    "            '实际值': y_train[target],\n",
    "            '集成预测值': y_train_pred,\n",
    "            '误差': np.abs(y_train[target] - y_train_pred)\n",
    "        })\n",
    "\n",
    "        test_prediction = pd.DataFrame({\n",
    "            '实际值': y_test[target],\n",
    "            '集成预测值': y_test_pred,\n",
    "            '误差': np.abs(y_test[target] - y_test_pred)\n",
    "        })\n",
    "\n",
    "        # 添加各基础模型的预测结果\n",
    "        for name, _ in base_models:\n",
    "            train_prediction[f'{name}预测值'] = train_predictions[name]\n",
    "            test_prediction[f'{name}预测值'] = test_predictions[name]\n",
    "\n",
    "        # 保存到文件\n",
    "        train_file = os.path.join(save_folder, f'{target}_投票集成模型训练集预测结果.csv')\n",
    "        test_file = os.path.join(save_folder, f'{target}_投票集成模型测试集预测结果.csv')\n",
    "\n",
    "        train_prediction.to_csv(train_file, index=False)\n",
    "        test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "        print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "        print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "        # 输出性能指标\n",
    "        print(f\"\\n投票集成模型性能:\")\n",
    "        print(f\"训练集: R²={train_r2:.4f}, 容忍度R²={train_tol_r2:.4f}, 在容忍范围内比例={train_within_tol:.2%}\")\n",
    "        print(f\"测试集: R²={test_r2:.4f}, 容忍度R²={test_tol_r2:.4f}, 在容忍范围内比例={test_within_tol:.2%}\")\n",
    "        \n",
    "        # 与各个基础模型比较性能\n",
    "        print(\"\\n与各基础模型性能比较:\")\n",
    "        for name, _ in base_models:\n",
    "            base_train_pred = train_predictions[name]\n",
    "            base_test_pred = test_predictions[name]\n",
    "            \n",
    "            base_train_r2 = r2_score(y_train[target], base_train_pred)\n",
    "            base_test_r2 = r2_score(y_test[target], base_test_pred)\n",
    "            \n",
    "            print(f\"  vs {name}:\")\n",
    "            print(f\"    训练集R²: {train_r2:.4f} vs {base_train_r2:.4f} (差异: {train_r2-base_train_r2:.4f})\")\n",
    "            print(f\"    测试集R²: {test_r2:.4f} vs {base_test_r2:.4f} (差异: {test_r2-base_test_r2:.4f})\")\n",
    "        \n",
    "        # 保存模型\n",
    "        models[target]['VotingEnsemble'] = voting_model\n",
    "        # 使用pickle保存投票集成模型\n",
    "        ensemble_model_file = os.path.join(model_folder, f'{target}_投票集成模型.pkl')\n",
    "        with open(ensemble_model_file, 'wb') as f:\n",
    "            pickle.dump(voting_model, f)\n",
    "        print(f\"投票集成模型已保存至 {ensemble_model_file}\")\n",
    "        # 可视化: 预测vs实际值散点图 (训练集和测试集)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 训练集散点图\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(y_train[target], y_train_pred, alpha=0.5)\n",
    "        plt.plot([y_train[target].min(), y_train[target].max()], [y_train[target].min(), y_train[target].max()], 'r--')\n",
    "        plt.xlabel('实际值')\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title(f'训练集: R²={train_r2:.4f}')\n",
    "        \n",
    "        # 测试集散点图\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(y_test[target], y_test_pred, alpha=0.5)\n",
    "        plt.plot([y_test[target].min(), y_test[target].max()], [y_test[target].min(), y_test[target].max()], 'r--')\n",
    "        plt.xlabel('实际值')\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title(f'测试集: R²={test_r2:.4f}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 绘制误差分布\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 训练集误差\n",
    "        plt.subplot(1, 2, 1)\n",
    "        train_errors = y_train[target] - y_train_pred\n",
    "        plt.hist(train_errors, bins=30, alpha=0.7)\n",
    "        plt.axvline(x=0, color='r', linestyle='--')\n",
    "        plt.xlabel('预测误差')\n",
    "        plt.ylabel('频次')\n",
    "        plt.title(f'训练集误差分布 (MAE={np.abs(train_errors).mean():.4f})')\n",
    "        \n",
    "        # 测试集误差\n",
    "        plt.subplot(1, 2, 2)\n",
    "        test_errors = y_test[target] - y_test_pred\n",
    "        plt.hist(test_errors, bins=30, alpha=0.7)\n",
    "        plt.axvline(x=0, color='r', linestyle='--')\n",
    "        plt.xlabel('预测误差')\n",
    "        plt.ylabel('频次')\n",
    "        plt.title(f'测试集误差分布 (MAE={np.abs(test_errors).mean():.4f})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 绘制权重分布\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        model_names = [name for name, _ in base_models]\n",
    "        plt.bar(model_names, weights)\n",
    "        plt.xlabel('模型')\n",
    "        plt.ylabel('权重')\n",
    "        plt.title(f'{target} - 投票集成模型权重分布')\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 对比各模型预测分布\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        model_preds = [y_test_pred] + [test_predictions[name] for name, _ in base_models]\n",
    "        model_labels = ['Voting'] + [name for name, _ in base_models]\n",
    "        \n",
    "        plt.boxplot(model_preds, labels=model_labels)\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title('投票集成模型与各基础模型预测分布对比')\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"创建投票集成模型失败: {str(e)}\")\n",
    "        print(f\"错误详情: {traceback.format_exc()}\")\n",
    "else:\n",
    "    print(\"没有足够的基础模型来创建投票集成\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 自适应集成模型 - 根据样本特征动态选择最佳模型\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(f\"训练 {target} 的自适应集成模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 创建有效模型列表及其对应的数据集\n",
    "available_models = []\n",
    "model_input_data = {}\n",
    "\n",
    "if 'XGBoost' in models[target]:\n",
    "    available_models.append('XGBoost')\n",
    "    model_input_data['XGBoost'] = {\n",
    "        'train': X_train_tree[target],\n",
    "        'test': X_test_tree[target]\n",
    "    }\n",
    "\n",
    "if 'LightGBM' in models[target]:\n",
    "    available_models.append('LightGBM')\n",
    "    model_input_data['LightGBM'] = {\n",
    "        'train': X_train_tree[target],\n",
    "        'test': X_test_tree[target]\n",
    "    }\n",
    "    \n",
    "if 'HistGradientBoosting' in models[target]:\n",
    "    available_models.append('HistGradientBoosting')\n",
    "    model_input_data['HistGradientBoosting'] = {\n",
    "        'train': X_train_tree[target],\n",
    "        'test': X_test_tree[target]\n",
    "    }\n",
    "    \n",
    "if 'RandomForest' in models[target]:\n",
    "    available_models.append('RandomForest')\n",
    "    model_input_data['RandomForest'] = {\n",
    "        'train': X_train_tree_filled[target],\n",
    "        'test': X_test_tree_filled[target]\n",
    "    }\n",
    "    \n",
    "if 'GaussianProcess' in models[target]:\n",
    "    available_models.append('GaussianProcess')\n",
    "    model_input_data['GaussianProcess'] = {\n",
    "        'train': X_train_linear[target],\n",
    "        'test': X_test_linear[target]\n",
    "    }\n",
    "\n",
    "print(f\"可用模型: {available_models}\")\n",
    "\n",
    "if len(available_models) < 2:\n",
    "    print(\"自适应集成至少需要两个模型，目前可用模型不足\")\n",
    "else:\n",
    "    try:\n",
    "        # 步骤1: 为每个样本生成各模型的预测\n",
    "        print(\"为每个样本生成所有模型的预测...\")\n",
    "        train_predictions = {}\n",
    "        test_predictions = {}\n",
    "        \n",
    "        for model_name in available_models:\n",
    "            model = models[target][model_name]\n",
    "            # 使用适当的数据集进行预测\n",
    "            train_data = model_input_data[model_name]['train']\n",
    "            test_data = model_input_data[model_name]['test']\n",
    "            \n",
    "            train_pred = model.predict(train_data)\n",
    "            test_pred = model.predict(test_data)\n",
    "            \n",
    "            train_predictions[model_name] = train_pred\n",
    "            test_predictions[model_name] = test_pred\n",
    "        \n",
    "        # 步骤2: 计算每个样本的每个模型预测误差\n",
    "        print(\"计算各模型在每个样本上的预测误差...\")\n",
    "        train_errors = {}\n",
    "        for model_name in available_models:\n",
    "            pred = train_predictions[model_name]\n",
    "            error = np.abs(y_train[target].values - pred)\n",
    "            train_errors[model_name] = error\n",
    "        \n",
    "        # 步骤3: 创建一个元模型，学习如何根据特征选择最佳模型\n",
    "        print(\"训练元模型来决定每个样本应使用哪个模型...\")\n",
    "        \n",
    "        # 为每个样本找出表现最好的模型\n",
    "        best_model_indices = np.zeros(len(y_train[target]), dtype=int)\n",
    "        model_name_to_idx = {name: idx for idx, name in enumerate(available_models)}\n",
    "        \n",
    "        for i in range(len(y_train[target])):\n",
    "            model_errors = [train_errors[model_name][i] for model_name in available_models]\n",
    "            best_model_idx = np.argmin(model_errors)\n",
    "            best_model_indices[i] = best_model_idx\n",
    "        \n",
    "        # 用原始特征训练一个分类器来预测最佳模型\n",
    "        meta_classifier = RandomForestClassifier(\n",
    "            n_estimators=200, \n",
    "            max_depth=4,\n",
    "            min_samples_split=2,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        meta_classifier.fit(X_train[target], best_model_indices)\n",
    "        \n",
    "        # 步骤4: 在训练集和测试集上使用元模型选择最佳模型\n",
    "        print(\"在训练集和测试集上应用元模型...\")\n",
    "        train_best_models = meta_classifier.predict(X_train[target])\n",
    "        test_best_models = meta_classifier.predict(X_test[target])\n",
    "        \n",
    "        # 步骤5: 根据元模型的选择，为每个样本选择相应的预测\n",
    "        train_adaptive_predictions = np.zeros(len(y_train[target]))\n",
    "        test_adaptive_predictions = np.zeros(len(y_test[target]))\n",
    "        \n",
    "        # 为训练集计算自适应预测\n",
    "        for i in range(len(y_train[target])):\n",
    "            selected_model = available_models[train_best_models[i]]\n",
    "            train_adaptive_predictions[i] = train_predictions[selected_model][i]\n",
    "        \n",
    "        # 为测试集计算自适应预测\n",
    "        for i in range(len(y_test[target])):\n",
    "            selected_model = available_models[test_best_models[i]]\n",
    "            test_adaptive_predictions[i] = test_predictions[selected_model][i]\n",
    "        \n",
    "        # 步骤6: 评估自适应集成的性能\n",
    "        train_r2 = r2_score(y_train[target], train_adaptive_predictions)\n",
    "        train_tol_r2 = tolerance_r2_score(y_train[target], train_adaptive_predictions, tolerance=current_tolerance, target=target)\n",
    "        train_within_tol = prediction_within_tolerance(y_train[target], train_adaptive_predictions, tolerance=current_tolerance, target=target)\n",
    "        \n",
    "        test_r2 = r2_score(y_test[target], test_adaptive_predictions)\n",
    "        test_tol_r2 = tolerance_r2_score(y_test[target], test_adaptive_predictions, tolerance=current_tolerance, target=target)\n",
    "        test_within_tol = prediction_within_tolerance(y_test[target], test_adaptive_predictions, tolerance=current_tolerance, target=target)\n",
    "        \n",
    "        print(f\"\\n自适应集成模型性能:\")\n",
    "        print(f\"训练集: R²={train_r2:.4f}, 容忍度R²={train_tol_r2:.4f}, 在容忍范围内比例={train_within_tol:.2%}\")\n",
    "        print(f\"测试集: R²={test_r2:.4f}, 容忍度R²={test_tol_r2:.4f}, 在容忍范围内比例={test_within_tol:.2%}\")\n",
    "        \n",
    "        # 步骤7: 比较自适应集成与各个基础模型的性能\n",
    "        print(\"\\n与各基础模型性能比较:\")\n",
    "        for model_name in available_models:\n",
    "            model_train_pred = train_predictions[model_name]\n",
    "            model_test_pred = test_predictions[model_name]\n",
    "            \n",
    "            model_train_r2 = r2_score(y_train[target], model_train_pred)\n",
    "            model_test_r2 = r2_score(y_test[target], model_test_pred)\n",
    "            \n",
    "            train_r2_diff = train_r2 - model_train_r2\n",
    "            test_r2_diff = test_r2 - model_test_r2\n",
    "            \n",
    "            print(f\"  vs {model_name}:\")\n",
    "            print(f\"    训练集R²: {train_r2:.4f} vs {model_train_r2:.4f} (差异: {train_r2_diff:.4f})\")\n",
    "            print(f\"    测试集R²: {test_r2:.4f} vs {model_test_r2:.4f} (差异: {test_r2_diff:.4f})\")\n",
    "        \n",
    "        # 步骤8: 分析各模型被选择的频率\n",
    "        train_model_selection_counts = np.bincount(train_best_models, minlength=len(available_models))\n",
    "        train_model_selection_percent = train_model_selection_counts / len(train_best_models) * 100\n",
    "        \n",
    "        test_model_selection_counts = np.bincount(test_best_models, minlength=len(available_models))\n",
    "        test_model_selection_percent = test_model_selection_counts / len(test_best_models) * 100\n",
    "        \n",
    "        print(\"\\n各模型在训练集上的选择频率:\")\n",
    "        for i, model_name in enumerate(available_models):\n",
    "            print(f\"  {model_name}: {train_model_selection_counts[i]} 次 ({train_model_selection_percent[i]:.2f}%)\")\n",
    "        \n",
    "        print(\"\\n各模型在测试集上的选择频率:\")\n",
    "        for i, model_name in enumerate(available_models):\n",
    "            print(f\"  {model_name}: {test_model_selection_counts[i]} 次 ({test_model_selection_percent[i]:.2f}%)\")\n",
    "        \n",
    "        # 步骤9: 创建并保存自适应集成模型\n",
    "        class AdaptiveEnsembleModel:\n",
    "            def __init__(self, meta_classifier, models_dict, available_models, model_input_data):\n",
    "                self.meta_classifier = meta_classifier\n",
    "                self.models_dict = models_dict\n",
    "                self.available_models = available_models\n",
    "                self.model_input_data = model_input_data\n",
    "                \n",
    "                # 添加数据类型映射\n",
    "                self.data_type_map = {\n",
    "                    'XGBoost': 'tree',\n",
    "                    'LightGBM': 'tree',\n",
    "                    'HistGradientBoosting': 'tree',\n",
    "                    'RandomForest': 'tree_filled',\n",
    "                    'GaussianProcess': 'linear'\n",
    "                }\n",
    "                \n",
    "            def predict(self, X):\n",
    "                # 确保X是DataFrame格式，保持列名\n",
    "                if not isinstance(X, pd.DataFrame):\n",
    "                    if hasattr(X, 'shape') and len(X.shape) == 2:\n",
    "                        if hasattr(X_train[target], 'columns'):\n",
    "                            X = pd.DataFrame(X, columns=X_train[target].columns)\n",
    "                        else:\n",
    "                            X = pd.DataFrame(X)\n",
    "                \n",
    "                # 首先预测每个样本应使用哪个模型\n",
    "                model_choices = self.meta_classifier.predict(X)\n",
    "                \n",
    "                # 初始化预测结果数组\n",
    "                predictions = np.zeros(len(X))\n",
    "                \n",
    "                # 为每个样本获取相应模型的预测\n",
    "                for i in range(len(X)):\n",
    "                    # 获取为当前样本选择的模型\n",
    "                    model_idx = model_choices[i]\n",
    "                    model_name = self.available_models[model_idx]\n",
    "                    model = self.models_dict[model_name]\n",
    "                    \n",
    "                    # 准备单个样本的数据\n",
    "                    if isinstance(X, pd.DataFrame):\n",
    "                        x_sample = X.iloc[[i]]\n",
    "                    else:\n",
    "                        if len(X.shape) == 1:\n",
    "                            x_sample = X.reshape(1, -1)\n",
    "                        else:\n",
    "                            x_sample = X[[i]]\n",
    "                    \n",
    "                    # 根据模型类型进行预处理\n",
    "                    data_type = self.data_type_map.get(model_name, 'standard')\n",
    "                    \n",
    "                    if data_type == 'tree':\n",
    "                        # 支持NaN值的树模型，不需要特殊处理\n",
    "                        x_processed = x_sample\n",
    "                    elif data_type == 'tree_filled':\n",
    "                        # 不支持NaN的树模型，需要填充\n",
    "                        if isinstance(x_sample, pd.DataFrame):\n",
    "                            x_processed = x_sample.fillna(0)\n",
    "                        else:\n",
    "                            x_processed = np.nan_to_num(x_sample, 0)\n",
    "                    elif data_type == 'linear':\n",
    "                        # 线性模型的特殊处理，如果有需要\n",
    "                        x_processed = x_sample\n",
    "                    else:\n",
    "                        # 默认情况\n",
    "                        x_processed = x_sample\n",
    "                    \n",
    "                    # 获取预测\n",
    "                    pred = model.predict(x_processed)\n",
    "                    predictions[i] = pred[0] if hasattr(pred, '__len__') else pred\n",
    "                \n",
    "                return predictions\n",
    "                \n",
    "            def get_feature_importances(self):\n",
    "                # 获取元分类器的特征重要性\n",
    "                if hasattr(self.meta_classifier, 'feature_importances_'):\n",
    "                    return self.meta_classifier.feature_importances_\n",
    "                return None\n",
    "        \n",
    "        # 创建自适应集成模型实例\n",
    "        adaptive_model = AdaptiveEnsembleModel(\n",
    "            meta_classifier=meta_classifier,\n",
    "            models_dict=models[target],\n",
    "            available_models=available_models,\n",
    "            model_input_data=model_input_data\n",
    "        )\n",
    "        \n",
    "        # 保存模型\n",
    "        models[target]['AdaptiveEnsemble'] = adaptive_model\n",
    "        # 使用pickle保存自适应集成模型\n",
    "        adaptive_model_file = os.path.join(model_folder, f'{target}_自适应集成模型.pkl')\n",
    "        with open(adaptive_model_file, 'wb') as f:\n",
    "            pickle.dump(adaptive_model, f)\n",
    "        print(f\"自适应集成模型已保存至 {adaptive_model_file}\")\n",
    "\n",
    "        # 保存训练集和测试集的预测结果\n",
    "        train_prediction = pd.DataFrame({\n",
    "            '实际值': y_train[target],\n",
    "            '自适应集成预测值': train_adaptive_predictions,\n",
    "            '误差': np.abs(y_train[target] - train_adaptive_predictions)\n",
    "        })\n",
    "\n",
    "        test_prediction = pd.DataFrame({\n",
    "            '实际值': y_test[target],\n",
    "            '自适应集成预测值': test_adaptive_predictions,\n",
    "            '误差': np.abs(y_test[target] - test_adaptive_predictions)\n",
    "        })\n",
    "\n",
    "        # 添加各基础模型的预测结果以便比较\n",
    "        for model_name in available_models:\n",
    "            train_prediction[f'{model_name}预测值'] = train_predictions[model_name]\n",
    "            test_prediction[f'{model_name}预测值'] = test_predictions[model_name]\n",
    "\n",
    "        # 保存到文件\n",
    "        train_file = os.path.join(save_folder, f'{target}_自适应集成训练集预测结果.csv')\n",
    "        test_file = os.path.join(save_folder, f'{target}_自适应集成测试集预测结果.csv')\n",
    "\n",
    "        train_prediction.to_csv(train_file, index=False)\n",
    "        test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "        print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "        print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "        # 可视化: 预测vs实际值散点图 (训练集和测试集)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 训练集散点图\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(y_train[target], train_adaptive_predictions, alpha=0.5)\n",
    "        plt.plot([y_train[target].min(), y_train[target].max()], [y_train[target].min(), y_train[target].max()], 'r--')\n",
    "        plt.xlabel('实际值')\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title(f'训练集: R²={train_r2:.4f}')\n",
    "        \n",
    "        # 测试集散点图\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(y_test[target], test_adaptive_predictions, alpha=0.5)\n",
    "        plt.plot([y_test[target].min(), y_test[target].max()], [y_test[target].min(), y_test[target].max()], 'r--')\n",
    "        plt.xlabel('实际值')\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title(f'测试集: R²={test_r2:.4f}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 绘制误差分布\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 训练集误差\n",
    "        plt.subplot(1, 2, 1)\n",
    "        train_errors_plot = y_train[target] - train_adaptive_predictions\n",
    "        plt.hist(train_errors_plot, bins=30, alpha=0.7)\n",
    "        plt.axvline(x=0, color='r', linestyle='--')\n",
    "        plt.xlabel('预测误差')\n",
    "        plt.ylabel('频次')\n",
    "        plt.title(f'训练集误差分布 (MAE={np.abs(train_errors_plot).mean():.4f})')\n",
    "        \n",
    "        # 测试集误差\n",
    "        plt.subplot(1, 2, 2)\n",
    "        test_errors_plot = y_test[target] - test_adaptive_predictions\n",
    "        plt.hist(test_errors_plot, bins=30, alpha=0.7)\n",
    "        plt.axvline(x=0, color='r', linestyle='--')\n",
    "        plt.xlabel('预测误差')\n",
    "        plt.ylabel('频次')\n",
    "        plt.title(f'测试集误差分布 (MAE={np.abs(test_errors_plot).mean():.4f})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 保存元分类器的特征重要性\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': X_train[target].columns,\n",
    "            'Importance': meta_classifier.feature_importances_\n",
    "        })\n",
    "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "        # 保存特征重要性数据\n",
    "        importance_file = os.path.join(save_folder, f'{target}_自适应集成特征重要性.csv')\n",
    "        feature_importance.to_csv(importance_file, index=False)\n",
    "        print(f\"特征重要性数据已保存至 {importance_file}\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "        plt.xlabel('重要性')\n",
    "        plt.ylabel('特征')\n",
    "        plt.title(f'{target} - 自适应集成模型选择特征重要性')\n",
    "        plt.grid(True, axis='x')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 绘制模型选择频率饼图\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 训练集上的模型选择频率\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.pie(train_model_selection_counts, labels=available_models, autopct='%1.1f%%')\n",
    "        plt.title(f'训练集 - 模型选择频率')\n",
    "        \n",
    "        # 测试集上的模型选择频率\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.pie(test_model_selection_counts, labels=available_models, autopct='%1.1f%%')\n",
    "        plt.title(f'测试集 - 模型选择频率')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        # 保存模型选择频率数据\n",
    "        model_selection_data = pd.DataFrame({\n",
    "            '模型': available_models,\n",
    "            '训练集选择次数': train_model_selection_counts,\n",
    "            '训练集选择百分比': train_model_selection_percent,\n",
    "            '测试集选择次数': test_model_selection_counts,\n",
    "            '测试集选择百分比': test_model_selection_percent\n",
    "        })\n",
    "\n",
    "        selection_file = os.path.join(save_folder, f'{target}_自适应集成模型选择频率.csv')\n",
    "        model_selection_data.to_csv(selection_file, index=False)\n",
    "        print(f\"模型选择频率数据已保存至 {selection_file}\")\n",
    "        # 绘制误差分布与模型选择关系\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # 对比测试集上各模型的预测结果\n",
    "        model_data = [test_adaptive_predictions] + [test_predictions[model] for model in available_models]\n",
    "        model_labels = ['自适应集成'] + available_models\n",
    "        \n",
    "        plt.boxplot(model_data, labels=model_labels)\n",
    "        plt.ylabel('预测值')\n",
    "        plt.title('自适应集成模型与各基础模型预测分布对比')\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"创建自适应集成模型失败: {str(e)}\")\n",
    "        print(f\"错误详情: {traceback.format_exc()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加权平均集成模型 - 使用标准R²优化权重\n",
    "print(f\"训练 {target} 的加权平均集成模型...\")\n",
    "\n",
    "# 获取目标特定的容忍度\n",
    "current_tolerance = target_tolerance.get(target, 0.15)\n",
    "\n",
    "# 创建有效模型列表及其预测结果\n",
    "available_models = []\n",
    "train_predictions = {}\n",
    "test_predictions = {}\n",
    "\n",
    "if 'XGBoost' in models[target]:\n",
    "    model = models[target]['XGBoost']\n",
    "    train_pred = model.predict(X_train_tree[target])\n",
    "    test_pred = model.predict(X_test_tree[target])\n",
    "    available_models.append('XGBoost')\n",
    "    train_predictions['XGBoost'] = train_pred\n",
    "    test_predictions['XGBoost'] = test_pred\n",
    "\n",
    "if 'LightGBM' in models[target]:\n",
    "    model = models[target]['LightGBM']\n",
    "    train_pred = model.predict(X_train_tree[target])\n",
    "    test_pred = model.predict(X_test_tree[target])\n",
    "    available_models.append('LightGBM')\n",
    "    train_predictions['LightGBM'] = train_pred\n",
    "    test_predictions['LightGBM'] = test_pred\n",
    "    \n",
    "if 'HistGradientBoosting' in models[target]:\n",
    "    model = models[target]['HistGradientBoosting']\n",
    "    train_pred = model.predict(X_train_tree[target])\n",
    "    test_pred = model.predict(X_test_tree[target])\n",
    "    available_models.append('HistGradientBoosting')\n",
    "    train_predictions['HistGradientBoosting'] = train_pred\n",
    "    test_predictions['HistGradientBoosting'] = test_pred\n",
    "    \n",
    "if 'RandomForest' in models[target]:\n",
    "    model = models[target]['RandomForest']\n",
    "    train_pred = model.predict(X_train_tree_filled[target])\n",
    "    test_pred = model.predict(X_test_tree_filled[target])\n",
    "    available_models.append('RandomForest')\n",
    "    train_predictions['RandomForest'] = train_pred\n",
    "    test_predictions['RandomForest'] = test_pred\n",
    "    \n",
    "if 'GaussianProcess' in models[target]:\n",
    "    model = models[target]['GaussianProcess']\n",
    "    train_pred = model.predict(X_train_linear[target])\n",
    "    test_pred = model.predict(X_test_linear[target])\n",
    "    available_models.append('GaussianProcess')\n",
    "    train_predictions['GaussianProcess'] = train_pred\n",
    "    test_predictions['GaussianProcess'] = test_pred\n",
    "\n",
    "print(f\"可用模型: {available_models}\")\n",
    "\n",
    "if len(available_models) < 2:\n",
    "    print(\"加权平均集成至少需要两个模型，目前可用模型不足\")\n",
    "else:\n",
    "    try:\n",
    "        # 通过优化找到最优权重\n",
    "        print(\"寻找最优权重组合...\")\n",
    "        from scipy.optimize import minimize\n",
    "        \n",
    "        # 定义自定义加权平均函数\n",
    "        def weighted_prediction(weights, preds_list):\n",
    "            weighted_preds = np.zeros(preds_list[0].shape)\n",
    "            for i, preds in enumerate(preds_list):\n",
    "                weighted_preds += weights[i] * preds\n",
    "            return weighted_preds\n",
    "        \n",
    "        # 定义要优化的损失函数（最大化标准R²）- 修改为使用标准R²而非容忍度R²\n",
    "        def neg_r2(weights, preds_list, y_true):\n",
    "            # 归一化权重确保和为1\n",
    "            weights = np.array(weights)\n",
    "            weights = weights / np.sum(weights)\n",
    "            \n",
    "            weighted_preds = weighted_prediction(weights, preds_list)\n",
    "            r2 = r2_score(y_true, weighted_preds)\n",
    "            return -r2  # 最小化负的R²（即最大化R²）\n",
    "        \n",
    "        # 准备用于优化的预测值列表\n",
    "        train_preds_list = [train_predictions[model_name] for model_name in available_models]\n",
    "        \n",
    "        # 初始权重（均等）\n",
    "        initial_weights = np.ones(len(available_models)) / len(available_models)\n",
    "        \n",
    "        # 约束：权重和为1，所有权重非负\n",
    "        constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "        bounds = [(0, 1) for _ in range(len(available_models))]\n",
    "        \n",
    "        # 使用SLSQP优化算法寻找最优权重\n",
    "        print(\"优化权重中...\")\n",
    "        result = minimize(\n",
    "            neg_r2, \n",
    "            initial_weights, \n",
    "            args=(train_preds_list, y_train[target]),\n",
    "            bounds=bounds,\n",
    "            constraints=constraints,\n",
    "            method='SLSQP'\n",
    "        )\n",
    "        \n",
    "        if result.success:\n",
    "            # 获取最优权重并归一化\n",
    "            optimal_weights = result.x\n",
    "            optimal_weights = optimal_weights / np.sum(optimal_weights)\n",
    "            \n",
    "            print(\"\\n找到最优权重组合:\")\n",
    "            for i, model_name in enumerate(available_models):\n",
    "                print(f\"  {model_name}: {optimal_weights[i]:.4f}\")\n",
    "                \n",
    "            # 使用最优权重在训练集和测试集上评估性能\n",
    "            train_weighted_preds = weighted_prediction(\n",
    "                optimal_weights, \n",
    "                [train_predictions[model_name] for model_name in available_models]\n",
    "            )\n",
    "            \n",
    "            test_weighted_preds = weighted_prediction(\n",
    "                optimal_weights, \n",
    "                [test_predictions[model_name] for model_name in available_models]\n",
    "            )\n",
    "            \n",
    "            # 计算性能指标\n",
    "            train_r2 = r2_score(y_train[target], train_weighted_preds)\n",
    "            train_tol_r2 = tolerance_r2_score(y_train[target], train_weighted_preds, tolerance=current_tolerance, target=target)\n",
    "            train_within_tol = prediction_within_tolerance(y_train[target], train_weighted_preds, tolerance=current_tolerance, target=target)\n",
    "            \n",
    "            test_r2 = r2_score(y_test[target], test_weighted_preds)\n",
    "            test_tol_r2 = tolerance_r2_score(y_test[target], test_weighted_preds, tolerance=current_tolerance, target=target)\n",
    "            test_within_tol = prediction_within_tolerance(y_test[target], test_weighted_preds, tolerance=current_tolerance, target=target)\n",
    "            \n",
    "            print(\"\\n加权平均集成性能:\")\n",
    "            print(f\"  训练集 - R²: {train_r2:.4f}, 容忍度R²: {train_tol_r2:.4f}, 在容忍范围内: {train_within_tol:.2%}\")\n",
    "            print(f\"  测试集 - R²: {test_r2:.4f}, 容忍度R²: {test_tol_r2:.4f}, 在容忍范围内: {test_within_tol:.2%}\")\n",
    "            \n",
    "            # 与各个基础模型比较性能\n",
    "            print(\"\\n与各基础模型性能比较:\")\n",
    "            for model_name in available_models:\n",
    "                model_test_pred = test_predictions[model_name]\n",
    "                model_r2 = r2_score(y_test[target], model_test_pred)\n",
    "                model_tol_r2 = tolerance_r2_score(y_test[target], model_test_pred, tolerance=current_tolerance, target=target)\n",
    "                \n",
    "                r2_diff = test_r2 - model_r2\n",
    "                tol_r2_diff = test_tol_r2 - model_tol_r2\n",
    "                \n",
    "                print(f\"  vs {model_name}:\")\n",
    "                print(f\"    R² 差异: {r2_diff:.4f} ({'+' if r2_diff > 0 else ''}{r2_diff/max(0.0001, abs(model_r2))*100:.2f}%)\")\n",
    "                print(f\"    容忍度R² 差异: {tol_r2_diff:.4f} ({'+' if tol_r2_diff > 0 else ''}{tol_r2_diff/max(0.0001, abs(model_tol_r2))*100:.2f}%)\")\n",
    "            \n",
    "            # 创建加权平均集成模型\n",
    "            class WeightedAverageEnsemble:\n",
    "                def __init__(self, models_dict, model_names, weights, model_datasets):\n",
    "                    self.models_dict = models_dict\n",
    "                    self.model_names = model_names\n",
    "                    self.weights = weights\n",
    "                    self.model_datasets = model_datasets\n",
    "                    \n",
    "                def predict(self, X):\n",
    "                    predictions = []\n",
    "                    \n",
    "                    for i, model_name in enumerate(self.model_names):\n",
    "                        model = self.models_dict[model_name]\n",
    "                        \n",
    "                        # 获取适当的数据格式\n",
    "                        if model_name in ['XGBoost', 'LightGBM', 'HistGradientBoosting']:\n",
    "                            if isinstance(X, pd.DataFrame):\n",
    "                                # 假设X是原始数据框，需要应用适当的预处理\n",
    "                                X_model = X  # 应该在实际应用中进行适当的预处理转换\n",
    "                            else:\n",
    "                                X_model = X\n",
    "                        elif model_name == 'RandomForest':\n",
    "                            if isinstance(X, pd.DataFrame):\n",
    "                                # 对于RandomForest需要填充NaN\n",
    "                                X_model = X.fillna(0)\n",
    "                            else:\n",
    "                                X_model = X\n",
    "                        elif model_name == 'GaussianProcess':\n",
    "                            if isinstance(X, pd.DataFrame):\n",
    "                                # 假设X是原始数据框，需要应用适当的预处理\n",
    "                                X_model = X  # 应该在实际应用中进行适当的预处理转换\n",
    "                            else:\n",
    "                                X_model = X\n",
    "                        else:\n",
    "                            X_model = X\n",
    "                            \n",
    "                        model_pred = model.predict(X_model)\n",
    "                        predictions.append(model_pred)\n",
    "                    \n",
    "                    # 应用权重\n",
    "                    weighted_preds = np.zeros(predictions[0].shape)\n",
    "                    for i, preds in enumerate(predictions):\n",
    "                        weighted_preds += self.weights[i] * preds\n",
    "                        \n",
    "                    return weighted_preds\n",
    "            \n",
    "            # 创建模型数据集字典\n",
    "            model_datasets = {\n",
    "                'XGBoost': 'tree',\n",
    "                'LightGBM': 'tree',\n",
    "                'HistGradientBoosting': 'tree',\n",
    "                'RandomForest': 'tree_filled',\n",
    "                'GaussianProcess': 'linear'\n",
    "            }\n",
    "            \n",
    "            # 实例化加权平均集成模型\n",
    "            weighted_model = WeightedAverageEnsemble(\n",
    "                models_dict=models[target],\n",
    "                model_names=available_models,\n",
    "                weights=optimal_weights,\n",
    "                model_datasets=model_datasets\n",
    "            )\n",
    "            \n",
    "            # 保存模型\n",
    "            models[target]['WeightedEnsemble'] = weighted_model\n",
    "            # 使用pickle保存加权平均集成模型\n",
    "            weighted_model_file = os.path.join(model_folder, f'{target}_加权平均集成模型.pkl')\n",
    "            with open(weighted_model_file, 'wb') as f:\n",
    "                pickle.dump(weighted_model, f)\n",
    "            print(f\"加权平均集成模型已保存至 {weighted_model_file}\")\n",
    "\n",
    "            # 保存训练集和测试集的预测结果\n",
    "            train_prediction = pd.DataFrame({\n",
    "                '实际值': y_train[target],\n",
    "                '加权平均预测值': train_weighted_preds,\n",
    "                '误差': np.abs(y_train[target] - train_weighted_preds)\n",
    "            })\n",
    "\n",
    "            test_prediction = pd.DataFrame({\n",
    "                '实际值': y_test[target],\n",
    "                '加权平均预测值': test_weighted_preds,\n",
    "                '误差': np.abs(y_test[target] - test_weighted_preds)\n",
    "            })\n",
    "\n",
    "            # 添加各基础模型的预测结果以便比较\n",
    "            for model_name in available_models:\n",
    "                train_prediction[f'{model_name}预测值'] = train_predictions[model_name]\n",
    "                test_prediction[f'{model_name}预测值'] = test_predictions[model_name]\n",
    "\n",
    "            # 保存到文件\n",
    "            train_file = os.path.join(save_folder, f'{target}_加权平均集成训练集预测结果.csv')\n",
    "            test_file = os.path.join(save_folder, f'{target}_加权平均集成测试集预测结果.csv')\n",
    "\n",
    "            train_prediction.to_csv(train_file, index=False)\n",
    "            test_prediction.to_csv(test_file, index=False)\n",
    "\n",
    "            print(f\"训练集预测结果已保存至 {train_file}\")\n",
    "            print(f\"测试集预测结果已保存至 {test_file}\")\n",
    "\n",
    "            # 可视化: 预测vs实际值散点图 (训练集和测试集)\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # 训练集散点图\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(y_train[target], train_weighted_preds, alpha=0.5)\n",
    "            plt.plot([y_train[target].min(), y_train[target].max()], [y_train[target].min(), y_train[target].max()], 'r--')\n",
    "            plt.xlabel('实际值')\n",
    "            plt.ylabel('预测值')\n",
    "            plt.title(f'训练集: R²={train_r2:.4f}')\n",
    "            \n",
    "            # 测试集散点图\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.scatter(y_test[target], test_weighted_preds, alpha=0.5)\n",
    "            plt.plot([y_test[target].min(), y_test[target].max()], [y_test[target].min(), y_test[target].max()], 'r--')\n",
    "            plt.xlabel('实际值')\n",
    "            plt.ylabel('预测值')\n",
    "            plt.title(f'测试集: R²={test_r2:.4f}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 绘制误差分布\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # 训练集误差\n",
    "            plt.subplot(1, 2, 1)\n",
    "            train_errors = y_train[target] - train_weighted_preds\n",
    "            plt.hist(train_errors, bins=30, alpha=0.7)\n",
    "            plt.axvline(x=0, color='r', linestyle='--')\n",
    "            plt.xlabel('预测误差')\n",
    "            plt.ylabel('频次')\n",
    "            plt.title(f'训练集误差分布 (MAE={np.abs(train_errors).mean():.4f})')\n",
    "            \n",
    "            # 测试集误差\n",
    "            plt.subplot(1, 2, 2)\n",
    "            test_errors = y_test[target] - test_weighted_preds\n",
    "            plt.hist(test_errors, bins=30, alpha=0.7)\n",
    "            plt.axvline(x=0, color='r', linestyle='--')\n",
    "            plt.xlabel('预测误差')\n",
    "            plt.ylabel('频次')\n",
    "            plt.title(f'测试集误差分布 (MAE={np.abs(test_errors).mean():.4f})')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 绘制权重条形图\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.bar(available_models, optimal_weights)\n",
    "            plt.xlabel('模型')\n",
    "            plt.ylabel('权重')\n",
    "            plt.title(f'{target} - 加权平均集成模型权重分布')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(True, axis='y')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 绘制各模型与加权平均模型的预测对比图\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            model_data = [test_weighted_preds] + [test_predictions[model] for model in available_models]\n",
    "            model_labels = ['加权平均'] + available_models\n",
    "            \n",
    "            plt.boxplot(model_data, labels=model_labels)\n",
    "            plt.ylabel('预测值')\n",
    "            plt.title('加权平均模型与各基础模型预测分布对比')\n",
    "            plt.grid(True, axis='y')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "            print(\"权重优化失败:\", result.message)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"创建加权平均集成模型失败: {str(e)}\")\n",
    "        print(f\"错误详情: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 模型评估与可视化 ======================\n",
    "print(\"=\" * 40)\n",
    "print(\"模型评估阶段\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "# 设置可视化主题\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n",
    "colors = sns.color_palette(\"viridis\", 8)\n",
    "\n",
    "class CustomLGBMRegressor:\n",
    "    def __init__(self, **params):\n",
    "        self.params = params\n",
    "        self.model = None\n",
    "    def fit(self, X, y):\n",
    "        feature_names = [f'f{i}' for i in range(X.shape[1])]\n",
    "        X_values = X.values if hasattr(X, 'values') else X\n",
    "        train_data = lgb.Dataset(X_values, label=y, feature_name=feature_names)\n",
    "        self.model = lgb.train(self.params, train_data, num_boost_round=self.params.get('n_estimators', 100))\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        if self.model is None: raise ValueError(\"Model not trained.\")\n",
    "        X_values = X.values if hasattr(X, 'values') else X\n",
    "        return self.model.predict(X_values)\n",
    "\n",
    "class EnhancedVotingRegressor:\n",
    "    def __init__(self, estimators, weights, datasets, target_name):\n",
    "        self.estimators = estimators\n",
    "        self.weights = np.array(weights)\n",
    "        self.datasets = datasets\n",
    "        self.target_name = target_name\n",
    "        if np.sum(self.weights) > 0: self.weights = self.weights / np.sum(self.weights)\n",
    "    def predict(self, X):\n",
    "        print(\"警告：直接调用 EnhancedVotingRegressor 的 predict 方法可能导致结果不准，请使用 predict_model_unified 函数。\")\n",
    "        return np.zeros(len(X) if hasattr(X, '__len__') else 1)\n",
    "\n",
    "class AdaptiveEnsembleModel:\n",
    "    def __init__(self, meta_classifier, models_dict, available_models, model_input_data):\n",
    "        self.meta_classifier = meta_classifier\n",
    "        self.models_dict = models_dict\n",
    "        self.available_models = available_models\n",
    "        self.model_input_data = model_input_data\n",
    "        self.data_type_map = {'XGBoost': 'tree', 'LightGBM': 'tree', 'HistGradientBoosting': 'tree', 'RandomForest': 'tree_filled', 'GaussianProcess': 'linear'}\n",
    "    def predict(self, X):\n",
    "        print(\"警告：直接调用 AdaptiveEnsembleModel 的 predict 方法可能导致结果不准，请使用 predict_model_unified 函数。\")\n",
    "        return np.zeros(len(X) if hasattr(X, '__len__') else 1)\n",
    "\n",
    "class WeightedAverageEnsemble:\n",
    "    def __init__(self, models_dict, model_names, weights, model_datasets):\n",
    "        self.models_dict = models_dict\n",
    "        self.model_names = model_names\n",
    "        self.weights = weights\n",
    "        self.model_datasets = model_datasets\n",
    "    def predict(self, X):\n",
    "        print(\"警告：直接调用 WeightedAverageEnsemble 的 predict 方法可能导致结果不准，请使用 predict_model_unified 函数。\")\n",
    "        return np.zeros(len(X) if hasattr(X, '__len__') else 1)\n",
    "\n",
    "# --- 2. 统一的、可靠的预测函数 ---\n",
    "def predict_model_unified(model_obj, model_name, target, data_dict):\n",
    "    if target not in data_dict['original']: return None\n",
    "\n",
    "    model_type_map = {\n",
    "        'VotingEnsemble': 'VotingEnsemble', '投票集成': 'VotingEnsemble',\n",
    "        'WeightedEnsemble': 'WeightedEnsemble', '加权平均集成': 'WeightedEnsemble',\n",
    "        'AdaptiveEnsemble': 'AdaptiveEnsemble', '自适应集成': 'AdaptiveEnsemble'\n",
    "    }\n",
    "    ensemble_type = model_type_map.get(model_name)\n",
    "\n",
    "    if ensemble_type == 'VotingEnsemble':\n",
    "        base_models, weights, model_datasets = model_obj.estimators, model_obj.weights, model_obj.datasets\n",
    "        predictions = {}\n",
    "        for name, base_model in base_models:\n",
    "            md_type = model_datasets.get(name, 'original')\n",
    "            predictions[name] = base_model.predict(data_dict[md_type][target])\n",
    "        return np.average([predictions[name] for name, _ in base_models], axis=0, weights=weights)\n",
    "    \n",
    "    elif ensemble_type == 'WeightedEnsemble':\n",
    "        model_names, weights, models_dict = model_obj.model_names, model_obj.weights, model_obj.models_dict\n",
    "        predictions = []\n",
    "        for mn in model_names:\n",
    "            base_model_instance = models_dict[mn]\n",
    "            md_type = model_obj.model_datasets.get(mn, 'original')\n",
    "            predictions.append(base_model_instance.predict(data_dict[md_type][target]))\n",
    "        return np.average(predictions, axis=0, weights=weights)\n",
    "\n",
    "    elif ensemble_type == 'AdaptiveEnsemble':\n",
    "        meta_classifier, models_dict, available_models, data_type_map = model_obj.meta_classifier, model_obj.models_dict, model_obj.available_models, model_obj.data_type_map\n",
    "        X_raw = data_dict['original'][target]\n",
    "        model_choices = meta_classifier.predict(X_raw)\n",
    "        \n",
    "        predictions_dict = {}\n",
    "        for mn in available_models:\n",
    "            base_model_instance = models_dict[mn]\n",
    "            md_type = data_type_map.get(mn, 'original')\n",
    "            predictions_dict[mn] = base_model_instance.predict(data_dict[md_type][target])\n",
    "        \n",
    "        return np.array([predictions_dict[available_models[choice]][i] for i, choice in enumerate(model_choices)])\n",
    "        \n",
    "    else: # 普通模型\n",
    "        model_type_map = {'XGBoost':'tree', 'LightGBM':'tree', 'HistGradientBoosting':'tree', 'RandomForest':'tree_filled', 'GaussianProcess':'linear'}\n",
    "        data_type_key = model_type_map.get(model_name, 'original')\n",
    "        return model_obj.predict(data_dict[data_type_key][target])\n",
    "        \n",
    "# 定义容忍度评估函数\n",
    "def tolerance_r2_score(y_true, y_pred, tolerance=0.15, target=None):\n",
    "    \"\"\"\n",
    "    计算容忍度R²评分，允许一定误差范围内的预测被视为准确\n",
    "    \"\"\"\n",
    "    # 确保输入数据是numpy数组并且形状正确\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    \n",
    "    # 如果提供了目标变量名，则使用目标特定的容忍度\n",
    "    if target and target in target_tolerance:\n",
    "        tolerance = target_tolerance[target]\n",
    "    \n",
    "    # 计算容忍范围\n",
    "    tolerance_values = tolerance * np.abs(y_true)\n",
    "    # 计算残差\n",
    "    residuals = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # 调整残差，使误差在容忍范围内的视为0\n",
    "    adjusted_residuals = np.maximum(0, residuals - tolerance_values)\n",
    "    \n",
    "    # 计算修正后的总平方和\n",
    "    y_true_mean = np.mean(y_true)\n",
    "    tss = np.sum((y_true - y_true_mean) ** 2)\n",
    "    \n",
    "    # 计算修正后的残差平方和\n",
    "    rss = np.sum(adjusted_residuals ** 2)\n",
    "    \n",
    "    # 计算修正后的R²\n",
    "    if tss == 0:\n",
    "        return 0  # 防止除以0\n",
    "    \n",
    "    tolerance_r2 = 1 - (rss / tss)\n",
    "    return tolerance_r2\n",
    "\n",
    "def prediction_within_tolerance(y_true, y_pred, tolerance=0.15, target=None):\n",
    "    \"\"\"\n",
    "    计算预测值在目标值±容忍范围内的比例\n",
    "    \"\"\"\n",
    "    # 确保输入为numpy数组\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    # 如果提供了目标变量名，则使用目标特定的容忍度\n",
    "    if target and target in target_tolerance:\n",
    "        tolerance = target_tolerance[target]\n",
    "    \n",
    "    # 计算容忍范围\n",
    "    tolerance_values = tolerance * np.abs(y_true)\n",
    "    \n",
    "    # 检查预测是否在容忍范围内\n",
    "    within_tolerance = np.abs(y_true - y_pred) <= tolerance_values\n",
    "    \n",
    "    # 计算在容忍范围内的预测比例\n",
    "    return np.mean(within_tolerance)\n",
    "\n",
    "def make_tolerance_scorer(target):\n",
    "    def scorer_function(estimator, X, y):\n",
    "        y_pred = estimator.predict(X)\n",
    "        return tolerance_r2_score(y, y_pred, target=target)\n",
    "    \n",
    "    # 设置函数名称\n",
    "    scorer_function.__name__ = f'tolerance_scorer_{target}'\n",
    "    return scorer_function\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"将文件名中的无效字符替换为下划线\"\"\"\n",
    "    invalid_chars = ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|']\n",
    "    for char in invalid_chars:\n",
    "        filename = filename.replace(char, '_')\n",
    "    return filename\n",
    "\n",
    "def save_plot_data(data, filename, description=\"\"):\n",
    "    \"\"\"保存作图数据到CSV文件\"\"\"\n",
    "    os.makedirs('plot_data', exist_ok=True)\n",
    "    data.to_csv(f'plot_data/{filename}.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"已保存{description}数据到 plot_data/{filename}.csv\")\n",
    "\n",
    "# 定义目标变量特定的容忍度值\n",
    "target_tolerance = {\n",
    "    '水接触角': 0.05,\n",
    "    '循环使用次数': 0.1,\n",
    "    '吸油能力': 0.1\n",
    "}\n",
    "\n",
    "# 定义模型组\n",
    "model_types = {\n",
    "    \"XGBoost\": \"tree\", \n",
    "    \"LightGBM\": \"tree\",\n",
    "    \"HistGradientBoosting\": \"tree\",\n",
    "    \"RandomForest\": \"tree_filled\",\n",
    "    \"GaussianProcess\": \"linear\",\n",
    "    \"VotingEnsemble\": \"ensemble\",\n",
    "    \"AdaptiveEnsemble\": \"ensemble\",\n",
    "    \"WeightedEnsemble\": \"ensemble\"\n",
    "}\n",
    "\n",
    "# 创建存储评估结果的字典\n",
    "evaluation_results = {}\n",
    "\n",
    "# 获取目标变量列表\n",
    "target_columns = ['水接触角', '循环使用次数', '吸油能力']\n",
    "valid_targets = target_columns  # 假设所有目标变量都有效\n",
    "\n",
    "# 加载测试数据（假设数据分割已完成）\n",
    "print(\"加载测试数据...\")\n",
    "try:\n",
    "    X_test = pickle.load(open('data_exports/X_test.pkl', 'rb'))\n",
    "    X_test_linear = pickle.load(open('data_exports/X_test_linear.pkl', 'rb'))\n",
    "    X_test_tree = pickle.load(open('data_exports/X_test_tree.pkl', 'rb'))\n",
    "    X_test_tree_filled = pickle.load(open('data_exports/X_test_tree_filled.pkl', 'rb'))\n",
    "    y_test = pickle.load(open('data_exports/y_test.pkl', 'rb'))\n",
    "    print(\"测试数据加载成功\")\n",
    "except Exception as e:\n",
    "    print(f\"加载测试数据失败: {str(e)}\")\n",
    "    # 如果无法加载，尝试使用调参预处理脚本中的数据\n",
    "    print(\"尝试使用调参预处理脚本中的数据...\")\n",
    "    # 这里应该包含从调参预处理脚本中获取数据的代码\n",
    "    # 但由于你没有提供完整的代码，我们假设数据已经可用\n",
    "\n",
    "# 创建模型评估文件夹\n",
    "os.makedirs('模型评估结果', exist_ok=True)\n",
    "\n",
    "# 评估每个目标变量的模型\n",
    "for target in valid_targets:\n",
    "    print(f\"\\n评估 {target} 的模型性能...\")\n",
    "    \n",
    "    # 获取目标特定的容忍度\n",
    "    current_tolerance = target_tolerance.get(target, 0.15)\n",
    "    \n",
    "    # 初始化目标的评估结果字典\n",
    "    evaluation_results[target] = {}\n",
    "    \n",
    "    # 加载模型文件夹中的所有模型\n",
    "    model_folder = '训练模型文件'\n",
    "    model_files = [f for f in os.listdir(model_folder) if f.startswith(f'{target}_') and f.endswith('.pkl') and not f.endswith('_features.pkl')]\n",
    "    \n",
    "    # 如果没有找到模型文件，跳过这个目标\n",
    "    if not model_files:\n",
    "        print(f\"没有找到 {target} 的模型文件\")\n",
    "        continue\n",
    "    \n",
    "    # 遍历所有模型文件并评估\n",
    "\n",
    "    # 在进入循环前，加载一次所有测试数据\n",
    "    X_test_dict = {\n",
    "        'original': X_test,\n",
    "        'linear': X_test_linear,\n",
    "        'tree': X_test_tree,\n",
    "        'tree_filled': X_test_tree_filled,\n",
    "    }\n",
    "\n",
    "    # 遍历所有模型文件并评估\n",
    "    for model_file in model_files:\n",
    "        # 从文件名提取模型名称\n",
    "        model_name = model_file.replace(f'{target}_', '').replace('模型.pkl', '')\n",
    "        print(f\"\\n评估 {model_name} 模型...\")\n",
    "        \n",
    "        # 加载模型\n",
    "        model_path = os.path.join(model_folder, model_file)\n",
    "        try:\n",
    "            with open(model_path, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            \n",
    "            # 关键修改：使用统一的预测函数，替换掉原来复杂的 if/else 和 model.predict\n",
    "            y_pred = predict_model_unified(model, model_name, target, X_test_dict)\n",
    "            \n",
    "            # --- 以下所有功能代码与您原来保持一致，确保功能不丢失 ---\n",
    "\n",
    "            # 计算评估指标\n",
    "            r2 = r2_score(y_test[target], y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test[target], y_pred))\n",
    "            mae = mean_absolute_error(y_test[target], y_pred)\n",
    "            \n",
    "            # 计算容忍度指标\n",
    "            tol_r2 = tolerance_r2_score(y_test[target], y_pred, tolerance=current_tolerance, target=target)\n",
    "            within_tol = prediction_within_tolerance(y_test[target], y_pred, tolerance=current_tolerance, target=target)\n",
    "            \n",
    "            # 存储结果\n",
    "            evaluation_results[target][model_name] = {\n",
    "                'r2': r2,\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'tolerance_r2': tol_r2,\n",
    "                'within_tolerance': within_tol,\n",
    "                'y_pred': y_pred\n",
    "            }\n",
    "            \n",
    "            print(f\"  {model_name} - R²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
    "            print(f\"  容忍度R² (容忍度={current_tolerance:.2f}): {tol_r2:.4f}, 预测在容忍范围内比例: {within_tol:.2%}\")\n",
    "            \n",
    "            # 生成预测对比散点图\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.scatter(y_test[target], y_pred, alpha=0.5, c='darkblue')\n",
    "            \n",
    "            min_val = min(min(y_test[target]), min(y_pred))\n",
    "            max_val = max(max(y_test[target]), max(y_pred))\n",
    "            margin = (max_val - min_val) * 0.1\n",
    "            plt.plot([min_val - margin, max_val + margin], [min_val - margin, max_val + margin], 'r--', \n",
    "                    label='理想预测线')\n",
    "            \n",
    "            if current_tolerance > 0:\n",
    "                plt.fill_between([min_val - margin, max_val + margin], \n",
    "                                [(min_val - margin) * (1 - current_tolerance), (max_val + margin) * (1 - current_tolerance)], \n",
    "                                [(min_val - margin) * (1 + current_tolerance), (max_val + margin) * (1 + current_tolerance)], \n",
    "                                color='green', alpha=0.1, label=f'±{current_tolerance:.0%}容忍范围')\n",
    "            \n",
    "            plt.title(f'{target} - {model_name}模型预测值与实际值对比', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('实际值', fontsize=12)\n",
    "            plt.ylabel('预测值', fontsize=12)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.legend()\n",
    "            \n",
    "            metrics_text = f'R² = {r2:.4f}\\nRMSE = {rmse:.4f}\\n容忍度R² = {tol_r2:.4f}\\n在容忍范围内比例 = {within_tol:.2%}'\n",
    "            plt.annotate(metrics_text, xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "                        fontsize=11, fontweight='bold', \n",
    "                        bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "            \n",
    "            # 保存图表\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'模型评估结果/散点图/{sanitize_filename(target)}_{model_name}_prediction.png', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  评估 {model_name} 失败: {str(e)}\")\n",
    "\n",
    "    \n",
    "    # 确定最佳模型 (使用三种标准)\n",
    "    if evaluation_results[target]:\n",
    "        # 标准R²最佳模型\n",
    "        best_r2_model = max(evaluation_results[target], \n",
    "                          key=lambda m: evaluation_results[target][m]['r2'])\n",
    "        best_r2 = evaluation_results[target][best_r2_model]['r2']\n",
    "        \n",
    "        # 容忍度R²最佳模型\n",
    "        best_tol_r2_model = max(evaluation_results[target], \n",
    "                              key=lambda m: evaluation_results[target][m]['tolerance_r2'])\n",
    "        best_tol_r2 = evaluation_results[target][best_tol_r2_model]['tolerance_r2']\n",
    "        \n",
    "        # 容忍范围内预测最佳模型\n",
    "        best_within_tol_model = max(evaluation_results[target], \n",
    "                                  key=lambda m: evaluation_results[target][m]['within_tolerance'])\n",
    "        best_within_tol = evaluation_results[target][best_within_tol_model]['within_tolerance']\n",
    "        \n",
    "        print(f\"\\n{target} 的最佳模型:\")\n",
    "        print(f\"  标准R²最佳模型: {best_r2_model} (R²={best_r2:.4f})\")\n",
    "        print(f\"  容忍度R²最佳模型: {best_tol_r2_model} (容忍度R²={best_tol_r2:.4f})\")\n",
    "        print(f\"  预测在容忍范围内比例最佳模型: {best_within_tol_model} (在容忍范围内={best_within_tol:.2%})\")\n",
    "        # 综合评价筛选最佳模型\n",
    "        print(f\"\\n使用综合评价标准筛选{target}的最佳模型:\")\n",
    "        # 定义权重\n",
    "        weights = {\n",
    "            'r2': 0.5,           # 标准R²权重最大\n",
    "            'tolerance_r2': 0.3,  # 容忍度R²权重居中\n",
    "            'within_tolerance': 0.2  # 在容忍范围内比例权重最小\n",
    "        }\n",
    "        print(f\"评价标准权重分配: 标准R²={weights['r2']}, 容忍度R²={weights['tolerance_r2']}, 容忍范围内比例={weights['within_tolerance']}\")\n",
    "\n",
    "        # 计算每个模型的综合得分\n",
    "        composite_scores = {}\n",
    "        for model_name in evaluation_results[target]:\n",
    "            # 获取各指标的值\n",
    "            r2_score_val = evaluation_results[target][model_name]['r2']\n",
    "            tol_r2_val = evaluation_results[target][model_name]['tolerance_r2']\n",
    "            within_tol_val = evaluation_results[target][model_name]['within_tolerance']\n",
    "            \n",
    "            # 计算综合得分\n",
    "            composite_score = (weights['r2'] * r2_score_val + \n",
    "                            weights['tolerance_r2'] * tol_r2_val + \n",
    "                            weights['within_tolerance'] * within_tol_val)\n",
    "            \n",
    "            composite_scores[model_name] = composite_score\n",
    "\n",
    "        # 找出综合得分最高的模型\n",
    "        best_composite_model = max(composite_scores, key=composite_scores.get)\n",
    "        best_composite_score = composite_scores[best_composite_model]\n",
    "\n",
    "        print(f\"正在为{target}创建最佳综合模型的副本...\")\n",
    "\n",
    "        try:\n",
    "            # 构建源模型路径\n",
    "            source_model_path = os.path.join(model_folder, f'{target}_{best_composite_model}模型.pkl')\n",
    "            \n",
    "            # 在模型评估结果目录中创建\"最佳模型\"文件夹\n",
    "            best_model_folder = os.path.join('模型评估结果', '最佳模型')\n",
    "            if not os.path.exists(best_model_folder):\n",
    "                os.makedirs(best_model_folder)\n",
    "                print(f\"已创建最佳模型文件夹：{best_model_folder}\")\n",
    "            \n",
    "            # 构建目标模型路径 - 在最佳模型文件夹中\n",
    "            best_model_path = os.path.join(best_model_folder, f'{target}_最佳模型.pkl')\n",
    "            \n",
    "            # 复制最佳模型文件\n",
    "            if os.path.exists(source_model_path):\n",
    "                # 使用shutil复制文件\n",
    "                import shutil\n",
    "                shutil.copy2(source_model_path, best_model_path)\n",
    "                print(f\"已复制最佳模型({best_composite_model})到: {best_model_path}\")\n",
    "                \n",
    "                # 同时在训练模型文件夹中创建一个带有\"最佳\"标识的副本，确保解释脚本能找到\n",
    "                best_model_in_train_folder = os.path.join(model_folder, f'{target}_最佳综合模型.pkl')\n",
    "                shutil.copy2(source_model_path, best_model_in_train_folder)\n",
    "                print(f\"已复制最佳模型到训练模型文件夹: {best_model_in_train_folder}\")\n",
    "            else:\n",
    "                print(f\"警告: 无法找到源模型文件 {source_model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"复制最佳模型时出错: {str(e)}\")\n",
    "        print(f\"综合评价最佳模型: {best_composite_model} (得分={best_composite_score:.4f})\")\n",
    "        print(f\"  - 标准R²: {evaluation_results[target][best_composite_model]['r2']:.4f}\")\n",
    "        print(f\"  - 容忍度R²: {evaluation_results[target][best_composite_model]['tolerance_r2']:.4f}\")\n",
    "        print(f\"  - 在容忍范围内比例: {evaluation_results[target][best_composite_model]['within_tolerance']:.2%}\")\n",
    "        # 将最佳模型信息保存到文件\n",
    "        best_models_info = {\n",
    "            'best_r2_model': best_r2_model,\n",
    "            'best_r2': best_r2,\n",
    "            'best_tol_r2_model': best_tol_r2_model,\n",
    "            'best_tol_r2': best_tol_r2,\n",
    "            'best_within_tol_model': best_within_tol_model,\n",
    "            'best_within_tol': best_within_tol,\n",
    "            'best_composite_model': best_composite_model,\n",
    "            'best_composite_score': best_composite_score\n",
    "        }\n",
    "        if best_composite_model in [\"XGBoost\", \"LightGBM\", \"HistGradientBoosting\"]:\n",
    "            best_models_info['model_type'] = 'tree'\n",
    "        elif best_composite_model == \"RandomForest\":\n",
    "            best_models_info['model_type'] = 'tree_filled'\n",
    "        elif best_composite_model == \"GaussianProcess\":\n",
    "            best_models_info['model_type'] = 'linear'\n",
    "        elif best_composite_model in [\"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"HuberRegressor\"]:\n",
    "            best_models_info['model_type'] = 'linear'\n",
    "        elif best_composite_model in [\"VotingEnsemble\", \"AdaptiveEnsemble\", \"WeightedEnsemble\"]:\n",
    "            best_models_info['model_type'] = 'ensemble'\n",
    "        elif best_composite_model in [\"DeepNN\"]:\n",
    "            best_models_info['model_type'] = 'nn'\n",
    "        else:\n",
    "            best_models_info['model_type'] = 'tree'  # 默认为tree类型\n",
    "\n",
    "        print(f\"最佳模型类型: {best_models_info['model_type']}\")\n",
    "        with open(f'模型评估结果/{sanitize_filename(target)}_best_models.pkl', 'wb') as f:\n",
    "            pickle.dump(best_models_info, f)\n",
    "    else:\n",
    "        print(f\"没有 {target} 的有效评估结果\")\n",
    "\n",
    "# 保存评估结果\n",
    "with open('模型评估结果/evaluation_results.pkl', 'wb') as f:\n",
    "    pickle.dump(evaluation_results, f)\n",
    "\n",
    "print(\"评估结果已保存\")\n",
    "\n",
    "# ====================== 结果可视化 ======================\n",
    "print(\"\\n生成评估结果可视化...\")\n",
    "\n",
    "# 为每个目标变量创建模型性能对比图\n",
    "for target in evaluation_results:\n",
    "    if not evaluation_results[target]:\n",
    "        continue\n",
    "    \n",
    "    # 获取目标特定的容忍度\n",
    "    current_tolerance = target_tolerance.get(target, 0.15)\n",
    "        \n",
    "    # 提取评估指标\n",
    "    model_names = list(evaluation_results[target].keys())\n",
    "    r2_scores = [evaluation_results[target][m]['r2'] for m in model_names]\n",
    "    rmse_scores = [evaluation_results[target][m]['rmse'] for m in model_names]\n",
    "    mae_scores = [evaluation_results[target][m]['mae'] for m in model_names]\n",
    "    tol_r2_scores = [evaluation_results[target][m]['tolerance_r2'] for m in model_names]\n",
    "    within_tol_scores = [evaluation_results[target][m]['within_tolerance'] for m in model_names]\n",
    "    \n",
    "    # 计算模型排名\n",
    "    model_ranks = pd.DataFrame({\n",
    "        '模型': model_names,\n",
    "        'R²': r2_scores,\n",
    "        'RMSE': rmse_scores,\n",
    "        'MAE': mae_scores,\n",
    "        f'容忍度R² (容忍度={current_tolerance:.2f})': tol_r2_scores,\n",
    "        f'在容忍范围内比例 (容忍度={current_tolerance:.2f})': within_tol_scores\n",
    "    }).sort_values(by='R²', ascending=False)\n",
    "    \n",
    "    # 保存排名数据\n",
    "    save_plot_data(model_ranks, f\"{sanitize_filename(target)}_model_rankings\", \"模型排名\")\n",
    "    \n",
    "    # 创建目录\n",
    "    os.makedirs('模型评估结果/性能对比图', exist_ok=True)\n",
    "    \n",
    "    # 绘制R²性能对比图\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sorted_r2 = model_ranks.sort_values(by='R²', ascending=False)\n",
    "    \n",
    "    bars = plt.barh(sorted_r2['模型'], sorted_r2['R²'])\n",
    "    plt.xlabel('R² 分数', fontsize=12)\n",
    "    plt.title(f'{target} - 模型标准R²性能对比', fontsize=14, fontweight='bold')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.xlim(-0.1, 1.0)  # 设置合理的R²范围\n",
    "\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(max(width, 0.01), bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.4f}', ha='left', va='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'模型评估结果/性能对比图/{sanitize_filename(target)}_r2_comparison.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 绘制容忍度R²性能对比图\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sorted_tol_r2 = model_ranks.sort_values(by=f'容忍度R² (容忍度={current_tolerance:.2f})', ascending=False)\n",
    "    \n",
    "    bars = plt.barh(sorted_tol_r2['模型'], sorted_tol_r2[f'容忍度R² (容忍度={current_tolerance:.2f})'])\n",
    "    plt.xlabel(f'容忍度R² (容忍度={current_tolerance:.2f})', fontsize=12)\n",
    "    plt.title(f'{target} - 模型容忍度R²性能对比', fontsize=14, fontweight='bold')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.xlim(-0.1, 1.0)  # 设置合理的R²范围\n",
    "    \n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(max(width, 0.01), bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.4f}', ha='left', va='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'模型评估结果/性能对比图/{sanitize_filename(target)}_tolerance_r2_comparison.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 绘制在容忍范围内比例对比图\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sorted_within_tol = model_ranks.sort_values(by=f'在容忍范围内比例 (容忍度={current_tolerance:.2f})', ascending=False)\n",
    "    \n",
    "    bars = plt.barh(sorted_within_tol['模型'], sorted_within_tol[f'在容忍范围内比例 (容忍度={current_tolerance:.2f})'])\n",
    "    plt.xlabel(f'在容忍范围内预测比例 (容忍度={current_tolerance:.2f})', fontsize=12)\n",
    "    plt.title(f'{target} - 模型预测在容忍范围内比例对比', fontsize=14, fontweight='bold')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.xlim(0, 1.0)\n",
    "    \n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(max(width, 0.01), bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.2%}', ha='left', va='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'模型评估结果/性能对比图/{sanitize_filename(target)}_within_tolerance_comparison.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 绘制RMSE对比图 (越低越好)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sorted_rmse = model_ranks.sort_values(by='RMSE')\n",
    "    \n",
    "    bars = plt.barh(sorted_rmse['模型'], sorted_rmse['RMSE'])\n",
    "    plt.xlabel('RMSE 值', fontsize=12)\n",
    "    plt.title(f'{target} - 模型RMSE性能对比 (越低越好)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(max(width, 0.01), bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.4f}', ha='left', va='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'模型评估结果/性能对比图/{sanitize_filename(target)}_rmse_comparison.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# 创建热图目录\n",
    "os.makedirs('模型评估结果/热图', exist_ok=True)\n",
    "\n",
    "# 创建三个目标变量的模型性能热图\n",
    "metrics = ['r2', 'tolerance_r2', 'within_tolerance']\n",
    "metric_names = {\n",
    "    'r2': 'R²',\n",
    "    'tolerance_r2': '容忍度R²',\n",
    "    'within_tolerance': '在容忍范围内比例'\n",
    "}\n",
    "\n",
    "# 获取所有模型名称和所有目标变量\n",
    "all_models = set()\n",
    "for target in evaluation_results:\n",
    "    all_models.update(evaluation_results[target].keys())\n",
    "all_models = sorted(all_models)\n",
    "\n",
    "all_targets = list(evaluation_results.keys())\n",
    "\n",
    "# 为每个指标创建热图\n",
    "for metric in metrics:\n",
    "    # 创建数据矩阵\n",
    "    data_matrix = np.zeros((len(all_targets), len(all_models)))\n",
    "    \n",
    "    for i, target in enumerate(all_targets):\n",
    "        for j, model in enumerate(all_models):\n",
    "            if model in evaluation_results[target]:\n",
    "                if metric in evaluation_results[target][model]:\n",
    "                    data_matrix[i, j] = evaluation_results[target][model][metric]\n",
    "                else:\n",
    "                    data_matrix[i, j] = np.nan\n",
    "            else:\n",
    "                data_matrix[i, j] = np.nan\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(data_matrix, index=all_targets, columns=all_models)\n",
    "    \n",
    "    # 保存数据\n",
    "    save_plot_data(df, f\"global_{metric}_heatmap\", f\"全局{metric_names[metric]}热图数据\")\n",
    "    \n",
    "    # 绘制热图\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    if metric == 'r2' or metric == 'tolerance_r2':\n",
    "        # 对于R²和容忍度指标，越高越好\n",
    "        cmap = sns.color_palette(\"YlGnBu\", as_cmap=True)\n",
    "        vmin = max(0, df.min().min())  # R²可能为负\n",
    "        vmax = min(1.0, df.max().max())  # 确保R²的合理范围\n",
    "    elif metric == 'within_tolerance':\n",
    "        # 对于容忍度范围内比例，使用百分比格式\n",
    "        cmap = sns.color_palette(\"YlGnBu\", as_cmap=True)\n",
    "        vmin = 0\n",
    "        vmax = 1.0\n",
    "        fmt = '.1%'\n",
    "    else:\n",
    "        # 默认情况\n",
    "        cmap = sns.color_palette(\"YlGnBu\", as_cmap=True)\n",
    "        vmin = None\n",
    "        vmax = None\n",
    "        fmt = '.3f'\n",
    "    \n",
    "    ax = sns.heatmap(df, annot=True, cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "                    linewidths=0.5, fmt='.3f' if metric != 'within_tolerance' else '.1%',\n",
    "                    cbar_kws={'label': metric_names[metric]})\n",
    "    \n",
    "    plt.title(f'所有目标变量的{metric_names[metric]}性能对比', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('模型', fontsize=12)\n",
    "    plt.ylabel('目标变量', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'模型评估结果/热图/global_{metric}_heatmap.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# 创建模型比较汇总表格\n",
    "summary_data = []\n",
    "\n",
    "for target in all_targets:\n",
    "\n",
    "    # 在评估模型的循环中，添加特征信息保存\n",
    "    for model_name in evaluation_results[target]:\n",
    "        # 获取模型类型\n",
    "        if model_name in [\"XGBoost\", \"LightGBM\", \"HistGradientBoosting\"]:\n",
    "            model_type = 'tree'\n",
    "            features_used = X_test_tree[target].columns.tolist()\n",
    "        elif model_name == \"RandomForest\":\n",
    "            model_type = 'tree_filled'\n",
    "            features_used = X_test_tree_filled[target].columns.tolist()\n",
    "        elif model_name in [\"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"HuberRegressor\", \"GaussianProcess\"]:\n",
    "            model_type = 'linear'\n",
    "            features_used = X_test_linear[target].columns.tolist()\n",
    "        else:\n",
    "            model_type = 'ensemble'\n",
    "            features_used = X_test[target].columns.tolist()\n",
    "        \n",
    "        # 保存特征信息\n",
    "        feature_info = {\n",
    "            'features': features_used,\n",
    "            'model_type': model_type\n",
    "        }\n",
    "        \n",
    "        feature_path = os.path.join('训练模型文件', f'{target}_{model_name}_features.pkl')\n",
    "        with open(feature_path, 'wb') as f:\n",
    "            pickle.dump(feature_info, f)\n",
    "        \n",
    "        # 同时为最佳模型保存一份特征信息\n",
    "        if model_name == best_composite_model:\n",
    "            best_feature_path = os.path.join('模型评估结果/最佳模型', f'{target}_最佳模型_features.pkl')\n",
    "            with open(best_feature_path, 'wb') as f:\n",
    "                pickle.dump(feature_info, f)\n",
    "\n",
    "    if target in evaluation_results and evaluation_results[target]:\n",
    "        # 获取每个指标的最佳模型\n",
    "        best_r2_model = max(evaluation_results[target], \n",
    "                          key=lambda m: evaluation_results[target][m]['r2'])\n",
    "        best_r2 = evaluation_results[target][best_r2_model]['r2']\n",
    "        \n",
    "        best_tol_r2_model = max(evaluation_results[target], \n",
    "                              key=lambda m: evaluation_results[target][m]['tolerance_r2'])\n",
    "        best_tol_r2 = evaluation_results[target][best_tol_r2_model]['tolerance_r2']\n",
    "        \n",
    "        best_within_tol_model = max(evaluation_results[target], \n",
    "                                  key=lambda m: evaluation_results[target][m]['within_tolerance'])\n",
    "        best_within_tol = evaluation_results[target][best_within_tol_model]['within_tolerance']\n",
    "        \n",
    "        # 添加到汇总数据\n",
    "        summary_data.append({\n",
    "            '目标变量': target,\n",
    "            '最佳R²模型': best_r2_model,\n",
    "            'R²值': best_r2,\n",
    "            '最佳容忍度R²模型': best_tol_r2_model,\n",
    "            '容忍度R²值': best_tol_r2,\n",
    "            '最佳容忍范围内模型': best_within_tol_model,\n",
    "            '容忍范围内比例': best_within_tol\n",
    "        })\n",
    "\n",
    "# 创建汇总DataFrame并保存\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    save_plot_data(summary_df, \"model_performance_summary\", \"模型性能汇总\")\n",
    "    \n",
    "    # 保存到Excel文件\n",
    "    summary_df.to_excel('模型评估结果/模型性能汇总.xlsx', index=False)\n",
    "    print(\"已创建模型性能汇总表\")\n",
    "\n",
    "print(\"\\n评估结果可视化完成\")\n",
    "print(\"\\n材料性能预测模型评估完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#           最终、功能对齐、完整解释性的 1-解释性机器学习.py 脚本\n",
    "# ==============================================================================\n",
    "\n",
    "# ====================== 1. 导入所需库 ======================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import shap\n",
    "import os\n",
    "import traceback\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ====================== 2. 核心修复与增强模块 ======================\n",
    "\n",
    "# --- 2.1 所有自定义模型类的定义 (保持不变) ---\n",
    "class CustomLGBMRegressor:\n",
    "    def __init__(self, **params): self.params, self.model = params, None\n",
    "    def fit(self, X, y):\n",
    "        X_values = X.values if hasattr(X, 'values') else X\n",
    "        train_data = lgb.Dataset(X_values, label=y, feature_name=[f'f{i}' for i in range(X.shape[1])])\n",
    "        self.model = lgb.train(self.params, train_data, num_boost_round=self.params.get('n_estimators', 100)); return self\n",
    "    def predict(self, X):\n",
    "        if self.model is None: raise ValueError(\"Model not trained.\")\n",
    "        return self.model.predict(X.values if hasattr(X, 'values') else X)\n",
    "\n",
    "class EnhancedVotingRegressor:\n",
    "    def __init__(self, estimators, weights, datasets, target_name):\n",
    "        self.estimators, self.weights, self.datasets, self.target_name = estimators, np.array(weights), datasets, target_name\n",
    "        if np.sum(self.weights) > 0: self.weights /= np.sum(self.weights)\n",
    "    def predict(self, X): return np.zeros(len(X) if hasattr(X, '__len__') else 1)\n",
    "\n",
    "class AdaptiveEnsembleModel: # (其他自定义类也应放在这里)\n",
    "    def __init__(self, meta_classifier, models_dict, available_models, model_input_data):\n",
    "        self.meta_classifier, self.models_dict, self.available_models, self.model_input_data = meta_classifier, models_dict, available_models, model_input_data\n",
    "        self.data_type_map = {'XGBoost': 'tree', 'LightGBM': 'tree', 'HistGradientBoosting': 'tree', 'RandomForest': 'tree_filled', 'GaussianProcess': 'linear'}\n",
    "    def predict(self, X): return np.zeros(len(X) if hasattr(X, '__len__') else 1)\n",
    "\n",
    "class WeightedAverageEnsemble:\n",
    "    def __init__(self, models_dict, model_names, weights, model_datasets):\n",
    "        self.models_dict, self.model_names, self.weights, self.model_datasets = models_dict, model_names, weights, model_datasets\n",
    "    def predict(self, X): return np.zeros(len(X) if hasattr(X, '__len__') else 1)\n",
    "\n",
    "\n",
    "# --- 2.2 统一预测函数 (保持不变) ---\n",
    "def predict_model_unified(model_obj, model_name, target, data_dict):\n",
    "    # (此函数内容与上一版回复一致，此处为保持完整性而包含)\n",
    "    if target not in data_dict['original']: return None\n",
    "    ensemble_map = {'VotingEnsemble': 'Voting', '投票集成': 'Voting', 'WeightedEnsemble': 'Weighted', '加权平均集成': 'Weighted', 'AdaptiveEnsemble': 'Adaptive', '自适应集成': 'Adaptive'}\n",
    "    ensemble_type = ensemble_map.get(model_name)\n",
    "    if ensemble_type == 'Voting':\n",
    "        base_models, weights, datasets = model_obj.estimators, model_obj.weights, model_obj.datasets\n",
    "        preds = {name: model.predict(data_dict[datasets.get(name, 'original')][target]) for name, model in base_models}\n",
    "        return np.average([preds[name] for name, _ in base_models], axis=0, weights=weights)\n",
    "    elif ensemble_type == 'Adaptive':\n",
    "        meta_classifier, models_dict, available_models, data_type_map = model_obj.meta_classifier, model_obj.models_dict, model_obj.available_models, model_obj.data_type_map\n",
    "        X_raw = data_dict['original'][target]\n",
    "        model_choices = meta_classifier.predict(X_raw)\n",
    "        predictions_dict = {mn: base_model.predict(data_dict[data_type_map.get(mn, 'original')][target]) for mn, base_model in zip(available_models, [models_dict[n] for n in available_models])}\n",
    "        return np.array([predictions_dict[available_models[choice]][i] for i, choice in enumerate(model_choices)])\n",
    "    else: # 普通模型或未明确处理的集成模型\n",
    "        model_map = {'XGBoost':'tree', 'LightGBM':'tree', 'HistGradientBoosting':'tree', 'RandomForest':'tree_filled', 'GaussianProcess':'linear'}\n",
    "        return model_obj.predict(data_dict[model_map.get(model_name, 'original')][target])\n",
    "        \n",
    "# --- 2.3 全新：通用预测包装器 (用于 SHAP 和 排列重要性) ---\n",
    "class UnifiedPredictor:\n",
    "    def __init__(self, model_obj, model_name, target, feature_names):\n",
    "        self.model = model_obj\n",
    "        self.model_name = model_name\n",
    "        self.target = target\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "    def predict(self, X_numpy):\n",
    "        # 将 SHAP 或 排列重要性 传入的 numpy 数组转换为带特征名的 DataFrame\n",
    "        X_df = pd.DataFrame(X_numpy, columns=self.feature_names)\n",
    "        \n",
    "        # 实时为这个新的 DataFrame 创建所有需要的预处理版本\n",
    "        data_dict = {\n",
    "            'original': {self.target: X_df},\n",
    "            'tree': {self.target: X_df.copy().replace(0, np.nan)},\n",
    "            'tree_filled': {self.target: X_df.copy().fillna(0)},\n",
    "            'linear': {self.target: X_df.copy()} # 假设线性模型需要特殊处理\n",
    "        }\n",
    "        # 模拟线性模型的预处理\n",
    "        for col in [c for c in X_df.columns if c.startswith('类别')]:\n",
    "            data_dict['linear'][self.target][col] /= 100.0\n",
    "\n",
    "        return predict_model_unified(self.model, self.model_name, self.target, data_dict)\n",
    "\n",
    "def sanitize_filename(filename): return \"\".join([c for c in filename if c.isalnum() or c in (' ', '_')]).rstrip()\n",
    "\n",
    "# ====================== 3. 全局设置和数据加载 ======================\n",
    "plt.style.use('ggplot'); sns.set(style=\"whitegrid\", font='SimHei'); plt.rcParams['axes.unicode_minus'] = False\n",
    "# (加载数据的代码与上一版一致)\n",
    "print(\"\\n\" + \"=\"*20, \"加载分析所需数据\", \"=\"*20)\n",
    "try:\n",
    "    data_folder, model_folder, eval_folder = 'data_exports', '训练模型文件', '模型评估结果'\n",
    "    X_test_dict = {\n",
    "        'original': pickle.load(open(os.path.join(data_folder, 'X_test.pkl'), 'rb')),\n",
    "        'linear': pickle.load(open(os.path.join(data_folder, 'X_test_linear.pkl'), 'rb')),\n",
    "        'tree': pickle.load(open(os.path.join(data_folder, 'X_test_tree.pkl'), 'rb')),\n",
    "        'tree_filled': pickle.load(open(os.path.join(data_folder, 'X_test_tree_filled.pkl'), 'rb')),\n",
    "    }\n",
    "    y_test = pickle.load(open(os.path.join(data_folder, 'y_test.pkl'), 'rb'))\n",
    "    feature_columns = pickle.load(open(os.path.join(data_folder, 'feature_columns.pkl'), 'rb'))\n",
    "    target_columns = pickle.load(open(os.path.join(data_folder, 'target_columns.pkl'), 'rb'))\n",
    "    evaluation_results = pickle.load(open(os.path.join(eval_folder, 'evaluation_results.pkl'), 'rb'))\n",
    "    print(\"✓ 所有数据加载成功\")\n",
    "except Exception as e:\n",
    "    print(f\"!!! 加载数据失败: {e}. 请确保已运行上游脚本。\"); exit()\n",
    "os.makedirs('模型解释', exist_ok=True); os.makedirs('模型解释/图表', exist_ok=True); os.makedirs('模型解释/数据', exist_ok=True)\n",
    "\n",
    "\n",
    "# ====================== 4. 分析函数定义 (已统一化) ======================\n",
    "\n",
    "def analyze_prediction_errors(y_true, y_pred, target, model_name):\n",
    "    print(f\"  - 正在分析预测误差...\")\n",
    "    y_true, y_pred = np.asarray(y_true).ravel(), np.asarray(y_pred).ravel()\n",
    "    r2, rmse, mae = r2_score(y_true, y_pred), np.sqrt(mean_squared_error(y_true, y_pred)), mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.6, color='blue')\n",
    "    min_val, max_val = min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='理想预测线 (y=x)')\n",
    "    plt.title(f'{target} - {model_name}\\n预测值 vs. 实际值', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('实际值', fontsize=12); plt.ylabel('预测值', fontsize=12)\n",
    "    stats_text = f'$R^2$ = {r2:.4f}\\nRMSE = {rmse:.4f}\\nMAE = {mae:.4f}'\n",
    "    plt.annotate(stats_text, xy=(0.05, 0.95), xycoords='axes fraction', fontsize=12, fontweight='bold',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", ec=\"gray\", alpha=0.8), verticalalignment='top')\n",
    "    plt.grid(True); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(f'模型解释/图表/{sanitize_filename(target)}_{model_name}_prediction_vs_actual.png', dpi=300); plt.close()\n",
    "\n",
    "def analyze_feature_importance(predictor, X, y, feature_names, target, model_name):\n",
    "    print(f\"  - 正在分析特征重要性 (使用排列重要性)...\")\n",
    "    try:\n",
    "        result = permutation_importance(predictor, X, y, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "        importance_df = pd.DataFrame({'特征': feature_names, '重要性': result.importances_mean})\n",
    "        importance_df = importance_df.sort_values(by='重要性', ascending=False).head(20)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(importance_df['特征'], importance_df['重要性'], color=sns.color_palette(\"viridis\", len(importance_df)))\n",
    "        plt.xlabel('特征重要性 (Permutation Importance)', fontsize=12); plt.ylabel('特征', fontsize=12)\n",
    "        plt.title(f'{target} - {model_name} 特征重要性', fontsize=16, fontweight='bold')\n",
    "        plt.gca().invert_yaxis(); plt.tight_layout()\n",
    "        plt.savefig(f'模型解释/图表/{sanitize_filename(target)}_{model_name}_feature_importance.png', dpi=300); plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"    分析特征重要性失败: {e}\")\n",
    "\n",
    "def analyze_shap_values(predictor, X, feature_names, target, model_name):\n",
    "    print(f\"  - 正在进行SHAP值分析 (这可能需要一些时间)...\")\n",
    "    try:\n",
    "        X_background = shap.sample(X, 50) # 使用50个样本作为背景数据\n",
    "        explainer = shap.KernelExplainer(predictor.predict, X_background)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "\n",
    "        # SHAP 摘要图\n",
    "        shap.summary_plot(shap_values, X, feature_names=feature_names, show=False)\n",
    "        plt.title(f'{target} - {model_name}\\nSHAP 值摘要', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'模型解释/图表/{sanitize_filename(target)}_{model_name}_shap_summary.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # SHAP 平均绝对值条形图\n",
    "        shap.summary_plot(shap_values, X, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "        plt.title(f'{target} - {model_name}\\n平均 |SHAP| 值', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'模型解释/图表/{sanitize_filename(target)}_{model_name}_shap_bar.png', dpi=300)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"    SHAP分析失败: {e}\")\n",
    "\n",
    "# ====================== 5. 主分析流程 ======================\n",
    "def main_analysis_flow():\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n启动模型解释性分析流程\\n\" + \"=\"*80)\n",
    "    for target in target_columns:\n",
    "        print(f\"\\n{'='*30} 分析目标: {target} {'='*30}\")\n",
    "        if not evaluation_results.get(target): continue\n",
    "        \n",
    "        try:\n",
    "            best_model_name = max(evaluation_results[target], key=lambda m: evaluation_results[target][m]['r2'])\n",
    "            print(f\"分析R²最佳模型: {best_model_name}\")\n",
    "        except (ValueError, KeyError): continue\n",
    "\n",
    "        model_path = os.path.join(model_folder, f'{target}_{best_model_name}模型.pkl')\n",
    "        if not os.path.exists(model_path): continue\n",
    "            \n",
    "        with open(model_path, 'rb') as f: model = pickle.load(f)\n",
    "\n",
    "        y_true = y_test[target]\n",
    "        X_for_analysis = X_test_dict['original'][target]\n",
    "        \n",
    "        # --- 创建通用预测包装器实例 ---\n",
    "        # 这个包装器将被传递给所有与模型无关的解释工具\n",
    "        predictor = UnifiedPredictor(model, best_model_name, target, feature_columns)\n",
    "        \n",
    "        # --- 生成预测并进行误差分析 ---\n",
    "        y_pred = predictor.predict(X_for_analysis.values)\n",
    "        analyze_prediction_errors(y_true, y_pred, target, best_model_name)\n",
    "        \n",
    "        # --- 统一调用特征重要性和SHAP分析 ---\n",
    "        analyze_feature_importance(predictor, X_for_analysis.values, feature_columns, y_true, target, best_model_name)\n",
    "        analyze_shap_values(predictor, X_for_analysis, feature_columns, target, best_model_name)\n",
    "        \n",
    "        print(f\"对 {target} 的分析完成。\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n所有分析流程已结束。\\n\" + \"=\"*80)\n",
    "\n",
    "# --- 脚本执行入口 ---\n",
    "if __name__ == '__main__':\n",
    "    main_analysis_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#           最终、功能完整、基于 gcastle 的 0-因果机器学习.py 脚本\n",
    "# ==============================================================================\n",
    "\n",
    "# ====================== 1. 导入所需库 ======================\n",
    "print(\"=\"*20, \"导入所需库\", \"=\"*20)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import traceback\n",
    "import networkx as nx\n",
    "\n",
    "# 导入 gcastle 用于因果发现\n",
    "try:\n",
    "    from gcastle.algorithms import PC\n",
    "    from gcastle.common import GraphDAG\n",
    "    from gcastle.common import pc_pc_lingam_cit as cit\n",
    "except ImportError:\n",
    "    print(\"!!! 错误: 找不到 gcastle 库。请先运行 'pip install gcastle' 进行安装。\")\n",
    "    exit()\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as lgb\n",
    "print(\"✓ 库导入完成\")\n",
    "\n",
    "\n",
    "# ====================== 2. 核心模块 (包含所有类定义和统一预测函数) ======================\n",
    "class CustomLGBMRegressor:\n",
    "    def __init__(self, **params): self.params, self.model = params, None\n",
    "    def fit(self, X, y):\n",
    "        X_values = X.values if hasattr(X, 'values') else X\n",
    "        train_data = lgb.Dataset(X_values, label=y, feature_name=[f'f{i}' for i in range(X.shape[1])])\n",
    "        self.model = lgb.train(self.params, train_data, num_boost_round=self.params.get('n_estimators', 100)); return self\n",
    "    def predict(self, X):\n",
    "        if self.model is None: raise ValueError(\"Model not trained.\")\n",
    "        return self.model.predict(X.values if hasattr(X, 'values') else X)\n",
    "\n",
    "class EnhancedVotingRegressor:\n",
    "    def __init__(self, estimators, weights, datasets, target_name):\n",
    "        self.estimators, self.weights, self.datasets, self.target_name = estimators, np.array(weights), datasets, target_name\n",
    "        if np.sum(self.weights) > 0: self.weights /= np.sum(self.weights)\n",
    "    def predict(self, X): return np.zeros(len(X) if hasattr(X, '__len__') else 1)\n",
    "\n",
    "class AdaptiveEnsembleModel:\n",
    "    def __init__(self, meta_classifier, models_dict, available_models, model_input_data):\n",
    "        self.meta_classifier, self.models_dict, self.available_models, self.model_input_data = meta_classifier, models_dict, available_models, model_input_data\n",
    "        self.data_type_map = {'XGBoost': 'tree', 'LightGBM': 'tree', 'HistGradientBoosting': 'tree', 'RandomForest': 'tree_filled', 'GaussianProcess': 'linear'}\n",
    "    def predict(self, X): return np.zeros(len(X) if hasattr(X, '__len__') else 1)\n",
    "\n",
    "class WeightedAverageEnsemble:\n",
    "    def __init__(self, models_dict, model_names, weights, model_datasets):\n",
    "        self.models_dict, self.model_names, self.weights, self.model_datasets = models_dict, model_names, weights, model_datasets\n",
    "    def predict(self, X): return np.zeros(len(X) if hasattr(X, '__len__') else 1)\n",
    "\n",
    "def predict_model_unified(model_obj, model_name, target, data_dict):\n",
    "    if target not in data_dict['original']: return None\n",
    "    ensemble_map = {'VotingEnsemble': 'Voting', '投票集成': 'Voting'}\n",
    "    ensemble_type = ensemble_map.get(model_name)\n",
    "    if ensemble_type == 'Voting':\n",
    "        base_models, weights, datasets = model_obj.estimators, model_obj.weights, model_obj.datasets\n",
    "        preds = {name: model.predict(data_dict[datasets.get(name, 'original')][target]) for name, model in base_models}\n",
    "        return np.average([preds[name] for name, _ in base_models], axis=0, weights=weights)\n",
    "    else:\n",
    "        model_map = {'XGBoost':'tree', 'LightGBM':'tree', 'HistGradientBoosting':'tree', 'RandomForest':'tree_filled', 'GaussianProcess':'linear'}\n",
    "        return model_obj.predict(data_dict[model_map.get(model_name, 'original')][target])\n",
    "\n",
    "\n",
    "# ====================== 3. 全局设置和数据加载 ======================\n",
    "print(\"\\n\" + \"=\"*20, \"全局设置和数据加载\", \"=\"*20)\n",
    "plt.style.use('ggplot'); sns.set(style=\"whitegrid\", font='SimHei'); plt.rcParams['axes.unicode_minus'] = False\n",
    "try:\n",
    "    data_folder, model_folder, eval_folder = 'data_exports', '训练模型文件', '模型评估结果'\n",
    "    X_train_dict = {\n",
    "        'original': pickle.load(open(os.path.join(data_folder, 'X_train.pkl'), 'rb')),\n",
    "        'linear': pickle.load(open(os.path.join(data_folder, 'X_train_linear.pkl'), 'rb')),\n",
    "        'tree': pickle.load(open(os.path.join(data_folder, 'X_train_tree.pkl'), 'rb')),\n",
    "        'tree_filled': pickle.load(open(os.path.join(data_folder, 'X_train_tree_filled.pkl'), 'rb')),\n",
    "    }\n",
    "    y_train = pickle.load(open(os.path.join(data_folder, 'y_train.pkl'), 'rb'))\n",
    "    best_models = {target: pickle.load(open(os.path.join(eval_folder, '最佳模型', f'{target}_最佳模型.pkl'), 'rb')) for target in ['水接触角', '循环使用次数', '吸油能力']}\n",
    "    feature_columns = pickle.load(open(os.path.join(data_folder, 'feature_columns.pkl'), 'rb'))\n",
    "    target_columns = pickle.load(open(os.path.join(data_folder, 'target_columns.pkl'), 'rb'))\n",
    "    print(\"✓ 所有必需的数据和最佳模型加载成功\")\n",
    "except Exception as e:\n",
    "    print(f\"!!! 加载数据失败: {e}. 请确保已运行上游脚本。\"); exit()\n",
    "\n",
    "feature_rename = {'类别1_无机纳米材料/金属氧化物': '无机材料', '类别2_有机高分子/聚合物': '有机高分子', '类别3_表面改性剂/硅烷类物质': '表面改性剂', '类别4_碳基材料': '碳基材料', '类别5_MOF/功能有机小分子/其他': '功能分子', '制备方法_编码': '制备方法', '基底材料_编码': '基底材料'}\n",
    "short_names = {**feature_rename, **{'水接触角': '接触角', '循环使用次数': '循环次数', '吸油能力': '吸油能力'}}\n",
    "os.makedirs('因果分析结果', exist_ok=True); os.makedirs('因果分析结果/因果图', exist_ok=True); os.makedirs('因果分析结果/敏感度分析', exist_ok=True); os.makedirs('因果分析结果/反事实分析', exist_ok=True)\n",
    "\n",
    "\n",
    "# ====================== 4. 分析函数定义 ======================\n",
    "\n",
    "def run_causal_discovery(data, target_name, feature_names):\n",
    "    print(f\"  - 正在为 '{target_name}' 运行因果发现 (gcastle.PC)...\")\n",
    "    try:\n",
    "        data_np = data.to_numpy()\n",
    "        causal_model = PC()\n",
    "        causal_model.learn(data_np)\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        node_labels = {i: short_names.get(col, col) for i, col in enumerate(data.columns)}\n",
    "        GraphDAG(causal_model.causal_matrix, labels=[node_labels[i] for i in range(len(node_labels))]).plot()\n",
    "        \n",
    "        plt.title(f'为 \"{short_names.get(target_name, target_name)}\" 学习到的因果关系图 (gcastle.PC)', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'因果分析结果/因果图/{target_name}_因果图.png', dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"  ✓ 因果图已保存\")\n",
    "    except Exception as e:\n",
    "        print(f\"  !!! 因果发现失败: {e}\")\n",
    "\n",
    "def run_sensitivity_analysis(model, model_name, X_train_orig, target, feature_names):\n",
    "    print(f\"  - 正在为 '{target}' 进行模型敏感度分析 (智能采样)...\")\n",
    "    \n",
    "    # 使用所有特征进行分析\n",
    "    for feature in feature_names:\n",
    "        X_ref = pd.DataFrame([X_train_orig.median()])\n",
    "        train_vals = X_train_orig[feature].dropna()\n",
    "        if train_vals.empty: continue\n",
    "        \n",
    "        f_min, f_max = train_vals.min(), train_vals.max()\n",
    "        if f_max - f_min < 1e-6: f_max = f_min + 1 # 避免范围为0\n",
    "\n",
    "        def get_pred_at_value(value):\n",
    "            cf_sample = X_ref.copy(); cf_sample[feature] = value\n",
    "            data_dict = {'original': {target: cf_sample}, 'tree': {target: cf_sample.copy().replace(0, np.nan)}, \n",
    "                         'tree_filled': {target: cf_sample.copy().fillna(0)}, 'linear': {target: cf_sample.copy()}}\n",
    "            for col in [c for c in cf_sample.columns if c.startswith('类别')]: data_dict['linear'][target][col] /= 100.0\n",
    "            return predict_model_unified(model, model_name, target, data_dict)[0]\n",
    "\n",
    "        initial_points = np.linspace(f_min, f_max, 20)\n",
    "        initial_preds = [(v, get_pred_at_value(v)) for v in initial_points]\n",
    "        refined_values = set(initial_points)\n",
    "        std_dev = np.std([p[1] for p in initial_preds])\n",
    "        for i in range(len(initial_preds) - 1):\n",
    "            x1, y1 = initial_preds[i]; x2, y2 = initial_preds[i+1]\n",
    "            gradient = abs(y2-y1)/(x2-x1) if x2!=x1 else 0\n",
    "            if gradient > std_dev and std_dev > 1e-6:\n",
    "                refined_values.update(np.linspace(x1, x2, 5))\n",
    "        \n",
    "        all_values = sorted(list(refined_values))\n",
    "        final_preds = [(v, get_pred_at_value(v)) for v in all_values]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        x_vals, y_vals = zip(*final_preds)\n",
    "        plt.plot(x_vals, y_vals, '-o', color='#f28e2c', markersize=4, linewidth=2)\n",
    "        \n",
    "        best_idx = np.argmax(y_vals)\n",
    "        best_x, best_y = x_vals[best_idx], y_vals[best_idx]\n",
    "        plt.scatter(best_x, best_y, color='red', s=120, zorder=10, label=f'最佳预测点\\n({best_x:.2f}, {best_y:.2f})', ec='black')\n",
    "\n",
    "        plt.title(f'\"{short_names.get(feature, feature)}\" 对 \"{short_names.get(target, target)}\" 的敏感度', fontsize=16)\n",
    "        plt.xlabel(f'特征 \"{short_names.get(feature, feature)}\" 的值 (预处理后)', fontsize=12)\n",
    "        plt.ylabel(f'模型预测的 \"{short_names.get(target, target)}\"', fontsize=12)\n",
    "        plt.grid(True, linestyle='--'); plt.legend(); plt.tight_layout()\n",
    "        plt.savefig(f'因果分析结果/敏感度分析/{target}_{short_names.get(feature, feature).replace(\"/\", \"_\")}_敏感度分析.png', dpi=300)\n",
    "        plt.close()\n",
    "    print(f\"  ✓ 敏感度分析图已保存\")\n",
    "\n",
    "def run_counterfactual_analysis(model, model_name, X_train_orig, target, feature_names):\n",
    "    print(f\"  - 正在为 '{target}' 进行反事实分析 (保留原逻辑)...\")\n",
    "    # --- 此处为您的原版反事实分析代码，已适配新数据加载方式 ---\n",
    "    X_ref = pd.DataFrame([X_train_orig.median()])\n",
    "    important_features = feature_names[:5] # 仅分析最重要的5个特征\n",
    "    cf_results = {}\n",
    "    \n",
    "    for feature in important_features:\n",
    "        train_vals = X_train_orig[feature].dropna()\n",
    "        if train_vals.empty: continue\n",
    "        feature_min, feature_max = train_vals.min(), train_vals.max()\n",
    "        if feature_max - feature_min < 1e-6: feature_max = feature_min + 1\n",
    "            \n",
    "        all_values = np.linspace(feature_min, feature_max, 50)\n",
    "        \n",
    "        def get_pred_at_value_cf(value):\n",
    "             cf_sample = X_ref.copy(); cf_sample[feature] = value\n",
    "             data_dict = {'original': {target: cf_sample}, 'tree': {target: cf_sample.copy().replace(0, np.nan)}, \n",
    "                          'tree_filled': {target: cf_sample.copy().fillna(0)}, 'linear': {target: cf_sample.copy()}}\n",
    "             for col in [c for c in cf_sample.columns if c.startswith('类别')]: data_dict['linear'][target][col] /= 100.0\n",
    "             return predict_model_unified(model, model_name, target, data_dict)[0]\n",
    "\n",
    "        predictions = [(v, get_pred_at_value_cf(v)) for v in all_values]\n",
    "        cf_results[feature] = predictions\n",
    "\n",
    "    # 可视化\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, feature in enumerate(important_features):\n",
    "        if feature not in cf_results: continue\n",
    "        predictions = cf_results[feature]\n",
    "        x_vals, y_vals = zip(*predictions)\n",
    "        plt.plot(x_vals, y_vals, '-', label=short_names.get(feature, feature), linewidth=2)\n",
    "    \n",
    "    plt.xlabel('特征值（预处理后）', fontsize=12); plt.ylabel(f'预测的{short_names.get(target, target)}', fontsize=12)\n",
    "    plt.title(f'{target}的反事实分析', fontsize=16); plt.grid(True, linestyle='--'); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(f'因果分析结果/反事实分析/{target}_反事实分析.png', dpi=300); plt.close()\n",
    "    print(f\"  ✓ 反事实分析图已保存\")\n",
    "\n",
    "# ====================== 5. 主分析流程 ======================\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n启动因果机器学习分析流程\\n\" + \"=\"*80)\n",
    "    for target in target_columns:\n",
    "        print(f\"\\n{'='*30} 分析目标: {target} {'='*30}\")\n",
    "        if target not in best_models:\n",
    "            print(f\"未找到 {target} 的最佳模型，跳过。\"); continue\n",
    "            \n",
    "        model = best_models[target]\n",
    "        model_name = \"最佳集成模型\" if \"Ensemble\" in model.__class__.__name__ else model.__class__.__name__\n",
    "\n",
    "        # 准备数据\n",
    "        X_train_target = X_train_dict['original'][target]\n",
    "        y_train_target = y_train[target]\n",
    "        data_for_discovery = X_train_target.join(y_train_target).dropna()\n",
    "\n",
    "        # 1. 运行因果发现\n",
    "        run_causal_discovery(data_for_discovery, target, list(data_for_discovery.columns))\n",
    "\n",
    "        # 2. 运行模型敏感度分析\n",
    "        run_sensitivity_analysis(model, model_name, X_train_target, target, feature_columns)\n",
    "        \n",
    "        # 3. 运行反事实分析\n",
    "        run_counterfactual_analysis(model, model_name, X_train_target, target, feature_columns)\n",
    "        \n",
    "        print(f\"对 '{target}' 的分析完成。\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n所有分析流程已结束。\\n\" + \"=\"*80)\n",
    "\n",
    "# --- 脚本执行入口 ---\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 材料性能多方法集成优化 ======================\n",
    "print(\"=\" * 40)\n",
    "print(\"材料性能多方法集成优化\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 导入所需库\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Real\n",
    "from skopt.utils import use_named_args\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from pymoo.core.problem import Problem\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# 忽略特定警告\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# 创建优化结果保存目录\n",
    "os.makedirs('optimization_results', exist_ok=True)\n",
    "\n",
    "# 加载必要数据和模型\n",
    "print(\"加载模型和数据...\")\n",
    "\n",
    "# 加载特征数据\n",
    "feature_data = pd.read_excel('data_exports/material_features_numeric.xlsx')\n",
    "print(f\"成功加载特征数据: {feature_data.shape[0]} 行，{feature_data.shape[1]} 列\")\n",
    "\n",
    "# 尝试加载预处理后的数据(如果存在)\n",
    "try:\n",
    "    processed_data = pd.read_excel('data_exports/processed_data.xlsx')\n",
    "    print(\"成功加载预处理后的特征数据\")\n",
    "except:\n",
    "    processed_data = None\n",
    "    print(\"未找到预处理数据，将使用原始特征数据\")\n",
    "\n",
    "# 加载目标变量的最佳模型信息\n",
    "best_model = {}\n",
    "for target in target_columns:\n",
    "    try:\n",
    "        # 从模型评估结果目录加载最佳模型信息\n",
    "        target_best_model_path = f'模型评估结果/{sanitize_filename(target)}_best_models.pkl'\n",
    "        if os.path.exists(target_best_model_path):\n",
    "            with open(target_best_model_path, 'rb') as f:\n",
    "                target_model_info = pickle.load(f)\n",
    "                \n",
    "            # 转换为优化代码期望的格式\n",
    "            best_model[target] = {\n",
    "                'tolerance_r2': {\n",
    "                    'model': target_model_info['best_tol_r2_model']\n",
    "                }\n",
    "            }\n",
    "            print(f\"成功加载 {target} 的最佳模型信息\")\n",
    "        else:\n",
    "            print(f\"未找到 {target} 的最佳模型信息文件: {target_best_model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"加载 {target} 的最佳模型信息失败: {str(e)}\")\n",
    "\n",
    "# 加载目标容忍度信息\n",
    "if os.path.exists('训练模型文件/target_tolerance_info.pkl'):\n",
    "    with open('训练模型文件/target_tolerance_info.pkl', 'rb') as f:\n",
    "        target_tolerance = pickle.load(f)\n",
    "    print(\"加载目标容忍度信息成功\")\n",
    "else:\n",
    "    # 创建默认目标容忍度\n",
    "    target_tolerance = {\n",
    "        '水接触角': 0.2,\n",
    "        '循环使用次数': 0.3,\n",
    "        '吸油能力': 0.3\n",
    "    }\n",
    "    print(\"使用默认目标容忍度值\")\n",
    "\n",
    "# 定义误差容忍的评估函数 - 与模型训练部分一致\n",
    "def tolerance_r2_score(y_true, y_pred, tolerance=0.15, target=None):\n",
    "    \"\"\"\n",
    "    计算容忍度R²评分，允许一定误差范围内的预测被视为准确\n",
    "    \"\"\"\n",
    "    # 确保输入数据是numpy数组并且形状正确\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    \n",
    "    # 如果提供了目标变量名，则使用目标特定的容忍度\n",
    "    if target and target in target_tolerance:\n",
    "        tolerance = target_tolerance[target]\n",
    "    \n",
    "    # 计算容忍范围\n",
    "    tolerance_values = tolerance * np.abs(y_true)\n",
    "    # 计算残差\n",
    "    residuals = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # 调整残差，使误差在容忍范围内的视为0\n",
    "    adjusted_residuals = np.maximum(0, residuals - tolerance_values)\n",
    "    \n",
    "    # 计算修正后的总平方和\n",
    "    y_true_mean = np.mean(y_true)\n",
    "    tss = np.sum((y_true - y_true_mean) ** 2)\n",
    "    \n",
    "    # 计算修正后的残差平方和\n",
    "    rss = np.sum(adjusted_residuals ** 2)\n",
    "    \n",
    "    # 计算修正后的R²\n",
    "    if tss == 0:\n",
    "        return 0  # 防止除以0\n",
    "    \n",
    "    tolerance_r2 = 1 - (rss / tss)\n",
    "    return tolerance_r2\n",
    "\n",
    "def prediction_within_tolerance(y_true, y_pred, tolerance=0.15, target=None):\n",
    "    \"\"\"\n",
    "    计算预测值在目标值±容忍范围内的比例\n",
    "    \"\"\"\n",
    "    # 确保输入为numpy数组\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    # 如果提供了目标变量名，则使用目标特定的容忍度\n",
    "    if target and target in target_tolerance:\n",
    "        tolerance = target_tolerance[target]\n",
    "    \n",
    "    # 计算容忍范围\n",
    "    tolerance_values = tolerance * np.abs(y_true)\n",
    "    \n",
    "    # 检查预测是否在容忍范围内\n",
    "    within_tolerance = np.abs(y_true - y_pred) <= tolerance_values\n",
    "    \n",
    "    # 计算在容忍范围内的预测比例\n",
    "    return np.mean(within_tolerance)\n",
    "\n",
    "# 识别特征列和目标列\n",
    "target_columns = feature_data.columns[-3:].tolist()  # 假设最后三列是目标列\n",
    "feature_columns = feature_data.columns[:-3].tolist()  # 特征列\n",
    "\n",
    "# 类别相关列\n",
    "category_columns = [col for col in feature_columns if col.startswith('类别')]\n",
    "method_column = '制备方法_编码'\n",
    "base_material_column = '基底材料_编码'\n",
    "\n",
    "print(\"目标变量:\", target_columns)\n",
    "print(\"特征变量总数:\", len(feature_columns))\n",
    "print(\"类别特征数:\", len(category_columns))\n",
    "\n",
    "# 进度条类 - 保持原有定义\n",
    "class CustomProgressBar:\n",
    "    def __init__(self, total, desc=\"进度\", bar_length=50):\n",
    "        self.total = total\n",
    "        self.desc = desc\n",
    "        self.n = 0\n",
    "        self.bar_length = bar_length\n",
    "        self.start_time = time.time()\n",
    "        self.last_print_time = 0\n",
    "        self._print_progress()\n",
    "        \n",
    "    def update(self, n=1):\n",
    "        self.n += n\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_print_time >= 0.1 or self.n >= self.total:\n",
    "            self._print_progress()\n",
    "            self.last_print_time = current_time\n",
    "    \n",
    "    def _print_progress(self):\n",
    "        percent = min(100, self.n * 100 / self.total)\n",
    "        filled_length = int(self.bar_length * self.n // self.total)\n",
    "        bar = '█' * filled_length + '-' * (self.bar_length - filled_length)\n",
    "        \n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        if self.n > 0:\n",
    "            time_per_iter = elapsed_time / self.n\n",
    "            remaining_iters = self.total - self.n\n",
    "            remaining_time = time_per_iter * remaining_iters\n",
    "            time_str = f\" - 预计剩余: {self._format_time(remaining_time)}\"\n",
    "        else:\n",
    "            time_str = \"\"\n",
    "            \n",
    "        print(f'\\r{self.desc}: |{bar}| {percent:.1f}% {self.n}/{self.total}{time_str}', end='', flush=True)\n",
    "        if self.n >= self.total:\n",
    "            print()\n",
    "    \n",
    "    def _format_time(self, seconds):\n",
    "        \"\"\"将秒数格式化为时:分:秒\"\"\"\n",
    "        m, s = divmod(int(seconds), 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        if h > 0:\n",
    "            return f\"{h}时{m}分{s}秒\"\n",
    "        elif m > 0:\n",
    "            return f\"{m}分{s}秒\"\n",
    "        else:\n",
    "            return f\"{s}秒\"\n",
    "            \n",
    "    def set_description(self, desc):\n",
    "        \"\"\"更新描述文字\"\"\"\n",
    "        self.desc = desc\n",
    "        self._print_progress()\n",
    "        \n",
    "    def close(self):\n",
    "        \"\"\"关闭进度条\"\"\"\n",
    "        if self.n < self.total:\n",
    "            self.n = self.total\n",
    "            self._print_progress()\n",
    "        print()\n",
    "\n",
    "# ====================== 材料空间定义 ======================\n",
    "\n",
    "# 提取材料空间中的唯一元素\n",
    "def extract_material_space():\n",
    "    \"\"\"从训练数据中提取可用的材料和制备方法\"\"\"\n",
    "    print(\"提取材料空间信息...\")\n",
    "    \n",
    "    # 初始化材料类型和方法集合\n",
    "    unique_materials = {\n",
    "        'base': set(),   # 基底材料\n",
    "        'category': set()  # 类别材料\n",
    "    }\n",
    "    unique_methods = set()  # 制备方法\n",
    "    \n",
    "    # 从特征列名提取类别\n",
    "    for col in category_columns:\n",
    "        if '_' in col:\n",
    "            category = col.split('_')[1]  # 提取类别名称\n",
    "            unique_materials['category'].add(category)\n",
    "    \n",
    "    # 从基底材料编码提取唯一值\n",
    "    base_materials = feature_data[base_material_column].dropna().unique()\n",
    "    for base in base_materials:\n",
    "        if pd.notna(base):\n",
    "            unique_materials['base'].add(int(base))  # 确保基底材料是整数\n",
    "    \n",
    "    # 从制备方法编码提取唯一值\n",
    "    methods = feature_data[method_column].dropna().unique()\n",
    "    for method in methods:\n",
    "        if pd.notna(method):\n",
    "            unique_methods.add(int(method))  # 确保制备方法是整数\n",
    "    \n",
    "    # 转换为排序后的列表\n",
    "    materials_dict = {\n",
    "        'base': sorted(list(unique_materials['base'])),\n",
    "        'category': sorted(list(unique_materials['category']))\n",
    "    }\n",
    "    methods_list = sorted(list(unique_methods))\n",
    "    \n",
    "    print(f\"材料空间: {len(materials_dict['base'])} 种基底材料, \"\n",
    "          f\"{len(materials_dict['category'])} 种类别材料, \"\n",
    "          f\"{len(methods_list)} 种制备方法\")\n",
    "    \n",
    "    return materials_dict, methods_list\n",
    "\n",
    "# 提取材料空间\n",
    "unique_materials, unique_methods = extract_material_space()\n",
    "\n",
    "# 预测函数与特征生成\n",
    "def create_material_features(base_material, categories, prep_method, features_template=None):\n",
    "    \"\"\"为给定的材料组合创建特征向量\"\"\"\n",
    "    # 如果未提供特征模板，从训练数据中获取特征列\n",
    "    if features_template is None:\n",
    "        # 获取第一个目标使用的特征列作为模板\n",
    "        if target_columns[0] in best_model:\n",
    "            model_info = best_model[target_columns[0]]\n",
    "            model_name = model_info['tolerance_r2']['model']\n",
    "            try:\n",
    "                # 尝试加载特征信息\n",
    "                feature_path = f'训练模型文件/{target_columns[0]}_{model_name}_features.pkl'\n",
    "                if os.path.exists(feature_path):\n",
    "                    with open(feature_path, 'rb') as f:\n",
    "                        feature_info = pickle.load(f)\n",
    "                    feature_cols = feature_info.get('features', feature_columns)\n",
    "                else:\n",
    "                    feature_cols = feature_columns\n",
    "            except Exception as e:\n",
    "                print(f\"无法加载特征模板: {str(e)}\")\n",
    "                feature_cols = feature_columns\n",
    "        else:\n",
    "            feature_cols = feature_columns\n",
    "    else:\n",
    "        feature_cols = features_template\n",
    "    \n",
    "    # 创建特征字典并初始化为0\n",
    "    features = {}\n",
    "    for col in feature_cols:\n",
    "        features[col] = 0.0\n",
    "    \n",
    "    # 设置基底材料特征\n",
    "    features[base_material_column] = float(base_material)\n",
    "        \n",
    "    # 设置制备方法特征\n",
    "    features[method_column] = float(prep_method)\n",
    "    \n",
    "    # 设置类别特征\n",
    "    for cat_id, cat_value in categories:\n",
    "        # 查找对应的特征列\n",
    "        for col in category_columns:\n",
    "            if f\"_{cat_id}\" in col:\n",
    "                features[col] = float(cat_value)  # 设置类别材料值\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame([features])\n",
    "    \n",
    "    # 确保列的顺序与特征列表一致\n",
    "    for col in feature_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "    \n",
    "    df = df[feature_cols]\n",
    "    \n",
    "    return df\n",
    "def predict_performance(target, features):\n",
    "    \"\"\"预测给定材料组合的性能\"\"\"\n",
    "    if target not in best_model:\n",
    "        print(f\"{target}不在best_model字典中\")\n",
    "        return None\n",
    "\n",
    "    # 使用正确的键路径获取模型名称\n",
    "    model_name = best_model[target]['tolerance_r2']['model']\n",
    "    \n",
    "    \n",
    "    # 确定模型类型\n",
    "    if model_name in [\"XGBoost\", \"LightGBM\", \"HistGradientBoosting\"]:\n",
    "        model_type = 'tree'\n",
    "    elif model_name == \"RandomForest\":\n",
    "        model_type = 'tree_filled'\n",
    "    elif model_name in [\"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"HuberRegressor\", \"GaussianProcess\"]:\n",
    "        model_type = 'linear'\n",
    "    elif model_name in [\"DeepNN\"]:\n",
    "        model_type = 'nn'\n",
    "    else:\n",
    "        model_type = 'ensemble'\n",
    "    \n",
    "    \n",
    "    # 加载最佳模型\n",
    "    model_path = f'模型评估结果/最佳模型/{target}_最佳模型.pkl'\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    # 识别类别列\n",
    "    category_columns = [col for col in features.columns if col.startswith('类别')]\n",
    "    \n",
    "    # 应用与训练相同的预处理\n",
    "    features_processed = features.copy()\n",
    "    if model_type == 'linear':\n",
    "        # 线性模型: 类别编码除以1000\n",
    "        for col in category_columns:\n",
    "            features_processed[col] = features_processed[col] / 1000.0\n",
    "    elif model_type == 'tree':\n",
    "        # 树模型: 将0替换为NaN\n",
    "        for col in category_columns:\n",
    "            features_processed[col] = features_processed[col].replace(0, np.nan)\n",
    "    \n",
    "    # 预定义的特征列\n",
    "    expected_features = category_columns + ['制备方法_编码', '基底材料_编码']\n",
    "    \n",
    "    # 确保所有需要的特征列都存在\n",
    "    for feat in expected_features:\n",
    "        if feat not in features_processed.columns:\n",
    "            features_processed[feat] = 0.0\n",
    "    \n",
    "    # 使用预期的特征列\n",
    "    features_for_model = features_processed[expected_features]\n",
    "    \n",
    "    \n",
    "    # 进行预测\n",
    "    if isinstance(model, dict) and 'model' in model:\n",
    "        if model.get('needs_scaling', False) and 'scaler' in model:\n",
    "            X_scaled = model['scaler'].transform(features_for_model)\n",
    "            pred = model['model'].predict(X_scaled)[0]\n",
    "        else:\n",
    "            pred = model['model'].predict(features_for_model)[0]\n",
    "    else:\n",
    "        # XGBoost模型可能需要特殊处理\n",
    "        if str(type(model)).find('xgboost') > -1:\n",
    "            try:\n",
    "                import xgboost as xgb\n",
    "                with xgb.config_context(verbosity=0, predict_disable_shape_check=True):\n",
    "                    pred = model.predict(features_for_model)[0]\n",
    "            except:\n",
    "                pred = model.predict(features_for_model)[0]\n",
    "        else:\n",
    "            pred = model.predict(features_for_model)[0]\n",
    "    \n",
    "    return pred\n",
    "\n",
    "# 文件名净化函数\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"将文件名中的无效字符替换为下划线\"\"\"\n",
    "    if filename is None:\n",
    "        return \"unknown\"\n",
    "    invalid_chars = ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|']\n",
    "    sanitized = str(filename)\n",
    "    for char in invalid_chars:\n",
    "        sanitized = sanitized.replace(char, '_')\n",
    "    return sanitized\n",
    "\n",
    "# 计算材料组合的综合评分\n",
    "def calculate_overall_score(base_material, categories, prep_method):\n",
    "    \"\"\"计算给定材料组合的综合评分\"\"\"\n",
    "    # 构建类别形式\n",
    "    categories_formatted = []\n",
    "    for cat_id, cat_value in categories:\n",
    "        if cat_value > 0:  # 只添加存在的材料\n",
    "            categories_formatted.append((cat_id, cat_value))\n",
    "    \n",
    "    if not categories_formatted:\n",
    "        print(\"没有有效的类别材料\")\n",
    "        return 0.0, {}  # 如果没有有效材料，返回0分\n",
    "    \n",
    "    # 创建原始编码特征\n",
    "    features = create_material_features(base_material, categories_formatted, prep_method)\n",
    "    \n",
    "    # 预测各指标性能\n",
    "    predictions = {}\n",
    "    for target in target_columns:\n",
    "        if target in best_model:\n",
    "            # 使用修改后的预测函数\n",
    "            pred = predict_performance(target, features)\n",
    "            if pred is not None:\n",
    "                predictions[target] = pred\n",
    "                #print(f\"预测 {target}: {pred:.2f}\")\n",
    "    \n",
    "    # 如果没有有效预测，返回0分\n",
    "    if not predictions:\n",
    "        print(\"没有有效的预测结果\")\n",
    "        return 0.0, {}\n",
    "    \n",
    "    # 根据因果分析结果调整权重\n",
    "    weights = {}\n",
    "    total_weight = 0.0\n",
    "    \n",
    "    for target in predictions.keys():\n",
    "        if target == '水接触角':\n",
    "            weights[target] = 1.0\n",
    "        elif target == '循环使用次数':\n",
    "            weights[target] = 1.2  # 略微提高权重\n",
    "        elif target == '吸油能力':\n",
    "            weights[target] = 1.3  # 更高权重\n",
    "        else:\n",
    "            weights[target] = 1.0\n",
    "        total_weight += weights[target]\n",
    "    \n",
    "    # 计算加权综合得分\n",
    "    score = 0.0\n",
    "    \n",
    "    for target, value in predictions.items():\n",
    "        # 归一化特定指标的得分\n",
    "        if target == '水接触角':\n",
    "            # 水接触角范围通常为90-150度，值越大越好\n",
    "            norm_score = min(1.0, max(0.0, value / 180.0))\n",
    "        elif target == '循环使用次数':\n",
    "            # 循环使用次数通常为10-50次，值越大越好\n",
    "            norm_score = min(1.0, max(0.0, value / 50.0))\n",
    "        elif target == '吸油能力':\n",
    "            # 吸油能力通常为5-50 g/g，值越大越好\n",
    "            norm_score = min(1.0, max(0.0, value / 50.0))\n",
    "        else:\n",
    "            norm_score = min(1.0, max(0.0, value / 100.0))\n",
    "        \n",
    "        #print(f\"{target} 归一化得分: {norm_score:.4f}, 权重: {weights[target]/total_weight:.2f}\")\n",
    "            \n",
    "        # 加权求和\n",
    "        if total_weight > 0:\n",
    "            score += (weights[target] / total_weight) * norm_score\n",
    "    \n",
    "    #print(f\"总综合得分: {score:.4f}\")\n",
    "    return score, predictions\n",
    "\n",
    "# ====================== 材料编码/解码函数 ======================\n",
    "\n",
    "def one_hot_encode_material(base_idx, method_idx, categories):\n",
    "    \"\"\"创建材料组合的独热编码\"\"\"\n",
    "    # 基底材料编码\n",
    "    base_encoding = np.zeros(len(unique_materials['base']))\n",
    "    base_encoding[base_idx] = 1\n",
    "    \n",
    "    # 制备方法编码\n",
    "    method_encoding = np.zeros(len(unique_methods))\n",
    "    method_encoding[method_idx] = 1\n",
    "    \n",
    "    # 类别材料编码 - 每个类别一个热独编码位置\n",
    "    category_encoding = np.zeros(5)  # 5个类别\n",
    "    for cat_id, cat_value in categories:\n",
    "        if 1 <= cat_id <= 5:  # 确保类别ID在有效范围内\n",
    "            category_encoding[cat_id-1] = cat_value\n",
    "    \n",
    "    # 组合所有编码\n",
    "    return np.concatenate([base_encoding, method_encoding, category_encoding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= 1. 贝叶斯优化材料搜索 =======\n",
    "print(\"\\n1. 执行贝叶斯优化...\")\n",
    "\n",
    "try:\n",
    "    # 为贝叶斯优化定义目标函数\n",
    "    @use_named_args(dimensions=[\n",
    "        Integer(0, len(unique_materials['base']) - 1, name='base_idx'),\n",
    "        Integer(0, len(unique_methods) - 1, name='method_idx'),\n",
    "        Integer(1, 5, name='num_categories'),  # 1-5个类别\n",
    "        Integer(1, 5, name='cat1_id'),\n",
    "        Real(100, 699, name='cat1_val'),\n",
    "        Integer(1, 5, name='cat2_id'),\n",
    "        Real(100, 699, name='cat2_val'),\n",
    "        Integer(1, 5, name='cat3_id'),\n",
    "        Real(100, 699, name='cat3_val'),\n",
    "        Integer(1, 5, name='cat4_id'),\n",
    "        Real(100, 699, name='cat4_val'),\n",
    "        Integer(1, 5, name='cat5_id'),\n",
    "        Real(100, 699, name='cat5_val')\n",
    "    ])\n",
    "    def bayesian_objective(**params):\n",
    "        # 解析参数\n",
    "        base_idx = params['base_idx']\n",
    "        method_idx = params['method_idx']\n",
    "        num_categories = params['num_categories']\n",
    "        \n",
    "        # 转换为实际材料\n",
    "        base_material = unique_materials['base'][base_idx]\n",
    "        prep_method = unique_methods[method_idx]\n",
    "        \n",
    "        # 提取类别材料\n",
    "        categories = []\n",
    "        for i in range(1, num_categories+1):\n",
    "            cat_id = params[f'cat{i}_id']\n",
    "            cat_val = params[f'cat{i}_val']\n",
    "            categories.append((cat_id, cat_val))\n",
    "        \n",
    "        # 确保没有重复类别\n",
    "        seen_categories = set()\n",
    "        unique_categories = []\n",
    "        for cat_id, cat_val in categories:\n",
    "            if cat_id not in seen_categories:\n",
    "                seen_categories.add(cat_id)\n",
    "                unique_categories.append((cat_id, cat_val))\n",
    "        \n",
    "        # 确保至少有一个类别\n",
    "        if not unique_categories:\n",
    "            unique_categories = [(params['cat1_id'], params['cat1_val'])]\n",
    "        \n",
    "        # 计算综合评分\n",
    "        score, _ = calculate_overall_score(base_material, unique_categories, prep_method)\n",
    "        \n",
    "        # 贝叶斯优化是最小化目标函数，所以返回负分数\n",
    "        return -score\n",
    "        \n",
    "    # 创建进度条\n",
    "    progress_bar = CustomProgressBar(total=50, desc=\"贝叶斯优化\")\n",
    "    \n",
    "    # 执行贝叶斯优化\n",
    "    result = gp_minimize(\n",
    "        func=bayesian_objective,\n",
    "        dimensions=[\n",
    "            Integer(0, len(unique_materials['base']) - 1),\n",
    "            Integer(0, len(unique_methods) - 1),\n",
    "            Integer(1, 5),  # 1-5个类别\n",
    "            Integer(1, 5),\n",
    "            Real(100, 699),\n",
    "            Integer(1, 5),\n",
    "            Real(100, 699),\n",
    "            Integer(1, 5),\n",
    "            Real(100, 699),\n",
    "            Integer(1, 5),\n",
    "            Real(100, 699),\n",
    "            Integer(1, 5),\n",
    "            Real(100, 699)\n",
    "        ],\n",
    "        n_calls=50,\n",
    "        n_random_starts=20,\n",
    "        random_state=42,\n",
    "        callback=lambda res: progress_bar.update(1)\n",
    "    )\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    # 解析最佳结果\n",
    "    best_params = result.x\n",
    "    base_idx = int(best_params[0])\n",
    "    method_idx = int(best_params[1])\n",
    "    num_categories = int(best_params[2])\n",
    "    \n",
    "    # 提取类别材料\n",
    "    bayes_categories = []\n",
    "    for i in range(num_categories):\n",
    "        cat_id = int(best_params[3 + i*2])\n",
    "        cat_val = best_params[4 + i*2]\n",
    "        bayes_categories.append((cat_id, cat_val))\n",
    "    \n",
    "    # 确保没有重复类别\n",
    "    seen_categories = set()\n",
    "    bayes_best_categories = []  # 修改变量名与后面使用一致\n",
    "    for cat_id, cat_val in bayes_categories:\n",
    "        if cat_id not in seen_categories:\n",
    "            seen_categories.add(cat_id)\n",
    "            bayes_best_categories.append((cat_id, cat_val))\n",
    "    \n",
    "    # 转换为实际材料\n",
    "    bayes_best_base = unique_materials['base'][base_idx]\n",
    "    bayes_best_method = unique_methods[method_idx]\n",
    "    \n",
    "    # 计算性能\n",
    "    bayes_score, bayes_predictions = calculate_overall_score(\n",
    "        bayes_best_base, bayes_best_categories, bayes_best_method\n",
    "    )\n",
    "    \n",
    "    # 获取材料名称 (如果可用)\n",
    "    try:\n",
    "        # 加载编码参照表\n",
    "        base_ref = pd.read_excel('data_exports/base_material_encoding_reference.xlsx')\n",
    "        method_ref = pd.read_excel('data_exports/method_encoding_reference.xlsx')\n",
    "        material_ref = pd.read_excel('data_exports/material_encoding_reference.xlsx')\n",
    "        \n",
    "        # 获取基底材料名称\n",
    "        base_name = base_ref[base_ref['编码值'] == bayes_best_base]['基底材料'].values[0] if len(base_ref[base_ref['编码值'] == bayes_best_base]) > 0 else f\"基底材料{bayes_best_base}\"\n",
    "        \n",
    "        # 获取制备方法名称\n",
    "        method_name = method_ref[method_ref['编码值'] == bayes_best_method]['制备方法'].values[0] if len(method_ref[method_ref['编码值'] == bayes_best_method]) > 0 else f\"制备方法{bayes_best_method}\"\n",
    "        \n",
    "        # 获取类别材料名称\n",
    "        category_names = []\n",
    "        for cat_id, cat_val in bayes_best_categories:\n",
    "            # 查找最接近的编码值\n",
    "            cat_df = material_ref[material_ref['分类ID'] == cat_id]\n",
    "            if not cat_df.empty:\n",
    "                # 找到最接近的编码值\n",
    "                cat_df['差值'] = abs(cat_df['编码值'] - cat_val)\n",
    "                closest = cat_df.loc[cat_df['差值'].idxmin()]\n",
    "                category_names.append(f\"{closest['材料名称']} (类别{cat_id}, 编码{cat_val:.1f})\")\n",
    "            else:\n",
    "                category_names.append(f\"类别{cat_id}材料 (编码{cat_val:.1f})\")\n",
    "        \n",
    "        print(f\"贝叶斯优化发现的最佳材料:\")\n",
    "        print(f\"  基底材料: {base_name} (编码: {bayes_best_base})\")\n",
    "        print(f\"  类别材料: {', '.join(category_names)}\")\n",
    "        print(f\"  制备方法: {method_name} (编码: {bayes_best_method})\")\n",
    "        print(f\"  综合评分: {bayes_score:.4f}\")\n",
    "        print(\"  预测性能:\")\n",
    "        for target, value in bayes_predictions.items():\n",
    "            print(f\"    {target}: {value:.2f}\")\n",
    "    except Exception as e:\n",
    "        # 如果无法获取名称，则显示编码\n",
    "        print(f\"贝叶斯优化发现的最佳材料:\")\n",
    "        print(f\"  基底材料: {bayes_best_base}\")\n",
    "        print(f\"  类别材料: {', '.join([f'(类别{cat_id}, 值{cat_val:.1f})' for cat_id, cat_val in bayes_best_categories])}\")\n",
    "        print(f\"  制备方法: {bayes_best_method}\")\n",
    "        print(f\"  综合评分: {bayes_score:.4f}\")\n",
    "        print(\"  预测性能:\")\n",
    "        for target, value in bayes_predictions.items():\n",
    "            print(f\"    {target}: {value:.2f}\")\n",
    "        print(f\"  注: 无法加载材料参照表: {str(e)}\")\n",
    "        \n",
    "    # 可视化优化过程\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体\n",
    "    plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "    \n",
    "    # 使用自定义样式绘制收敛曲线\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    iterations = range(len(result.func_vals))\n",
    "    plt.plot(iterations, result.func_vals, 'o-', color='#1f77b4', \n",
    "            markersize=8, linewidth=2, label='目标函数值')\n",
    "    plt.plot(iterations[result.func_vals.argmin()], result.func_vals.min(), \n",
    "            'r*', markersize=20, label='全局最优解')\n",
    "    \n",
    "    # 增加移动平均线，展示趋势\n",
    "    window_size = max(2, len(result.func_vals) // 10)\n",
    "    if len(result.func_vals) > window_size:\n",
    "        moving_avg = np.convolve(result.func_vals, np.ones(window_size)/window_size, mode='valid')\n",
    "        plt.plot(range(window_size-1, len(result.func_vals)), moving_avg, \n",
    "                '--', color='#ff7f0e', linewidth=2, label=f'移动平均 (窗口={window_size})')\n",
    "    \n",
    "    # 添加标题和标签\n",
    "    plt.title(\"贝叶斯优化收敛过程\", fontsize=18, fontweight='bold', pad=20)\n",
    "    plt.xlabel(\"迭代次数\", fontsize=14)\n",
    "    plt.ylabel(\"目标函数值\", fontsize=14)\n",
    "    \n",
    "    # 美化坐标轴和网格\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax = plt.gca()  # 获取当前坐标轴\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # 添加注解，标记最优点\n",
    "    min_idx = result.func_vals.argmin()\n",
    "    min_val = result.func_vals.min()\n",
    "    plt.annotate(f'最小值: {-min_val:.4f}', \n",
    "                xy=(min_idx, min_val), \n",
    "                xytext=(min_idx+2, min_val+0.05),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05, width=1.5),\n",
    "                fontsize=12)\n",
    "    \n",
    "    # 添加图例\n",
    "    plt.legend(loc='upper right', fontsize=12, frameon=True, framealpha=0.9)\n",
    "    \n",
    "    # 添加水印和优化信息\n",
    "    info_text = (f\"贝叶斯优化信息:\\n\"\n",
    "                f\"· 总迭代次数: {len(result.func_vals)}\\n\"\n",
    "                f\"· 初始随机采样: 20\\n\"\n",
    "                f\"· 最优评分: {-min_val:.4f}\")\n",
    "    \n",
    "    plt.figtext(0.02, 0.02, info_text, fontsize=10, \n",
    "                bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('optimization_results/bayesian_convergence.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # 在这一行之后、plt.close()之前插入我提供的代码\n",
    "    # 保存优化数据供后续自定义可视化使用\n",
    "    import os\n",
    "\n",
    "    # 确保目录存在\n",
    "    os.makedirs('optimization_results', exist_ok=True)\n",
    "\n",
    "    # 准备数据\n",
    "    iterations = list(range(len(result.func_vals)))\n",
    "    func_vals = list(result.func_vals)\n",
    "    best_idx = result.func_vals.argmin()\n",
    "    best_val = result.func_vals.min()\n",
    "\n",
    "    # 计算移动平均线数据（如果适用）\n",
    "    moving_avg_data = []\n",
    "    window_size = max(2, len(result.func_vals) // 10)\n",
    "    if len(result.func_vals) > window_size:\n",
    "        moving_avg = np.convolve(result.func_vals, np.ones(window_size)/window_size, mode='valid')\n",
    "        # 用NaN填充开头以匹配长度\n",
    "        moving_avg_full = [float('nan')] * (window_size - 1) + list(moving_avg)\n",
    "    else:\n",
    "        moving_avg_full = [float('nan')] * len(result.func_vals)\n",
    "\n",
    "    # 创建数据字典\n",
    "    data = {\n",
    "        '迭代次数': iterations,\n",
    "        '目标函数值': func_vals,\n",
    "        '移动平均': moving_avg_full,\n",
    "        '是否最优点': [i == best_idx for i in iterations]\n",
    "    }\n",
    "\n",
    "    # 转换为DataFrame并保存\n",
    "    optimization_df = pd.DataFrame(data)\n",
    "    optimization_df.to_csv('optimization_results/bayesian_optimization_data.csv', index=False)\n",
    "\n",
    "    # 保存元数据\n",
    "    metadata = pd.DataFrame({\n",
    "        '指标': ['总迭代次数', '随机采样数', '最优评分', '最优迭代序号', '移动平均窗口大小'],\n",
    "        '数值': [len(result.func_vals), 20, -best_val, best_idx, window_size]\n",
    "    })\n",
    "    metadata.to_csv('optimization_results/bayesian_optimization_metadata.csv', index=False)\n",
    "\n",
    "    # 保存最优解的详细信息\n",
    "    best_solution = pd.DataFrame({\n",
    "        '参数': ['基底材料', '制备方法', '综合评分'] + \n",
    "            [f'类别{cat_id}材料值' for cat_id, _ in bayes_best_categories] +\n",
    "            list(bayes_predictions.keys()),\n",
    "        '数值': [bayes_best_base, bayes_best_method, bayes_score] + \n",
    "            [cat_val for _, cat_val in bayes_best_categories] +\n",
    "            list(bayes_predictions.values())\n",
    "    })\n",
    "    best_solution.to_csv('optimization_results/bayesian_best_solution.csv', index=False)\n",
    "\n",
    "    print(f\"优化数据已保存至: optimization_results/bayesian_optimization_data.csv\")\n",
    "    print(f\"优化元数据已保存至: optimization_results/bayesian_optimization_metadata.csv\")\n",
    "    print(f\"优化最优解已保存至: optimization_results/bayesian_best_solution.csv\")\n",
    "\n",
    "\n",
    "    plt.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"贝叶斯优化执行失败: {str(e)}\")\n",
    "    bayes_best_base = None\n",
    "    bayes_best_categories = None\n",
    "    bayes_best_method = None\n",
    "    bayes_score = 0\n",
    "    bayes_predictions = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= 精准化生成式AI优化 =======\n",
    "print(\"\\n2. 执行精准化生成式AI优化...\")\n",
    "\n",
    "# 首先验证模型加载状态\n",
    "print(\"验证模型加载状态...\")\n",
    "print(f\"目标变量: {target_columns}\")\n",
    "print(f\"best_model中的键: {list(best_model.keys())}\")\n",
    "\n",
    "if not best_model:\n",
    "    print(\"警告: best_model为空，检查模型文件路径...\")\n",
    "    # 显示可能的文件路径用于调试\n",
    "    for target in target_columns:\n",
    "        target_sanitized = sanitize_filename(target)\n",
    "        expected_path = f'模型评估结果/{target_sanitized}_best_models.pkl'\n",
    "        print(f\"  期待的路径: {expected_path}\")\n",
    "        print(f\"  文件是否存在: {os.path.exists(expected_path)}\")\n",
    "    \n",
    "    # 如果模型未加载，尝试手动加载一次\n",
    "    print(\"尝试重新加载模型...\")\n",
    "    for target in target_columns:\n",
    "        target_sanitized = sanitize_filename(target)\n",
    "        possible_paths = [\n",
    "            f'模型评估结果/{target_sanitized}_best_models.pkl',\n",
    "            f'模型评估结果/{target}_best_models.pkl',\n",
    "            f'训练模型文件/{target_sanitized}_best_models.pkl',\n",
    "            f'训练模型文件/{target}_best_models.pkl'\n",
    "        ]\n",
    "        \n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                try:\n",
    "                    with open(path, 'rb') as f:\n",
    "                        target_model_info = pickle.load(f)\n",
    "                    best_model[target] = {\n",
    "                        'tolerance_r2': {\n",
    "                            'model': target_model_info['best_tol_r2_model']\n",
    "                        }\n",
    "                    }\n",
    "                    print(f\"  ✓ 成功加载 {target} 的模型\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"  ❌ 加载 {path} 失败: {str(e)}\")\n",
    "\n",
    "# 验证最终的模型状态\n",
    "if best_model:\n",
    "    print(f\"✓ 模型加载成功，共 {len(best_model)} 个目标变量\")\n",
    "    for target, model_info in best_model.items():\n",
    "        print(f\"  {target}: {model_info['tolerance_r2']['model']}\")\n",
    "else:\n",
    "    print(\"❌ 模型加载失败，无法进行精准预测\")\n",
    "    # 设置空结果并退出\n",
    "    gen_best_base = None\n",
    "    gen_best_categories = None\n",
    "    gen_best_method = None\n",
    "    gen_score = 0\n",
    "    gen_predictions = {}\n",
    "    print(\"生成式AI优化跳过\")\n",
    "\n",
    "# 只有在模型成功加载后才继续\n",
    "if best_model:\n",
    "    print(\"\\n开始收集高质量训练样本...\")\n",
    "    \n",
    "    # 精准的材料组合收集\n",
    "    valid_combinations = []\n",
    "    material_scores = []\n",
    "    material_encodings = []\n",
    "    \n",
    "    # 创建精准的编码函数\n",
    "    def create_precise_encoding(base_material, categories, prep_method):\n",
    "        \"\"\"创建精准的材料编码，保留更多信息\"\"\"\n",
    "        encoding = []\n",
    "        \n",
    "        # 基底材料 - 使用标准化编码\n",
    "        base_normalized = (base_material - min(unique_materials['base'])) / \\\n",
    "                         max(1, max(unique_materials['base']) - min(unique_materials['base']))\n",
    "        encoding.append(base_normalized)\n",
    "        \n",
    "        # 制备方法 - 使用标准化编码  \n",
    "        method_normalized = (prep_method - min(unique_methods)) / \\\n",
    "                           max(1, max(unique_methods) - min(unique_methods))\n",
    "        encoding.append(method_normalized)\n",
    "        \n",
    "        # 类别材料 - 使用更精细的编码\n",
    "        # 为每个可能的类别ID创建独立的编码维度\n",
    "        for cat_id in range(1, 6):  # 5个类别\n",
    "            # 存在标志\n",
    "            exists = 1.0 if any(cid == cat_id for cid, _ in categories) else 0.0\n",
    "            encoding.append(exists)\n",
    "            \n",
    "            # 数值 - 归一化到[0,1]\n",
    "            cat_value = 0.0\n",
    "            for cid, cval in categories:\n",
    "                if cid == cat_id:\n",
    "                    # 根据类别ID的不同范围进行归一化\n",
    "                    if cat_id <= 4:\n",
    "                        cat_value = (cval - 100) / 99.0  # 100-199范围\n",
    "                    else:  # cat_id == 5\n",
    "                        cat_value = (cval - 500) / 199.0  # 500-699范围\n",
    "                    break\n",
    "            encoding.append(cat_value)\n",
    "        \n",
    "        # 添加组合特征：类别数量\n",
    "        num_categories = len(categories)\n",
    "        encoding.append(num_categories / 5.0)  # 归一化\n",
    "        \n",
    "        # 添加组合特征：类别多样性\n",
    "        category_ids = sorted([cid for cid, _ in categories])\n",
    "        diversity = len(set(category_ids)) / 5.0  # 归一化\n",
    "        encoding.append(diversity)\n",
    "        \n",
    "        return np.array(encoding, dtype=np.float32)\n",
    "    \n",
    "    # 收集训练样本，增加质量控制\n",
    "    progress_bar = CustomProgressBar(total=len(feature_data), desc=\"收集高质量样本\")\n",
    "    \n",
    "    collected_samples = 0\n",
    "    for i, row in feature_data.iterrows():\n",
    "        try:\n",
    "            # 提取基本信息\n",
    "            base_material = row[base_material_column]\n",
    "            prep_method = row[method_column]\n",
    "            \n",
    "            if pd.isna(base_material) or pd.isna(prep_method):\n",
    "                progress_bar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # 提取并验证类别材料\n",
    "            categories = []\n",
    "            for col in category_columns:\n",
    "                if pd.notna(row[col]) and row[col] > 0:\n",
    "                    if '_' in col:\n",
    "                        try:\n",
    "                            category_id = int(col.split('_')[0].replace('类别', ''))\n",
    "                            if 1 <= category_id <= 5:\n",
    "                                categories.append((category_id, row[col]))\n",
    "                        except (ValueError, IndexError):\n",
    "                            continue\n",
    "            \n",
    "            # 质量控制：确保数据完整性\n",
    "            if not categories or len(categories) > 5:\n",
    "                progress_bar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # 去重类别\n",
    "            seen_categories = set()\n",
    "            unique_categories = []\n",
    "            for cat_id, cat_val in categories:\n",
    "                if cat_id not in seen_categories:\n",
    "                    seen_categories.add(cat_id)\n",
    "                    unique_categories.append((cat_id, cat_val))\n",
    "            \n",
    "            # 确保材料在已知范围内\n",
    "            if (base_material not in unique_materials['base'] or \n",
    "                prep_method not in unique_methods):\n",
    "                progress_bar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # 计算真实性能评分\n",
    "            score, predictions = calculate_overall_score(base_material, unique_categories, prep_method)\n",
    "            \n",
    "            # 质量控制：只保留高质量样本\n",
    "            if score > 0 and predictions and len(predictions) >= 2:\n",
    "                valid_combinations.append((base_material, unique_categories, prep_method))\n",
    "                material_scores.append(score)\n",
    "                \n",
    "                # 创建精准编码\n",
    "                encoding = create_precise_encoding(base_material, unique_categories, prep_method)\n",
    "                material_encodings.append(encoding)\n",
    "                collected_samples += 1\n",
    "                \n",
    "                # 显示前几个成功样本的信息\n",
    "                if collected_samples <= 3:\n",
    "                    print(f\"\\n样本 {collected_samples}:\")\n",
    "                    print(f\"  基底: {base_material}, 方法: {prep_method}\")\n",
    "                    print(f\"  类别: {unique_categories}\")\n",
    "                    print(f\"  评分: {score:.4f}\")\n",
    "                    print(f\"  预测: {list(predictions.values())}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            if collected_samples < 3:\n",
    "                print(f\"\\n处理样本 {i} 时出错: {str(e)}\")\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    print(f\"成功收集 {collected_samples} 个高质量训练样本\")\n",
    "    \n",
    "    if collected_samples >= 20:  # 提高最小样本要求\n",
    "        print(\"\\n训练精准生成模型...\")\n",
    "        \n",
    "        # 转换为NumPy数组\n",
    "        X_train = np.array(material_encodings)  \n",
    "        y_train = np.array(material_scores)\n",
    "        \n",
    "        # 数据质量检查和清理\n",
    "        print(f\"训练数据形状: {X_train.shape}\")\n",
    "        print(f\"特征维度: {X_train.shape[1]}\")\n",
    "        print(f\"评分范围: [{y_train.min():.4f}, {y_train.max():.4f}]\")\n",
    "        \n",
    "        # 处理异常值\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "        scaler = RobustScaler()  # 对异常值更鲁棒\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        \n",
    "        # 使用改进的生成模型 - 组合高斯混合模型和变分采样\n",
    "        from sklearn.mixture import GaussianMixture\n",
    "        \n",
    "        # 优化的高斯混合模型参数\n",
    "        n_components = min(8, max(3, collected_samples // 8))  # 更精细的组件选择\n",
    "        print(f\"使用 {n_components} 个高斯组件\")\n",
    "        \n",
    "        # 训练高斯混合模型\n",
    "        gmm = GaussianMixture(\n",
    "            n_components=n_components,\n",
    "            covariance_type='full',  # 使用完整协方差矩阵\n",
    "            max_iter=200,  # 增加迭代次数\n",
    "            random_state=42,\n",
    "            reg_covar=1e-6,  # 正则化参数\n",
    "            tol=1e-4  # 收敛容忍度\n",
    "        )\n",
    "        \n",
    "        gmm.fit(X_train_scaled)\n",
    "        print(f\"高斯混合模型训练完成，收敛: {gmm.converged_}\")\n",
    "        print(f\"对数似然: {gmm.score(X_train_scaled):.4f}\")\n",
    "        \n",
    "        # 精准的材料生成策略\n",
    "        print(\"生成高质量候选材料...\")\n",
    "        \n",
    "        # 多种生成策略组合\n",
    "        all_candidates = []\n",
    "        \n",
    "        # 策略1: 从高斯混合模型采样\n",
    "        n_gmm_samples = min(300, collected_samples * 3)\n",
    "        gmm_samples, _ = gmm.sample(n_gmm_samples)\n",
    "        gmm_samples_original = scaler.inverse_transform(gmm_samples)\n",
    "        \n",
    "        # 策略2: 基于最佳样本的变分采样\n",
    "        top_indices = np.argsort(y_train)[-min(10, len(y_train)):]  # 取前10个最佳样本\n",
    "        for idx in top_indices:\n",
    "            base_sample = X_train_scaled[idx]\n",
    "            # 在最佳样本周围生成变分\n",
    "            for _ in range(20):  # 每个最佳样本生成20个变分\n",
    "                noise = np.random.normal(0, 0.1, base_sample.shape)  # 小幅随机扰动\n",
    "                variant = base_sample + noise\n",
    "                variant_original = scaler.inverse_transform([variant])[0]\n",
    "                all_candidates.append(variant_original)\n",
    "        \n",
    "        # 合并所有候选样本\n",
    "        all_candidates.extend(gmm_samples_original)\n",
    "        \n",
    "        print(f\"生成 {len(all_candidates)} 个候选材料组合\")\n",
    "        \n",
    "        # 精准的材料解码和评估\n",
    "        evaluated_materials = []\n",
    "        decode_errors = 0\n",
    "        \n",
    "        progress_bar = CustomProgressBar(total=len(all_candidates), desc=\"精准评估候选材料\")\n",
    "        \n",
    "        for sample in all_candidates:\n",
    "            try:\n",
    "                # 精准解码基底材料\n",
    "                base_norm = sample[0]\n",
    "                base_range = max(unique_materials['base']) - min(unique_materials['base'])\n",
    "                base_material = min(unique_materials['base']) + base_norm * base_range\n",
    "                # 选择最接近的实际基底材料\n",
    "                base_material = min(unique_materials['base'], \n",
    "                                  key=lambda x: abs(x - base_material))\n",
    "                \n",
    "                # 精准解码制备方法\n",
    "                method_norm = sample[1]\n",
    "                method_range = max(unique_methods) - min(unique_methods)\n",
    "                prep_method = min(unique_methods) + method_norm * method_range\n",
    "                # 选择最接近的实际制备方法\n",
    "                prep_method = min(unique_methods,\n",
    "                                key=lambda x: abs(x - prep_method))\n",
    "                \n",
    "                # 精准解码类别材料\n",
    "                categories = []\n",
    "                for cat_id in range(1, 6):\n",
    "                    exists_idx = 2 + (cat_id - 1) * 2\n",
    "                    value_idx = exists_idx + 1\n",
    "                    \n",
    "                    if exists_idx < len(sample) and value_idx < len(sample):\n",
    "                        exists_prob = sample[exists_idx]\n",
    "                        value_norm = sample[value_idx]\n",
    "                        \n",
    "                        # 更严格的存在判断\n",
    "                        if exists_prob > 0.4:  # 提高阈值\n",
    "                            # 精准的数值恢复\n",
    "                            if cat_id <= 4:\n",
    "                                cat_value = 100 + value_norm * 99\n",
    "                            else:\n",
    "                                cat_value = 500 + value_norm * 199\n",
    "                            \n",
    "                            # 确保数值在合理范围内\n",
    "                            cat_value = max(100, min(699, cat_value))\n",
    "                            categories.append((cat_id, cat_value))\n",
    "                \n",
    "                # 质量控制：确保至少有一个类别\n",
    "                if not categories:\n",
    "                    # 基于样本特征选择最可能的类别\n",
    "                    best_cat_id = 1\n",
    "                    best_prob = 0\n",
    "                    for cat_id in range(1, 6):\n",
    "                        exists_idx = 2 + (cat_id - 1) * 2\n",
    "                        if exists_idx < len(sample) and sample[exists_idx] > best_prob:\n",
    "                            best_prob = sample[exists_idx]\n",
    "                            best_cat_id = cat_id\n",
    "                    \n",
    "                    # 为最可能的类别设置默认值\n",
    "                    if best_cat_id <= 4:\n",
    "                        default_val = 100 * best_cat_id + 50\n",
    "                    else:\n",
    "                        default_val = 600\n",
    "                    categories.append((best_cat_id, default_val))\n",
    "                \n",
    "                # 去重和验证\n",
    "                seen_cats = set()\n",
    "                final_categories = []\n",
    "                for cat_id, cat_val in categories:\n",
    "                    if cat_id not in seen_cats:\n",
    "                        seen_cats.add(cat_id)\n",
    "                        final_categories.append((cat_id, cat_val))\n",
    "                \n",
    "                # 精准的性能评估\n",
    "                score, predictions = calculate_overall_score(base_material, final_categories, prep_method)\n",
    "                \n",
    "                if score > 0 and predictions:\n",
    "                    evaluated_materials.append({\n",
    "                        'base_material': base_material,\n",
    "                        'categories': final_categories,\n",
    "                        'prep_method': prep_method,\n",
    "                        'score': score,\n",
    "                        'predictions': predictions\n",
    "                    })\n",
    "            \n",
    "            except Exception as e:\n",
    "                decode_errors += 1\n",
    "                if decode_errors <= 3:\n",
    "                    print(f\"\\n解码错误: {str(e)}\")\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        progress_bar.close()\n",
    "        \n",
    "        print(f\"成功评估 {len(evaluated_materials)} 个有效材料组合\")\n",
    "        print(f\"解码错误: {decode_errors} 个\")\n",
    "        \n",
    "        if evaluated_materials:\n",
    "            # 按评分排序\n",
    "            evaluated_materials.sort(key=lambda x: x['score'], reverse=True)\n",
    "            \n",
    "            # 选择最佳材料\n",
    "            best_material = evaluated_materials[0]\n",
    "            \n",
    "            gen_best_base = best_material['base_material']\n",
    "            gen_best_categories = best_material['categories']\n",
    "            gen_best_method = best_material['prep_method']\n",
    "            gen_score = best_material['score']\n",
    "            gen_predictions = best_material['predictions']\n",
    "            \n",
    "            print(f\"\\n🎯 精准生成式AI发现的最佳材料组合:\")\n",
    "            print(f\"  基底材料编码: {gen_best_base}\")\n",
    "            print(f\"  制备方法编码: {gen_best_method}\")\n",
    "            print(f\"  类别材料组合: {gen_best_categories}\")\n",
    "            print(f\"  综合评分: {gen_score:.6f}\")\n",
    "            print(f\"  详细性能预测:\")\n",
    "            for target, value in gen_predictions.items():\n",
    "                print(f\"    {target}: {value:.4f}\")\n",
    "            \n",
    "            # 与训练数据对比\n",
    "            training_scores = material_scores\n",
    "            improvement = (gen_score - np.mean(training_scores)) / np.mean(training_scores) * 100\n",
    "            print(f\"\\n📊 性能对比:\")\n",
    "            print(f\"  训练数据平均评分: {np.mean(training_scores):.6f}\")\n",
    "            print(f\"  训练数据最高评分: {np.max(training_scores):.6f}\")\n",
    "            print(f\"  生成式AI评分: {gen_score:.6f}\")\n",
    "            print(f\"  相对于平均值的提升: {improvement:.2f}%\")\n",
    "            \n",
    "            if gen_score > np.max(training_scores):\n",
    "                print(f\"  🎉 超越了所有训练样本！\")\n",
    "            else:\n",
    "                rank = sum(1 for s in training_scores if s > gen_score) + 1\n",
    "                print(f\"  排名: 第 {rank}/{len(training_scores)} 位\")\n",
    "        \n",
    "        else:\n",
    "            print(\"❌ 未生成有效的材料组合\")\n",
    "            gen_best_base = None\n",
    "            gen_best_categories = None\n",
    "            gen_best_method = None\n",
    "            gen_score = 0\n",
    "            gen_predictions = {}\n",
    "    \n",
    "    else:\n",
    "        print(f\"❌ 高质量样本不足 ({collected_samples} < 20)，无法训练精准模型\")\n",
    "        gen_best_base = None\n",
    "        gen_best_categories = None\n",
    "        gen_best_method = None\n",
    "        gen_score = 0\n",
    "        gen_predictions = {}\n",
    "\n",
    "print(\"\\n精准化生成式AI优化完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= 3. 差分进化优化 =======\n",
    "print(\"\\n3. 执行差分进化优化...\")\n",
    "\n",
    "try:\n",
    "    # 修改 OptimizationProgress 类，增加历史记录\n",
    "    class OptimizationProgress:\n",
    "        def __init__(self, description=\"优化进度\"):\n",
    "            self.description = description\n",
    "            self.iteration = 0\n",
    "            self.history = []\n",
    "            self.best_values = []\n",
    "            self.start_time = time.time()\n",
    "            \n",
    "            # 创建进度条\n",
    "            print(f\"{self.description}开始...\")\n",
    "        \n",
    "        def update(self, xk, convergence=None):\n",
    "            \"\"\"记录优化过程\"\"\"\n",
    "            self.iteration += 1\n",
    "            \n",
    "            # 记录当前最优值\n",
    "            if isinstance(xk, dict) and 'fun' in xk:\n",
    "                func_value = float(xk['fun'])  # 确保转换为Python浮点数\n",
    "            else:\n",
    "                # 如果是数组，确保转换为Python浮点数\n",
    "                func_value = float(xk) if np.isscalar(xk) else float(xk[0]) if hasattr(xk, '__len__') and len(xk) > 0 else 0.0\n",
    "            \n",
    "            self.history.append(func_value)\n",
    "            \n",
    "            # 记录历史最优值\n",
    "            if not self.best_values or func_value < min(self.best_values):\n",
    "                self.best_values.append(func_value)\n",
    "            else:\n",
    "                self.best_values.append(min(self.best_values))\n",
    "            \n",
    "            # 每10次迭代显示一次进度\n",
    "            if self.iteration % 10 == 0 or self.iteration == 1:\n",
    "                elapsed = time.time() - self.start_time\n",
    "                best_value = float(min(self.best_values))  # 确保是Python浮点数\n",
    "                print(f\"{self.description} - 迭代: {self.iteration}, 最优值: {best_value:.6f}, 用时: {elapsed:.2f}秒\")\n",
    "        \n",
    "        def close(self):\n",
    "            \"\"\"完成优化过程\"\"\"\n",
    "            elapsed = time.time() - self.start_time\n",
    "            if self.history:\n",
    "                best_value = float(min(self.best_values))  # 确保是Python浮点数\n",
    "                print(f\"{self.description}完成 - 共{self.iteration}次迭代, 最优值: {best_value:.6f}, 总用时: {elapsed:.2f}秒\")\n",
    "            else:\n",
    "                print(f\"{self.description}完成 - 无有效记录\")\n",
    "\n",
    "    # 定义差分进化目标函数\n",
    "    def de_objective(x):\n",
    "        \"\"\"差分进化优化的目标函数\"\"\"\n",
    "        try:\n",
    "            # 确保 x 是一个标准Python列表或NumPy数组\n",
    "            x_list = x.tolist() if hasattr(x, 'tolist') else list(x)\n",
    "            \n",
    "            # 基底材料和制备方法索引\n",
    "            base_idx = int(x_list[0]) % len(unique_materials['base'])\n",
    "            method_idx = int(x_list[1]) % len(unique_methods)\n",
    "            \n",
    "            # 从参数中提取类别信息\n",
    "            categories = []\n",
    "            for i in range(0, min(10, len(x_list)-2), 2):  # 最多5对类别参数\n",
    "                if i+3 < len(x_list):\n",
    "                    cat_id = max(1, min(5, int(x_list[i+2])))  # 确保类别ID在1-5范围内\n",
    "                    cat_val = float(x_list[i+3])  # 确保是标量值\n",
    "                    # 将值映射到有效范围 (100-699)\n",
    "                    if cat_val > 0:  # 只添加正值\n",
    "                        mapped_val = max(100, min(699, cat_val))\n",
    "                        categories.append((cat_id, mapped_val))\n",
    "            \n",
    "            # 去除重复类别\n",
    "            seen_cats = set()\n",
    "            unique_cats = []\n",
    "            for cat_id, cat_val in categories:\n",
    "                if cat_id not in seen_cats:\n",
    "                    seen_cats.add(cat_id)\n",
    "                    unique_cats.append((cat_id, cat_val))\n",
    "            \n",
    "            # 确保至少有一个类别\n",
    "            if not unique_cats and len(x_list) > 3:\n",
    "                cat_id = max(1, min(5, int(x_list[2])))\n",
    "                cat_val = max(100, min(699, float(x_list[3])))\n",
    "                unique_cats.append((cat_id, cat_val))\n",
    "            \n",
    "            # 转换为实际材料\n",
    "            base_material = unique_materials['base'][base_idx]\n",
    "            prep_method = unique_methods[method_idx]\n",
    "            \n",
    "            # 计算综合评分\n",
    "            score, _ = calculate_overall_score(base_material, unique_cats, prep_method)\n",
    "            \n",
    "            # 返回负分数用于最小化，确保是标量\n",
    "            return -float(score)\n",
    "        except Exception as e:\n",
    "            return 0.0  # 出错时返回默认值\n",
    "    \n",
    "    # 定义参数范围\n",
    "    bounds = [\n",
    "        (0, len(unique_materials['base']) - 1),  # 基底材料索引\n",
    "        (0, len(unique_methods) - 1),  # 制备方法索引\n",
    "    ]\n",
    "    \n",
    "    # 添加类别材料参数的边界 (ID和值)\n",
    "    for _ in range(5):  # 最多5种类别\n",
    "        bounds.append((1, 5))  # 类别ID: 1-5\n",
    "        bounds.append((100, 699))  # 类别材料编码值\n",
    "    \n",
    "    # 创建进度记录\n",
    "    opt_progress = OptimizationProgress(\"差分进化优化\")\n",
    "    \n",
    "    try:\n",
    "        # 使用差分进化算法搜索最佳材料\n",
    "        result = differential_evolution(\n",
    "            de_objective,\n",
    "            bounds,\n",
    "            popsize=20,\n",
    "            mutation=(0.5, 1.0),\n",
    "            recombination=0.7,\n",
    "            maxiter=200,\n",
    "            tol=0.01,\n",
    "            seed=42,\n",
    "            updating='immediate',\n",
    "            callback=opt_progress.update,\n",
    "            workers=1,\n",
    "            polish=True\n",
    "        )\n",
    "        \n",
    "        opt_progress.close()\n",
    "        \n",
    "        # 解析最佳参数\n",
    "        best_params = result.x.tolist()  # 确保转换为Python列表\n",
    "        \n",
    "        # 基底材料和制备方法\n",
    "        base_idx = int(best_params[0]) % len(unique_materials['base'])\n",
    "        method_idx = int(best_params[1]) % len(unique_methods)\n",
    "        \n",
    "        # 从参数中提取类别信息\n",
    "        de_categories = []\n",
    "        for i in range(0, min(10, len(best_params)-2), 2):\n",
    "            if i+3 < len(best_params):\n",
    "                cat_id = max(1, min(5, int(best_params[i+2])))\n",
    "                cat_val = float(best_params[i+3])  # 确保是标量\n",
    "                if cat_val > 0:\n",
    "                    # 映射到有效范围\n",
    "                    mapped_val = max(100, min(699, cat_val))\n",
    "                    de_categories.append((cat_id, mapped_val))\n",
    "        \n",
    "        # 去除重复类别\n",
    "        seen_cats = set()\n",
    "        de_best_categories = []  # 修改变量名与后面使用一致\n",
    "        for cat_id, cat_val in de_categories:\n",
    "            if cat_id not in seen_cats:\n",
    "                seen_cats.add(cat_id)\n",
    "                de_best_categories.append((cat_id, cat_val))\n",
    "        \n",
    "        # 确保至少有一个类别\n",
    "        if not de_best_categories and len(best_params) > 3:\n",
    "            cat_id = max(1, min(5, int(best_params[2])))\n",
    "            cat_val = max(100, min(699, float(best_params[3])))\n",
    "            de_best_categories.append((cat_id, cat_val))\n",
    "        \n",
    "        # 转换为实际材料编码\n",
    "        de_best_base = unique_materials['base'][base_idx]\n",
    "        de_best_method = unique_methods[method_idx]\n",
    "        \n",
    "        # 计算性能\n",
    "        de_score, de_predictions = calculate_overall_score(\n",
    "            de_best_base, de_best_categories, de_best_method\n",
    "        )\n",
    "        \n",
    "        # 获取材料名称 (如果可用)\n",
    "        try:\n",
    "            # 加载编码参照表\n",
    "            base_ref = pd.read_excel('data_exports/base_material_encoding_reference.xlsx')\n",
    "            method_ref = pd.read_excel('data_exports/method_encoding_reference.xlsx')\n",
    "            material_ref = pd.read_excel('data_exports/material_encoding_reference.xlsx')\n",
    "            \n",
    "            # 获取基底材料名称\n",
    "            base_name = base_ref[base_ref['编码值'] == de_best_base]['基底材料'].values[0] if len(base_ref[base_ref['编码值'] == de_best_base]) > 0 else f\"基底材料{de_best_base}\"\n",
    "            \n",
    "            # 获取制备方法名称\n",
    "            method_name = method_ref[method_ref['编码值'] == de_best_method]['制备方法'].values[0] if len(method_ref[method_ref['编码值'] == de_best_method]) > 0 else f\"制备方法{de_best_method}\"\n",
    "            \n",
    "            # 获取类别材料名称\n",
    "            category_names = []\n",
    "            for cat_id, cat_val in de_best_categories:\n",
    "                # 查找最接近的编码值\n",
    "                cat_df = material_ref[material_ref['分类ID'] == cat_id]\n",
    "                if not cat_df.empty:\n",
    "                    # 找到最接近的编码值\n",
    "                    cat_df['差值'] = abs(cat_df['编码值'] - cat_val)\n",
    "                    closest = cat_df.loc[cat_df['差值'].idxmin()]\n",
    "                    category_names.append(f\"{closest['材料名称']} (类别{cat_id}, 编码{cat_val:.1f})\")\n",
    "                else:\n",
    "                    category_names.append(f\"类别{cat_id}材料 (编码{cat_val:.1f})\")\n",
    "            \n",
    "            print(f\"差分进化发现的最佳材料:\")\n",
    "            print(f\"  基底材料: {base_name} (编码: {de_best_base})\")\n",
    "            print(f\"  类别材料: {', '.join(category_names)}\")\n",
    "            print(f\"  制备方法: {method_name} (编码: {de_best_method})\")\n",
    "            print(f\"  综合评分: {de_score:.4f}\")\n",
    "            print(\"  预测性能:\")\n",
    "            for target, value in de_predictions.items():\n",
    "                print(f\"    {target}: {value:.2f}\")\n",
    "        except Exception as e:\n",
    "            # 如果无法获取名称，则显示编码\n",
    "            print(f\"差分进化发现的最佳材料:\")\n",
    "            print(f\"  基底材料: {de_best_base}\")\n",
    "            print(f\"  类别材料: {', '.join([f'(类别{cat_id}, 值{cat_val:.1f})' for cat_id, cat_val in de_best_categories])}\")\n",
    "            print(f\"  制备方法: {de_best_method}\")\n",
    "            print(f\"  综合评分: {de_score:.4f}\")\n",
    "            print(\"  预测性能:\")\n",
    "            for target, value in de_predictions.items():\n",
    "                print(f\"    {target}: {value:.2f}\")\n",
    "            print(f\"  注: 无法加载材料参照表: {str(e)}\")\n",
    "        \n",
    "        # 添加差分进化可视化\n",
    "        def plot_de_optimization(opt_progress):\n",
    "            \"\"\"可视化差分进化优化过程\"\"\"\n",
    "            if not opt_progress.history:\n",
    "                print(\"无优化历史记录可供可视化\")\n",
    "                return\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "            plt.rcParams['axes.unicode_minus'] = False\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            \n",
    "            # 绘制目标函数值\n",
    "            iterations = range(1, len(opt_progress.history) + 1)\n",
    "            ax.plot(iterations, opt_progress.history, 'o-', color='#4292c6', \n",
    "                    markersize=6, linewidth=1.5, alpha=0.7, label='目标函数值')\n",
    "            \n",
    "            # 绘制历史最优值\n",
    "            ax.plot(iterations, opt_progress.best_values, '-', color='#d62728', \n",
    "                    linewidth=2.5, label='历史最优值')\n",
    "            \n",
    "            # 标记全局最优点\n",
    "            best_idx = np.argmin(opt_progress.history)\n",
    "            best_value = opt_progress.history[best_idx]\n",
    "            ax.plot(best_idx + 1, best_value, 'r*', markersize=15)\n",
    "            \n",
    "            # 添加最优值标注\n",
    "            ax.annotate(f'全局最优: {best_value:.4f}', \n",
    "                        xy=(best_idx + 1, best_value), \n",
    "                        xytext=(best_idx + 1 + 5, best_value - 0.02),\n",
    "                        arrowprops=dict(facecolor='black', shrink=0.05, width=1),\n",
    "                        fontsize=12)\n",
    "            \n",
    "            # 设置标题和标签\n",
    "            ax.set_title(\"差分进化优化收敛过程\", fontsize=18, fontweight='bold', pad=20)\n",
    "            ax.set_xlabel(\"迭代次数\", fontsize=14)\n",
    "            ax.set_ylabel(\"目标函数值\", fontsize=14)\n",
    "            \n",
    "            # 美化坐标轴和网格\n",
    "            ax.grid(True, linestyle='--', alpha=0.7)\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            \n",
    "            # 添加收敛分析区域\n",
    "            if len(opt_progress.history) > 20:\n",
    "                # 计算近期收敛速度 (最后20%的迭代)\n",
    "                last_segment = opt_progress.best_values[-int(len(opt_progress.best_values)*0.2):]\n",
    "                if last_segment and last_segment[0] != last_segment[-1]:\n",
    "                    improve_rate = (last_segment[0] - last_segment[-1]) / last_segment[0] * 100\n",
    "                    ax.text(0.02, 0.1, f\"最近阶段改进率: {improve_rate:.2f}%\", \n",
    "                            transform=ax.transAxes, fontsize=12,\n",
    "                            bbox=dict(facecolor='white', alpha=0.8))\n",
    "            \n",
    "            # 添加统计信息\n",
    "            info_text = (f\"差分进化优化统计:\\n\"\n",
    "                        f\"· 总迭代次数: {len(opt_progress.history)}\\n\"\n",
    "                        f\"· 初始值: {opt_progress.history[0]:.4f}\\n\"\n",
    "                        f\"· 最终值: {opt_progress.history[-1]:.4f}\\n\"\n",
    "                        f\"· 全局最优: {best_value:.4f}\\n\"\n",
    "                        f\"· 总体改进: {(1 - best_value/opt_progress.history[0])*100:.1f}%\")\n",
    "            \n",
    "            plt.figtext(0.02, 0.02, info_text, fontsize=10, \n",
    "                        bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "            \n",
    "            # 添加图例\n",
    "            ax.legend(loc='upper right', fontsize=12)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('optimization_results/differential_evolution_convergence.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "            # 保存优化迭代数据\n",
    "            iterations = list(range(1, len(opt_progress.history) + 1))\n",
    "            optimization_data = pd.DataFrame({\n",
    "                '迭代次数': iterations,\n",
    "                '目标函数值': opt_progress.history,\n",
    "                '历史最优值': opt_progress.best_values,\n",
    "                '是否全局最优点': [i == np.argmin(opt_progress.history) + 1 for i in iterations]\n",
    "            })\n",
    "            optimization_data.to_csv('optimization_results/differential_evolution_data.csv', index=False)\n",
    "\n",
    "            # 保存优化统计元数据\n",
    "            best_idx = np.argmin(opt_progress.history)\n",
    "            best_value = opt_progress.history[best_idx]\n",
    "\n",
    "            # 计算近期收敛速度（如果有足够的迭代次数）\n",
    "            improve_rate = float('nan')\n",
    "            if len(opt_progress.history) > 20:\n",
    "                last_segment = opt_progress.best_values[-int(len(opt_progress.best_values)*0.2):]\n",
    "                if last_segment and last_segment[0] != last_segment[-1]:\n",
    "                    improve_rate = (last_segment[0] - last_segment[-1]) / last_segment[0] * 100\n",
    "\n",
    "            metadata = pd.DataFrame({\n",
    "                '指标': [\n",
    "                    '总迭代次数', \n",
    "                    '初始值', \n",
    "                    '最终值', \n",
    "                    '全局最优值',\n",
    "                    '全局最优迭代次数',\n",
    "                    '总体改进率(%)',\n",
    "                    '最近阶段改进率(%)'\n",
    "                ],\n",
    "                '数值': [\n",
    "                    len(opt_progress.history),\n",
    "                    opt_progress.history[0],\n",
    "                    opt_progress.history[-1],\n",
    "                    best_value,\n",
    "                    best_idx + 1,\n",
    "                    (1 - best_value/opt_progress.history[0])*100 if opt_progress.history[0] != 0 else float('nan'),\n",
    "                    improve_rate\n",
    "                ]\n",
    "            })\n",
    "            metadata.to_csv('optimization_results/differential_evolution_metadata.csv', index=False)\n",
    "\n",
    "            # 保存最优解的详细信息（如果可用）\n",
    "            if 'de_best_base' in globals() and de_best_base is not None:\n",
    "                best_solution = pd.DataFrame({\n",
    "                    '参数': ['基底材料', '制备方法', '综合评分'] + \n",
    "                        [f'类别{cat_id}材料值' for cat_id, _ in de_best_categories] +\n",
    "                        list(de_predictions.keys()),\n",
    "                    '数值': [de_best_base, de_best_method, de_score] + \n",
    "                        [cat_val for _, cat_val in de_best_categories] +\n",
    "                        list(de_predictions.values())\n",
    "                })\n",
    "                best_solution.to_csv('optimization_results/differential_evolution_best_solution.csv', index=False)\n",
    "\n",
    "            print(f\"差分进化优化数据已保存至: optimization_results/differential_evolution_data.csv\")\n",
    "            print(f\"差分进化优化元数据已保存至: optimization_results/differential_evolution_metadata.csv\")\n",
    "            if 'de_best_base' in globals() and de_best_base is not None:\n",
    "                print(f\"差分进化最优解已保存至: optimization_results/differential_evolution_best_solution.csv\")\n",
    "\n",
    "            plt.close()\n",
    "        \n",
    "        # 绘制差分进化优化过程\n",
    "        plot_de_optimization(opt_progress)\n",
    "            \n",
    "    except Exception as e:\n",
    "        opt_progress.close()\n",
    "        print(f\"差分进化优化执行失败: {str(e)}\")\n",
    "        de_best_base = None\n",
    "        de_best_categories = None\n",
    "        de_best_method = None\n",
    "        de_score = 0\n",
    "        de_predictions = {}\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"准备差分进化优化时出错: {str(e)}\")\n",
    "    de_best_base = None\n",
    "    de_best_categories = None\n",
    "    de_best_method = None\n",
    "    de_score = 0\n",
    "    de_predictions = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= 4. 多目标优化 =======\n",
    "print(\"\\n4. 执行多目标优化...\")\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
    "from pymoo.operators.crossover.sbx import SBX\n",
    "from pymoo.operators.mutation.pm import PM\n",
    "from pymoo.optimize import minimize\n",
    "\n",
    "pymoo_available = True\n",
    "def predict_performance(target, features, model_type):\n",
    "    \"\"\"预测给定材料组合的性能\"\"\"\n",
    "    if target not in best_model:\n",
    "        print(f\"{target}不在best_model字典中\")\n",
    "        return None\n",
    "\n",
    "    # 获取模型名称\n",
    "    model_name = best_model[target]['tolerance_r2']['model']\n",
    "    \n",
    "    # 加载最佳模型\n",
    "    model_path = f'模型评估结果/最佳模型/{target}_最佳模型.pkl'\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    # 识别类别列\n",
    "    category_columns = [col for col in features.columns if col.startswith('类别')]\n",
    "    \n",
    "    # 根据传入的模型类型应用不同的预处理\n",
    "    features_processed = features.copy()\n",
    "    if model_type == 'linear':\n",
    "        # 线性模型: 类别编码除以1000\n",
    "        for col in category_columns:\n",
    "            features_processed[col] = features_processed[col] / 1000.0\n",
    "    elif model_type == 'tree':\n",
    "        # 树模型: 将0替换为NaN\n",
    "        for col in category_columns:\n",
    "            features_processed[col] = features_processed[col].replace(0, np.nan)\n",
    "    \n",
    "    # 预定义的特征列\n",
    "    expected_features = category_columns + ['制备方法_编码', '基底材料_编码']\n",
    "    \n",
    "    # 确保所有需要的特征列都存在\n",
    "    for feat in expected_features:\n",
    "        if feat not in features_processed.columns:\n",
    "            features_processed[feat] = 0.0\n",
    "    \n",
    "    # 使用预期的特征列\n",
    "    features_for_model = features_processed[expected_features]\n",
    "    \n",
    "    # 进行预测\n",
    "    if isinstance(model, dict) and 'model' in model:\n",
    "        if model.get('needs_scaling', False) and 'scaler' in model:\n",
    "            X_scaled = model['scaler'].transform(features_for_model)\n",
    "            pred = model['model'].predict(X_scaled)[0]\n",
    "        else:\n",
    "            pred = model['model'].predict(features_for_model)[0]\n",
    "    else:\n",
    "        # XGBoost模型可能需要特殊处理\n",
    "        if str(type(model)).find('xgboost') > -1:\n",
    "            try:\n",
    "                import xgboost as xgb\n",
    "                with xgb.config_context(verbosity=0, predict_disable_shape_check=True):\n",
    "                    pred = model.predict(features_for_model)[0]\n",
    "            except:\n",
    "                pred = model.predict(features_for_model)[0]\n",
    "        else:\n",
    "            pred = model.predict(features_for_model)[0]\n",
    "    \n",
    "    return pred\n",
    "def calculate_overall_score(base_material, categories, prep_method):\n",
    "    \"\"\"计算给定材料组合的综合评分\"\"\"\n",
    "    # 构建类别形式\n",
    "    categories_formatted = []\n",
    "    for cat_id, cat_value in categories:\n",
    "        if cat_value > 0:  # 只添加存在的材料\n",
    "            categories_formatted.append((cat_id, cat_value))\n",
    "    \n",
    "    if not categories_formatted:\n",
    "        print(\"没有有效的类别材料\")\n",
    "        return 0.0, {}  # 如果没有有效材料，返回0分\n",
    "    \n",
    "    # 创建原始编码特征\n",
    "    features = create_material_features(base_material, categories_formatted, prep_method)\n",
    "    \n",
    "    # 预测各指标性能\n",
    "    predictions = {}\n",
    "    for target in target_columns:\n",
    "        if target in best_model:\n",
    "            # 获取模型类型\n",
    "            model_name = best_model[target]['tolerance_r2']['model']\n",
    "            # 根据模型名推断类型\n",
    "            if model_name in ['LinearRegression', 'Ridge', 'Lasso', 'ElasticNet', 'HuberRegressor']:\n",
    "                model_type = 'linear'\n",
    "            elif model_name in ['XGBoost', 'RandomForest', 'GradientBoosting', 'LightGBM', 'HistGradientBoosting']:\n",
    "                model_type = 'tree'\n",
    "            elif model_name in ['DeepNN']:\n",
    "                model_type = 'nn'\n",
    "            else:\n",
    "                model_type = 'linear'  # 默认为线性模型\n",
    "            \n",
    "            # 使用修改后的预测函数，传递模型类型\n",
    "            pred = predict_performance(target, features, model_type)\n",
    "            if pred is not None:\n",
    "                predictions[target] = pred\n",
    "                #print(f\"预测 {target}: {pred:.2f}\")\n",
    "    \n",
    "    # 如果没有有效预测，返回0分\n",
    "    if not predictions:\n",
    "        print(\"没有有效的预测结果\")\n",
    "        return 0.0, {}\n",
    "    \n",
    "    # 根据因果分析结果调整权重\n",
    "    weights = {}\n",
    "    total_weight = 0.0\n",
    "    \n",
    "    for target in predictions.keys():\n",
    "        if target == '水接触角':\n",
    "            weights[target] = 1.0\n",
    "        elif target == '循环使用次数':\n",
    "            weights[target] = 1.2  # 略微提高权重\n",
    "        elif target == '吸油能力':\n",
    "            weights[target] = 1.3  # 更高权重\n",
    "        else:\n",
    "            weights[target] = 1.0\n",
    "        total_weight += weights[target]\n",
    "    \n",
    "    # 计算加权综合得分\n",
    "    score = 0.0\n",
    "    \n",
    "    for target, value in predictions.items():\n",
    "        # 归一化特定指标的得分\n",
    "        if target == '水接触角':\n",
    "            # 水接触角范围通常为90-150度，值越大越好\n",
    "            norm_score = min(1.0, max(0.0, value / 180.0))\n",
    "        elif target == '循环使用次数':\n",
    "            # 循环使用次数通常为10-50次，值越大越好\n",
    "            norm_score = min(1.0, max(0.0, value / 50.0))\n",
    "        elif target == '吸油能力':\n",
    "            # 吸油能力通常为5-50 g/g，值越大越好\n",
    "            norm_score = min(1.0, max(0.0, value / 50.0))\n",
    "        else:\n",
    "            norm_score = min(1.0, max(0.0, value / 100.0))\n",
    "        \n",
    "        #print(f\"{target} 归一化得分: {norm_score:.4f}, 权重: {weights[target]/total_weight:.2f}\")\n",
    "            \n",
    "        # 加权求和\n",
    "        if total_weight > 0:\n",
    "            score += (weights[target] / total_weight) * norm_score\n",
    "    \n",
    "    #print(f\"总综合得分: {score:.4f}\")\n",
    "    return score, predictions\n",
    "if pymoo_available:\n",
    "    # 定义多目标优化问题\n",
    "    class MaterialProblem(Problem):\n",
    "        def __init__(self):\n",
    "            # 定义参数：[base_idx, method_idx, cat1_id, cat1_val, cat2_id, cat2_val, ...]\n",
    "            n_var = 12  # 基底材料, 制备方法, 和5对类别参数\n",
    "            \n",
    "            # 定义边界\n",
    "            xl = np.zeros(n_var)\n",
    "            xu = np.zeros(n_var)\n",
    "            \n",
    "            # 基底材料和制备方法\n",
    "            xl[0] = 0\n",
    "            xu[0] = len(unique_materials['base']) - 1\n",
    "            \n",
    "            xl[1] = 0\n",
    "            xu[1] = len(unique_methods) - 1\n",
    "            \n",
    "            # 类别参数 (ID和值)\n",
    "            for i in range(5):  # 5个类别\n",
    "                # 类别ID\n",
    "                xl[2+i*2] = 1\n",
    "                xu[2+i*2] = 5\n",
    "                \n",
    "                # 类别材料编码值\n",
    "                xl[3+i*2] = 100\n",
    "                xu[3+i*2] = 699\n",
    "            \n",
    "            # 目标函数数量：每个目标变量一个\n",
    "            n_obj = len(target_columns)\n",
    "            \n",
    "            # 约束数量\n",
    "            n_constr = 0\n",
    "            \n",
    "            super().__init__(n_var=n_var, n_obj=n_obj, n_constr=n_constr, xl=xl, xu=xu)\n",
    "        \n",
    "        def _evaluate(self, x, out, *args, **kwargs):\n",
    "            # 初始化目标值数组\n",
    "            f = np.zeros((x.shape[0], self.n_obj))\n",
    "            \n",
    "            # 对每个决策向量评估目标函数\n",
    "            for i in range(x.shape[0]):\n",
    "                # 解码决策向量\n",
    "                base_idx = int(x[i, 0]) % len(unique_materials['base'])\n",
    "                method_idx = int(x[i, 1]) % len(unique_methods)\n",
    "                \n",
    "                # 提取类别信息\n",
    "                categories = []\n",
    "                for j in range(5):  # 5个类别\n",
    "                    cat_id = max(1, min(5, int(x[i, 2+j*2])))\n",
    "                    cat_val = max(100, min(699, x[i, 3+j*2]))\n",
    "                    if cat_val > 0:\n",
    "                        categories.append((cat_id, cat_val))\n",
    "                \n",
    "                # 去除重复类别\n",
    "                seen_cats = set()\n",
    "                unique_cats = []\n",
    "                for cat_id, cat_val in categories:\n",
    "                    if cat_id not in seen_cats:\n",
    "                        seen_cats.add(cat_id)\n",
    "                        unique_cats.append((cat_id, cat_val))\n",
    "                \n",
    "                # 确保至少有一个类别\n",
    "                if not unique_cats:\n",
    "                    cat_id = max(1, min(5, int(x[i, 2])))\n",
    "                    cat_val = max(100, min(699, x[i, 3]))\n",
    "                    unique_cats.append((cat_id, cat_val))\n",
    "                \n",
    "                # 转换为实际材料编码\n",
    "                base_material = unique_materials['base'][base_idx]\n",
    "                prep_method = unique_methods[method_idx]\n",
    "                \n",
    "                # 创建特征\n",
    "                features = create_material_features(base_material, unique_cats, prep_method)\n",
    "                \n",
    "                # 对每个目标变量进行预测\n",
    "                for j, target in enumerate(target_columns):\n",
    "                    if target in best_model:\n",
    "                        # 获取模型类型\n",
    "                        model_name = best_model[target]['tolerance_r2']['model']\n",
    "                        # 根据模型名推断类型\n",
    "                        if model_name in ['LinearRegression', 'Ridge', 'Lasso', 'ElasticNet', 'HuberRegressor']:\n",
    "                            model_type = 'linear'\n",
    "                        elif model_name in ['XGBoost', 'RandomForest', 'GradientBoosting', 'LightGBM', 'HistGradientBoosting']:\n",
    "                            model_type = 'tree'\n",
    "                        elif model_name in ['DeepNN']:\n",
    "                            model_type = 'nn'\n",
    "                        else:\n",
    "                            model_type = 'linear'  # 默认为线性模型\n",
    "                        \n",
    "                        # 使用适当的预处理方式进行预测\n",
    "                        pred = predict_performance(target, features, model_type)\n",
    "                        \n",
    "                        if pred is not None:\n",
    "                            # 水接触角需要取反，因为多目标优化是最小化问题，而我们希望水接触角越大越好\n",
    "                            if target == '水接触角':\n",
    "                                # 理想值是180，所以用180减去预测值，越接近0越好\n",
    "                                f[i, j] = 180.0 - pred\n",
    "                            else:\n",
    "                                # 其他指标直接取负值，因为越大越好\n",
    "                                f[i, j] = -pred\n",
    "                        else:\n",
    "                            # 默认值\n",
    "                            if target == '水接触角':\n",
    "                                f[i, j] = 90.0  # 180 - 90 = 90\n",
    "                            else:\n",
    "                                f[i, j] = -10.0\n",
    "                    else:\n",
    "                        # 默认值\n",
    "                        if target == '水接触角':\n",
    "                            f[i, j] = 90.0\n",
    "                        else:\n",
    "                            f[i, j] = -10.0\n",
    "            \n",
    "            out[\"F\"] = f\n",
    "    \n",
    "    # 创建和求解问题\n",
    "    problem = MaterialProblem()\n",
    "    \n",
    "    # 创建进度条\n",
    "    print(\"运行多目标优化...\")\n",
    "    progress_bar = CustomProgressBar(total=50, desc=\"多目标优化\")\n",
    "    \n",
    "    # 设置算法\n",
    "    algorithm = NSGA2(\n",
    "        pop_size=50,\n",
    "        sampling=FloatRandomSampling(),\n",
    "        crossover=SBX(prob=0.9, eta=15),\n",
    "        mutation=PM(eta=20),\n",
    "        eliminate_duplicates=True\n",
    "    )\n",
    "    \n",
    "    # 创建定制回调函数来更新进度条\n",
    "    def progress_callback(algorithm):\n",
    "        progress_bar.update(1)\n",
    "        return False  # 继续运行算法\n",
    "    \n",
    "    # 运行优化\n",
    "    res = minimize(\n",
    "        problem,\n",
    "        algorithm,\n",
    "        ('n_gen', 50),\n",
    "        seed=42,\n",
    "        callback=progress_callback,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    # 获取帕累托前沿解\n",
    "    pareto_front = res.F\n",
    "    pareto_solutions = res.X\n",
    "    \n",
    "    # 将目标函数值转换回正数\n",
    "    pareto_front_original = -pareto_front.copy()\n",
    "    # 特殊处理水接触角(如果是第一个目标)\n",
    "    if target_columns[0] == '水接触角':\n",
    "        pareto_front_original[:, 0] = 180.0 - pareto_front[:, 0]\n",
    "    \n",
    "    # 解析和评估帕累托最优解\n",
    "    multi_solutions = []\n",
    "    \n",
    "    print(f\"发现 {len(pareto_solutions)} 个帕累托最优解\")\n",
    "    \n",
    "    for i, solution in enumerate(pareto_solutions):\n",
    "        # 解码决策向量\n",
    "        base_idx = int(solution[0]) % len(unique_materials['base'])\n",
    "        method_idx = int(solution[1]) % len(unique_methods)\n",
    "        \n",
    "        # 提取类别信息\n",
    "        categories = []\n",
    "        for j in range(5):  # 5个类别\n",
    "            if 2+j*2 < len(solution):\n",
    "                cat_id = max(1, min(5, int(solution[2+j*2])))\n",
    "                cat_val = max(100, min(699, solution[3+j*2]))\n",
    "                if cat_val > 0:\n",
    "                    categories.append((cat_id, cat_val))\n",
    "        \n",
    "        # 去除重复类别\n",
    "        seen_cats = set()\n",
    "        unique_cats = []\n",
    "        for cat_id, cat_val in categories:\n",
    "            if cat_id not in seen_cats:\n",
    "                seen_cats.add(cat_id)\n",
    "                unique_cats.append((cat_id, cat_val))\n",
    "        \n",
    "        # 确保至少有一个类别\n",
    "        if not unique_cats:\n",
    "            cat_id = max(1, min(5, int(solution[2])))\n",
    "            cat_val = max(100, min(699, solution[3]))\n",
    "            unique_cats.append((cat_id, cat_val))\n",
    "        \n",
    "        # 转换为实际材料编码\n",
    "        mo_base = unique_materials['base'][base_idx]\n",
    "        mo_method = unique_methods[method_idx]\n",
    "        \n",
    "        # 计算综合性能\n",
    "        mo_score, mo_predictions = calculate_overall_score(mo_base, unique_cats, mo_method)\n",
    "        \n",
    "        # 存储解\n",
    "        multi_solutions.append({\n",
    "            '基底材料': mo_base,\n",
    "            '类别材料': unique_cats,\n",
    "            '制备方法': mo_method,\n",
    "            '综合评分': mo_score,\n",
    "            '预测性能': mo_predictions,\n",
    "            '帕累托等级': i + 1\n",
    "        })\n",
    "    \n",
    "    # 按综合评分排序\n",
    "    multi_solutions.sort(key=lambda x: x['综合评分'], reverse=True)\n",
    "    \n",
    "    # 输出前几个最佳解\n",
    "    print(\"多目标优化发现的最佳材料:\")\n",
    "    \n",
    "    # 尝试加载材料参照表\n",
    "    try:\n",
    "        # 加载编码参照表\n",
    "        base_ref = pd.read_excel('data_exports/base_material_encoding_reference.xlsx')\n",
    "        method_ref = pd.read_excel('data_exports/method_encoding_reference.xlsx')\n",
    "        material_ref = pd.read_excel('data_exports/material_encoding_reference.xlsx')\n",
    "        \n",
    "        # 对前三个解获取材料名称\n",
    "        for i, solution in enumerate(multi_solutions[:3]):\n",
    "            # 获取基底材料名称\n",
    "            base_name = base_ref[base_ref['编码值'] == solution['基底材料']]['基底材料'].values[0] if len(base_ref[base_ref['编码值'] == solution['基底材料']]) > 0 else f\"基底材料{solution['基底材料']}\"\n",
    "            \n",
    "            # 获取制备方法名称\n",
    "            method_name = method_ref[method_ref['编码值'] == solution['制备方法']]['制备方法'].values[0] if len(method_ref[method_ref['编码值'] == solution['制备方法']]) > 0 else f\"制备方法{solution['制备方法']}\"\n",
    "            \n",
    "            # 获取类别材料名称\n",
    "            category_names = []\n",
    "            for cat_id, cat_val in solution['类别材料']:\n",
    "                # 查找最接近的编码值\n",
    "                cat_df = material_ref[material_ref['分类ID'] == cat_id]\n",
    "                if not cat_df.empty:\n",
    "                    # 找到最接近的编码值\n",
    "                    cat_df['差值'] = abs(cat_df['编码值'] - cat_val)\n",
    "                    closest = cat_df.loc[cat_df['差值'].idxmin()]\n",
    "                    category_names.append(f\"{closest['材料名称']} (类别{cat_id}, 编码{cat_val:.1f})\")\n",
    "                else:\n",
    "                    category_names.append(f\"类别{cat_id}材料 (编码{cat_val:.1f})\")\n",
    "            \n",
    "            print(f\"\\n解 #{i+1}:\")\n",
    "            print(f\"  基底材料: {base_name} (编码: {solution['基底材料']})\")\n",
    "            print(f\"  类别材料: {', '.join(category_names)}\")\n",
    "            print(f\"  制备方法: {method_name} (编码: {solution['制备方法']})\")\n",
    "            print(f\"  综合评分: {solution['综合评分']:.4f}\")\n",
    "            print(\"  预测性能:\")\n",
    "            for target, value in solution['预测性能'].items():\n",
    "                print(f\"    {target}: {value:.2f}\")\n",
    "    except Exception as e:\n",
    "        # 如果无法获取名称，则显示编码\n",
    "        for i, solution in enumerate(multi_solutions[:3]):\n",
    "            print(f\"\\n解 #{i+1}:\")\n",
    "            print(f\"  基底材料: {solution['基底材料']}\")\n",
    "            print(f\"  类别材料: {', '.join([f'(类别{cat_id}, 值{cat_val:.1f})' for cat_id, cat_val in solution['类别材料']])}\")\n",
    "            print(f\"  制备方法: {solution['制备方法']}\")\n",
    "            print(f\"  综合评分: {solution['综合评分']:.4f}\")\n",
    "            print(\"  预测性能:\")\n",
    "            for target, value in solution['预测性能'].items():\n",
    "                print(f\"    {target}: {value:.2f}\")\n",
    "        print(f\"  注: 无法加载材料参照表: {str(e)}\")\n",
    "    \n",
    "    # 选择最佳解\n",
    "    if multi_solutions:\n",
    "        best_solution = multi_solutions[0]\n",
    "        mo_best_base = best_solution['基底材料']\n",
    "        mo_best_categories = best_solution['类别材料']\n",
    "        mo_best_method = best_solution['制备方法']\n",
    "        mo_score = best_solution['综合评分']\n",
    "        mo_predictions = best_solution['预测性能']\n",
    "    else:\n",
    "        print(\"多目标优化未能找到有效的材料组合\")\n",
    "        mo_best_base = None\n",
    "        mo_best_categories = None\n",
    "        mo_best_method = None\n",
    "        mo_score = 0\n",
    "        mo_predictions = {}\n",
    "        \n",
    "    # 可视化帕累托前沿\n",
    "    if pareto_front.shape[1] >= 2:\n",
    "        try:\n",
    "            # 绘制二维投影\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体\n",
    "            plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "            \n",
    "            # 如果有两个以上的目标，使用前两个\n",
    "            sc = plt.scatter(\n",
    "                pareto_front_original[:, 0], \n",
    "                pareto_front_original[:, 1],\n",
    "                c=range(len(pareto_front)),\n",
    "                cmap='viridis',\n",
    "                s=100, alpha=0.7\n",
    "            )\n",
    "            \n",
    "            # 添加标题和标签\n",
    "            plt.title('材料性能帕累托前沿', fontsize=15)\n",
    "            plt.xlabel(target_columns[0], fontsize=12)\n",
    "            plt.ylabel(target_columns[1], fontsize=12)\n",
    "            \n",
    "            # 添加颜色条表示解的排名\n",
    "            cbar = plt.colorbar(sc)\n",
    "            cbar.set_label('解的排名', fontsize=12)\n",
    "            \n",
    "            # 高亮最佳综合解\n",
    "            if multi_solutions:\n",
    "                best_idx = 0\n",
    "                plt.scatter(\n",
    "                    pareto_front_original[best_idx, 0],\n",
    "                    pareto_front_original[best_idx, 1],\n",
    "                    s=200, alpha=1, color='red',\n",
    "                    marker='*', label='最佳综合解'\n",
    "                )\n",
    "            \n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('optimization_results/pareto_front.png', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            # 如果有3个或更多目标，创建三维图\n",
    "            if pareto_front.shape[1] >= 3:\n",
    "                from mpl_toolkits.mplot3d import Axes3D\n",
    "                \n",
    "                fig = plt.figure(figsize=(12, 10))\n",
    "                ax = fig.add_subplot(111, projection='3d')\n",
    "                \n",
    "                sc = ax.scatter(\n",
    "                    pareto_front_original[:, 0],\n",
    "                    pareto_front_original[:, 1],\n",
    "                    pareto_front_original[:, 2],\n",
    "                    c=range(len(pareto_front)),\n",
    "                    cmap='viridis',\n",
    "                    s=100, alpha=0.7\n",
    "                )\n",
    "                \n",
    "                ax.set_title('材料性能三维帕累托前沿', fontsize=15)\n",
    "                ax.set_xlabel(target_columns[0], fontsize=12)\n",
    "                ax.set_ylabel(target_columns[1], fontsize=12)\n",
    "                ax.set_zlabel(target_columns[2], fontsize=12)\n",
    "                \n",
    "                # 高亮最佳综合解\n",
    "                if multi_solutions:\n",
    "                    best_idx = 0\n",
    "                    ax.scatter(\n",
    "                        pareto_front_original[best_idx, 0],\n",
    "                        pareto_front_original[best_idx, 1],\n",
    "                        pareto_front_original[best_idx, 2],\n",
    "                        s=200, alpha=1, color='red',\n",
    "                        marker='*', label='最佳综合解'\n",
    "                    )\n",
    "                \n",
    "                ax.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('optimization_results/pareto_front_3d.png', dpi=300)\n",
    "                plt.close()\n",
    "                # 保存帕累托前沿原始数据\n",
    "                try:\n",
    "                    # 创建数据导出目录（如果不存在）\n",
    "                    os.makedirs('optimization_results/raw_data', exist_ok=True)\n",
    "                    \n",
    "                    # 1. 保存帕累托前沿数据点\n",
    "                    pareto_data = pd.DataFrame()\n",
    "                    for i in range(pareto_front_original.shape[1]):\n",
    "                        if i < len(target_columns):\n",
    "                            column_name = target_columns[i]\n",
    "                        else:\n",
    "                            column_name = f\"目标_{i+1}\"\n",
    "                        pareto_data[column_name] = pareto_front_original[:, i]\n",
    "                    \n",
    "                    # 添加排名列\n",
    "                    pareto_data['解的排名'] = range(1, len(pareto_front_original) + 1)\n",
    "                    \n",
    "                    # 标记最佳综合解\n",
    "                    pareto_data['是最佳综合解'] = False\n",
    "                    if multi_solutions:\n",
    "                        pareto_data.loc[0, '是最佳综合解'] = True\n",
    "                    \n",
    "                    # 保存为CSV文件\n",
    "                    pareto_data.to_csv('optimization_results/raw_data/pareto_front_data.csv', index=False, encoding='utf-8-sig')\n",
    "                    print(f\"已保存帕累托前沿数据点到 'optimization_results/raw_data/pareto_front_data.csv'\")\n",
    "                    \n",
    "                    # 2. 保存解决方案详细数据\n",
    "                    solutions_data = pd.DataFrame()\n",
    "                    solutions_data['解的排名'] = range(1, len(multi_solutions) + 1)\n",
    "                    solutions_data['基底材料'] = [sol['基底材料'] for sol in multi_solutions]\n",
    "                    solutions_data['制备方法'] = [sol['制备方法'] for sol in multi_solutions]\n",
    "                    solutions_data['综合评分'] = [sol['综合评分'] for sol in multi_solutions]\n",
    "                    \n",
    "                    # 添加类别材料列\n",
    "                    solutions_data['类别材料'] = [str(sol['类别材料']) for sol in multi_solutions]\n",
    "                    \n",
    "                    # 添加各目标预测性能\n",
    "                    for target in target_columns:\n",
    "                        solutions_data[f'预测_{target}'] = [sol['预测性能'].get(target, float('nan')) for sol in multi_solutions]\n",
    "                    \n",
    "                    # 保存为CSV文件\n",
    "                    solutions_data.to_csv('optimization_results/raw_data/solutions_data.csv', index=False, encoding='utf-8-sig')\n",
    "                    print(f\"已保存解决方案数据到 'optimization_results/raw_data/solutions_data.csv'\")\n",
    "                    \n",
    "                    # 3. 保存为Excel文件，包含多个工作表\n",
    "                    with pd.ExcelWriter('optimization_results/raw_data/optimization_results.xlsx', engine='openpyxl') as writer:\n",
    "                        pareto_data.to_excel(writer, sheet_name='帕累托前沿数据', index=False)\n",
    "                        solutions_data.to_excel(writer, sheet_name='解决方案数据', index=False)\n",
    "                        \n",
    "                        # 创建更详细的解决方案工作表\n",
    "                        detailed_solutions = pd.DataFrame()\n",
    "                        for i, sol in enumerate(multi_solutions):\n",
    "                            row = {\n",
    "                                '解的排名': i + 1,\n",
    "                                '基底材料': sol['基底材料'],\n",
    "                                '制备方法': sol['制备方法'],\n",
    "                                '综合评分': sol['综合评分']\n",
    "                            }\n",
    "                            \n",
    "                            # 添加类别材料\n",
    "                            for j, (cat_id, cat_val) in enumerate(sol['类别材料']):\n",
    "                                row[f'类别{cat_id}_ID'] = cat_id\n",
    "                                row[f'类别{cat_id}_值'] = cat_val\n",
    "                            \n",
    "                            # 添加预测性能\n",
    "                            for target, value in sol['预测性能'].items():\n",
    "                                row[f'预测_{target}'] = value\n",
    "                                \n",
    "                            # 添加到DataFrame\n",
    "                            detailed_solutions = pd.concat([detailed_solutions, pd.DataFrame([row])], ignore_index=True)\n",
    "                        \n",
    "                        detailed_solutions.to_excel(writer, sheet_name='详细解决方案', index=False)\n",
    "                    \n",
    "                    print(f\"已保存完整优化结果到 'optimization_results/raw_data/optimization_results.xlsx'\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"保存帕累托前沿原始数据时出错: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"绘制帕累托前沿时出错: {str(e)}\")\n",
    "else:\n",
    "    print(\"已跳过多目标优化，没有可用的pymoo库\")\n",
    "    mo_best_base = None\n",
    "    mo_best_categories = None\n",
    "    mo_best_method = None\n",
    "    mo_score = 0\n",
    "    mo_predictions = {}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= 5. 集成所有优化结果 =======\n",
    "print(\"\\n5. 集成所有优化结果...\")\n",
    "\n",
    "# 收集所有找到的解\n",
    "all_solutions = []\n",
    "\n",
    "# 添加有效的解\n",
    "if bayes_best_base is not None:\n",
    "    all_solutions.append({\n",
    "        '优化方法': '贝叶斯优化',\n",
    "        '基底材料': bayes_best_base,\n",
    "        '类别材料': bayes_best_categories,\n",
    "        '制备方法': bayes_best_method,\n",
    "        '综合评分': bayes_score,\n",
    "        '预测性能': bayes_predictions\n",
    "    })\n",
    "\n",
    "if gen_best_base is not None:\n",
    "    all_solutions.append({\n",
    "        '优化方法': '生成式AI',\n",
    "        '基底材料': gen_best_base,\n",
    "        '类别材料': gen_best_categories,\n",
    "        '制备方法': gen_best_method,\n",
    "        '综合评分': gen_score,\n",
    "        '预测性能': gen_predictions\n",
    "    })\n",
    "\n",
    "if de_best_base is not None:\n",
    "    all_solutions.append({\n",
    "        '优化方法': '差分进化',\n",
    "        '基底材料': de_best_base,\n",
    "        '类别材料': de_best_categories,\n",
    "        '制备方法': de_best_method,\n",
    "        '综合评分': de_score,\n",
    "        '预测性能': de_predictions\n",
    "    })\n",
    "\n",
    "if mo_best_base is not None:\n",
    "    all_solutions.append({\n",
    "        '优化方法': '多目标优化',\n",
    "        '基底材料': mo_best_base,\n",
    "        '类别材料': mo_best_categories,\n",
    "        '制备方法': mo_best_method,\n",
    "        '综合评分': mo_score,\n",
    "        '预测性能': mo_predictions\n",
    "    })\n",
    "\n",
    "# 按照综合评分排序\n",
    "all_solutions.sort(key=lambda x: x['综合评分'], reverse=True)\n",
    "\n",
    "# 输出所有解的比较\n",
    "print(\"\\n所有优化方法的结果比较:\")\n",
    "\n",
    "if all_solutions:\n",
    "    # 尝试加载材料参照表以获取名称\n",
    "    try:\n",
    "        # 加载编码参照表\n",
    "        base_ref = pd.read_excel('data_exports/base_material_encoding_reference.xlsx')\n",
    "        method_ref = pd.read_excel('data_exports/method_encoding_reference.xlsx')\n",
    "        material_ref = pd.read_excel('data_exports/material_encoding_reference.xlsx')\n",
    "        \n",
    "        for i, solution in enumerate(all_solutions):\n",
    "            # 获取基底材料名称\n",
    "            base_name = base_ref[base_ref['编码值'] == solution['基底材料']]['基底材料'].values[0] if len(base_ref[base_ref['编码值'] == solution['基底材料']]) > 0 else f\"基底材料{solution['基底材料']}\"\n",
    "            \n",
    "            # 获取制备方法名称\n",
    "            method_name = method_ref[method_ref['编码值'] == solution['制备方法']]['制备方法'].values[0] if len(method_ref[method_ref['编码值'] == solution['制备方法']]) > 0 else f\"制备方法{solution['制备方法']}\"\n",
    "            \n",
    "            # 获取类别材料名称\n",
    "            category_names = []\n",
    "            for cat_id, cat_val in solution['类别材料']:\n",
    "                # 查找最接近的编码值\n",
    "                cat_df = material_ref[material_ref['分类ID'] == cat_id]\n",
    "                if not cat_df.empty:\n",
    "                    # 找到最接近的编码值\n",
    "                    cat_df['差值'] = abs(cat_df['编码值'] - cat_val)\n",
    "                    closest = cat_df.loc[cat_df['差值'].idxmin()]\n",
    "                    category_names.append(f\"{closest['材料名称']} (类别{cat_id}, 编码{cat_val:.1f})\")\n",
    "                else:\n",
    "                    category_names.append(f\"类别{cat_id}材料 (编码{cat_val:.1f})\")\n",
    "            \n",
    "            print(f\"\\n{i+1}. {solution['优化方法']}:\")\n",
    "            print(f\"  基底材料: {base_name} (编码: {solution['基底材料']})\")\n",
    "            print(f\"  类别材料: {', '.join(category_names)}\")\n",
    "            print(f\"  制备方法: {method_name} (编码: {solution['制备方法']})\")\n",
    "            print(f\"  综合评分: {solution['综合评分']:.4f}\")\n",
    "            print(\"  预测性能:\")\n",
    "            for target, value in solution['预测性能'].items():\n",
    "                print(f\"    {target}: {value:.2f}\")\n",
    "    except Exception as e:\n",
    "        # 如果无法获取名称，则显示编码\n",
    "        for i, solution in enumerate(all_solutions):\n",
    "            print(f\"\\n{i+1}. {solution['优化方法']}:\")\n",
    "            print(f\"  基底材料: {solution['基底材料']}\")\n",
    "            print(f\"  类别材料: {', '.join([f'(类别{cat_id}, 值{cat_val:.1f})' for cat_id, cat_val in solution['类别材料']])}\")\n",
    "            print(f\"  制备方法: {solution['制备方法']}\")\n",
    "            print(f\"  综合评分: {solution['综合评分']:.4f}\")\n",
    "            print(\"  预测性能:\")\n",
    "            for target, value in solution['预测性能'].items():\n",
    "                print(f\"    {target}: {value:.2f}\")\n",
    "        print(f\"  注: 无法加载材料参照表: {str(e)}\")\n",
    "    \n",
    "    # 选择集成方案（排名前三的方案）\n",
    "    top_solutions = all_solutions[:min(3, len(all_solutions))]\n",
    "    \n",
    "    # 创建集成分析可视化\n",
    "    if len(top_solutions) >= 2:\n",
    "        # 性能雷达图\n",
    "        categories = target_columns\n",
    "        if categories:\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体\n",
    "            plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "            \n",
    "            ax = plt.subplot(111, polar=True)\n",
    "            \n",
    "            # 计算角度\n",
    "            N = len(categories)\n",
    "            angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "            angles += angles[:1]  # 闭合\n",
    "            \n",
    "            # 绘制每种方案\n",
    "            for i, solution in enumerate(top_solutions):\n",
    "                # 提取性能值\n",
    "                values = []\n",
    "                for cat in categories:\n",
    "                    if cat in solution['预测性能']:\n",
    "                        val = solution['预测性能'][cat]\n",
    "                        # 归一化\n",
    "                        if cat == '水接触角':\n",
    "                            norm_val = min(val / 180.0, 1.0)\n",
    "                        elif cat == '循环使用次数':\n",
    "                            norm_val = min(val / 50.0, 1.0)\n",
    "                        elif cat == '吸油能力':\n",
    "                            norm_val = min(val / 50.0, 1.0)\n",
    "                        else:\n",
    "                            norm_val = val / 100.0\n",
    "                        values.append(norm_val)\n",
    "                    else:\n",
    "                        values.append(0)\n",
    "                \n",
    "                # 闭合\n",
    "                values += values[:1]\n",
    "                \n",
    "                # 绘制\n",
    "                ax.plot(angles, values, 'o-', linewidth=2, label=solution['优化方法'])\n",
    "                ax.fill(angles, values, alpha=0.25)\n",
    "            \n",
    "            # 设置标签\n",
    "            ax.set_thetagrids(np.degrees(angles[:-1]), categories)\n",
    "            \n",
    "            plt.title('不同优化方法的材料性能比较', size=15)\n",
    "            plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "            plt.savefig('optimization_results/optimization_comparison.png', dpi=300)\n",
    "            plt.close()\n",
    "    \n",
    "    # 保存最佳材料组合\n",
    "    best_solution = all_solutions[0]\n",
    "    \n",
    "    # 获取并打印最佳材料组合的详细信息\n",
    "    try:\n",
    "        # 加载编码参照表\n",
    "        base_ref = pd.read_excel('data_exports/base_material_encoding_reference.xlsx')\n",
    "        method_ref = pd.read_excel('data_exports/method_encoding_reference.xlsx')\n",
    "        material_ref = pd.read_excel('data_exports/material_encoding_reference.xlsx')\n",
    "        \n",
    "        # 获取基底材料名称\n",
    "        base_name = base_ref[base_ref['编码值'] == best_solution['基底材料']]['基底材料'].values[0] if len(base_ref[base_ref['编码值'] == best_solution['基底材料']]) > 0 else f\"基底材料{best_solution['基底材料']}\"\n",
    "        \n",
    "        # 获取制备方法名称\n",
    "        method_name = method_ref[method_ref['编码值'] == best_solution['制备方法']]['制备方法'].values[0] if len(method_ref[method_ref['编码值'] == best_solution['制备方法']]) > 0 else f\"制备方法{best_solution['制备方法']}\"\n",
    "        \n",
    "        # 获取类别材料名称\n",
    "        category_names = []\n",
    "        for cat_id, cat_val in best_solution['类别材料']:\n",
    "            # 查找最接近的编码值\n",
    "            cat_df = material_ref[material_ref['分类ID'] == cat_id]\n",
    "            if not cat_df.empty:\n",
    "                # 找到最接近的编码值\n",
    "                cat_df['差值'] = abs(cat_df['编码值'] - cat_val)\n",
    "                closest = cat_df.loc[cat_df['差值'].idxmin()]\n",
    "                category_names.append(f\"{closest['材料名称']} (类别{cat_id}, 编码{cat_val:.1f})\")\n",
    "            else:\n",
    "                category_names.append(f\"类别{cat_id}材料 (编码{cat_val:.1f})\")\n",
    "        \n",
    "        print(f\"\\n集成优化框架推荐的最佳材料组合:\")\n",
    "        print(f\"  优化方法: {best_solution['优化方法']}\")\n",
    "        print(f\"  基底材料: {base_name} (编码: {best_solution['基底材料']})\")\n",
    "        print(f\"  类别材料: {', '.join(category_names)}\")\n",
    "        print(f\"  制备方法: {method_name} (编码: {best_solution['制备方法']})\")\n",
    "        print(f\"  综合评分: {best_solution['综合评分']:.4f}\")\n",
    "        print(\"  预测性能:\")\n",
    "        for target, value in best_solution['预测性能'].items():\n",
    "            print(f\"    {target}: {value:.2f}\")\n",
    "    except Exception as e:\n",
    "        # 如果无法获取名称，则显示编码\n",
    "        print(f\"\\n集成优化框架推荐的最佳材料组合:\")\n",
    "        print(f\"  优化方法: {best_solution['优化方法']}\")\n",
    "        print(f\"  基底材料: {best_solution['基底材料']}\")\n",
    "        print(f\"  类别材料: {', '.join([f'(类别{cat_id}, 值{cat_val:.1f})' for cat_id, cat_val in best_solution['类别材料']])}\")\n",
    "        print(f\"  制备方法: {best_solution['制备方法']}\")\n",
    "        print(f\"  综合评分: {best_solution['综合评分']:.4f}\")\n",
    "        print(\"  预测性能:\")\n",
    "        for target, value in best_solution['预测性能'].items():\n",
    "            print(f\"    {target}: {value:.2f}\")\n",
    "        print(f\"  注: 无法加载材料参照表: {str(e)}\")\n",
    "    \n",
    "    # 创建性能对比柱状图\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体\n",
    "    plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "    \n",
    "    # 准备数据\n",
    "    targets = target_columns\n",
    "    methods = [s['优化方法'] for s in all_solutions]\n",
    "    \n",
    "    # 多组柱状图\n",
    "    x = np.arange(len(targets))\n",
    "    width = 0.8 / len(methods)\n",
    "    \n",
    "    # 绘制每个方法的柱状图\n",
    "    for i, method in enumerate(methods):\n",
    "        performances = []\n",
    "        for target in targets:\n",
    "            solution = all_solutions[i]\n",
    "            perf = solution['预测性能'].get(target, 0)\n",
    "            performances.append(perf)\n",
    "        \n",
    "        pos = x - 0.4 + (i + 0.5) * width\n",
    "        plt.bar(pos, performances, width, label=method)\n",
    "    \n",
    "    # 添加标签和标题\n",
    "    plt.xlabel('性能指标')\n",
    "    plt.ylabel('预测值')\n",
    "    plt.title('不同优化方法的材料性能对比')\n",
    "    plt.xticks(x, targets)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('optimization_results/performance_comparison.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 保存所有解到CSV\n",
    "    solutions_data = []\n",
    "    \n",
    "    for solution in all_solutions:\n",
    "        # 基础信息\n",
    "        row = {\n",
    "            '优化方法': solution['优化方法'],\n",
    "            '基底材料': solution['基底材料'],\n",
    "            '类别材料': str([(cat_id, f\"{cat_val:.1f}\") for cat_id, cat_val in solution['类别材料']]),\n",
    "            '制备方法': solution['制备方法'],\n",
    "            '综合评分': solution['综合评分']\n",
    "        }\n",
    "        \n",
    "        # 添加各性能指标\n",
    "        for target, value in solution['预测性能'].items():\n",
    "            row[target] = value\n",
    "            \n",
    "        solutions_data.append(row)\n",
    "    \n",
    "    # 创建DataFrame并保存\n",
    "    solutions_df = pd.DataFrame(solutions_data)\n",
    "    solutions_df.to_csv('optimization_results/optimized_materials.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\n优化结果已保存到: optimization_results/optimized_materials.csv\")\n",
    "    \n",
    "else:\n",
    "    print(\"没有任何优化方法找到有效的材料组合\")\n",
    "\n",
    "print(\"\\n材料性能多方法集成优化完成\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unet)",
   "language": "python",
   "name": "unet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
